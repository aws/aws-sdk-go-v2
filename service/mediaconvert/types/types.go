// Code generated by smithy-go-codegen DO NOT EDIT.

package types

import (
	"time"
)

// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
// value AAC. The service accepts one of two mutually exclusive groups of AAC
// settings--VBR and CBR. To select one of these modes, set the value of Bitrate
// control mode (rateControlMode) to "VBR" or "CBR". In VBR mode, you control the
// audio quality with the setting VBR quality (vbrQuality). In CBR mode, you use
// the setting Bitrate (bitrate). Defaults and valid values depend on the rate
// control mode.
type AacSettings struct {

	// Choose BROADCASTER_MIXED_AD when the input contains pre-mixed main audio + audio
	// description (AD) as a stereo pair. The value for AudioType will be set to 3,
	// which signals to downstream systems that this stream contains "broadcaster mixed
	// AD". Note that the input received by the encoder must contain pre-mixed audio;
	// the encoder does not perform the mixing. When you choose BROADCASTER_MIXED_AD,
	// the encoder ignores any values you provide in AudioType and
	// FollowInputAudioType. Choose NORMAL when the input does not contain pre-mixed
	// audio + audio description (AD). In this case, the encoder will use any values
	// you provide for AudioType and FollowInputAudioType.
	AudioDescriptionBroadcasterMix AacAudioDescriptionBroadcasterMix

	// Specify the average bitrate in bits per second. The set of valid values for this
	// setting is: 6000, 8000, 10000, 12000, 14000, 16000, 20000, 24000, 28000, 32000,
	// 40000, 48000, 56000, 64000, 80000, 96000, 112000, 128000, 160000, 192000,
	// 224000, 256000, 288000, 320000, 384000, 448000, 512000, 576000, 640000, 768000,
	// 896000, 1024000. The value you set is also constrained by the values that you
	// choose for Profile (codecProfile), Bitrate control mode (codingMode), and Sample
	// rate (sampleRate). Default values depend on Bitrate control mode and Profile.
	Bitrate int32

	// AAC Profile.
	CodecProfile AacCodecProfile

	// Mono (Audio Description), Mono, Stereo, or 5.1 channel layout. Valid values
	// depend on rate control mode and profile. "1.0 - Audio Description (Receiver
	// Mix)" setting receives a stereo description plus control track and emits a mono
	// AAC encode of the description track, with control data emitted in the PES header
	// as per ETSI TS 101 154 Annex E.
	CodingMode AacCodingMode

	// Rate Control Mode.
	RateControlMode AacRateControlMode

	// Enables LATM/LOAS AAC output. Note that if you use LATM/LOAS AAC in an output,
	// you must choose "No container" for the output container.
	RawFormat AacRawFormat

	// Sample rate in Hz. Valid values depend on rate control mode and profile.
	SampleRate int32

	// Use MPEG-2 AAC instead of MPEG-4 AAC audio for raw or MPEG-2 Transport Stream
	// containers.
	Specification AacSpecification

	// VBR Quality Level - Only used if rate_control_mode is VBR.
	VbrQuality AacVbrQuality
}

// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
// value AC3.
type Ac3Settings struct {

	// Specify the average bitrate in bits per second. Valid bitrates depend on the
	// coding mode.
	Bitrate int32

	// Specify the bitstream mode for the AC-3 stream that the encoder emits. For more
	// information about the AC3 bitstream mode, see ATSC A/52-2012 (Annex E).
	BitstreamMode Ac3BitstreamMode

	// Dolby Digital coding mode. Determines number of channels.
	CodingMode Ac3CodingMode

	// Sets the dialnorm for the output. If blank and input audio is Dolby Digital,
	// dialnorm will be passed through.
	Dialnorm int32

	// If set to FILM_STANDARD, adds dynamic range compression signaling to the output
	// bitstream as defined in the Dolby Digital specification.
	DynamicRangeCompressionProfile Ac3DynamicRangeCompressionProfile

	// Applies a 120Hz lowpass filter to the LFE channel prior to encoding. Only valid
	// with 3_2_LFE coding mode.
	LfeFilter Ac3LfeFilter

	// When set to FOLLOW_INPUT, encoder metadata will be sourced from the DD, DD+, or
	// DolbyE decoder that supplied this audio data. If audio was not supplied from one
	// of these streams, then the static metadata settings will be used.
	MetadataControl Ac3MetadataControl

	// This value is always 48000. It represents the sample rate in Hz.
	SampleRate int32
}

// Accelerated transcoding can significantly speed up jobs with long, visually
// complex content.
type AccelerationSettings struct {

	// Specify the conditions when the service will run your job with accelerated
	// transcoding.
	//
	// This member is required.
	Mode AccelerationMode
}

// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
// value AIFF.
type AiffSettings struct {

	// Specify Bit depth (BitDepth), in bits per sample, to choose the encoding quality
	// for this audio track.
	BitDepth int32

	// Specify the number of channels in this output audio track. Valid values are 1
	// and even numbers up to 64. For example, 1, 2, 4, 6, and so on, up to 64.
	Channels int32

	// Sample rate in hz.
	SampleRate int32
}

// Settings for ancillary captions source.
type AncillarySourceSettings struct {

	// Specify whether this set of input captions appears in your outputs in both 608
	// and 708 format. If you choose Upconvert (UPCONVERT), MediaConvert includes the
	// captions data in two ways: it passes the 608 data through using the 608
	// compatibility bytes fields of the 708 wrapper, and it also translates the 608
	// data into 708.
	Convert608To708 AncillaryConvert608To708

	// Specifies the 608 channel number in the ancillary data track from which to
	// extract captions. Unused for passthrough.
	SourceAncillaryChannelNumber int32

	// By default, the service terminates any unterminated captions at the end of each
	// input. If you want the caption to continue onto your next input, disable this
	// setting.
	TerminateCaptions AncillaryTerminateCaptions
}

// When you mimic a multi-channel audio layout with multiple mono-channel tracks,
// you can tag each channel layout manually. For example, you would tag the tracks
// that contain your left, right, and center audio with Left (L), Right (R), and
// Center (C), respectively. When you don't specify a value, MediaConvert labels
// your track as Center (C) by default. To use audio layout tagging, your output
// must be in a QuickTime (.mov) container; your audio codec must be AAC, WAV, or
// AIFF; and you must set up your audio track to have only one channel.
type AudioChannelTaggingSettings struct {

	// You can add a tag for this mono-channel audio track to mimic its placement in a
	// multi-channel layout. For example, if this track is the left surround channel,
	// choose Left surround (LS).
	ChannelTag AudioChannelTag
}

// Audio codec settings (CodecSettings) under (AudioDescriptions) contains the
// group of settings related to audio encoding. The settings in this group vary
// depending on the value that you choose for Audio codec (Codec). For each codec
// enum that you choose, define the corresponding settings object. The following
// lists the codec enum, settings object pairs. * AAC, AacSettings * MP2,
// Mp2Settings * MP3, Mp3Settings * WAV, WavSettings * AIFF, AiffSettings * AC3,
// Ac3Settings * EAC3, Eac3Settings * EAC3_ATMOS, Eac3AtmosSettings * VORBIS,
// VorbisSettings * OPUS, OpusSettings
type AudioCodecSettings struct {

	// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
	// value AAC. The service accepts one of two mutually exclusive groups of AAC
	// settings--VBR and CBR. To select one of these modes, set the value of Bitrate
	// control mode (rateControlMode) to "VBR" or "CBR". In VBR mode, you control the
	// audio quality with the setting VBR quality (vbrQuality). In CBR mode, you use
	// the setting Bitrate (bitrate). Defaults and valid values depend on the rate
	// control mode.
	AacSettings *AacSettings

	// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
	// value AC3.
	Ac3Settings *Ac3Settings

	// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
	// value AIFF.
	AiffSettings *AiffSettings

	// Type of Audio codec.
	Codec AudioCodec

	// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
	// value EAC3_ATMOS.
	Eac3AtmosSettings *Eac3AtmosSettings

	// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
	// value EAC3.
	Eac3Settings *Eac3Settings

	// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
	// value MP2.
	Mp2Settings *Mp2Settings

	// Required when you set Codec, under AudioDescriptions>CodecSettings, to the value
	// MP3.
	Mp3Settings *Mp3Settings

	// Required when you set Codec, under AudioDescriptions>CodecSettings, to the value
	// OPUS.
	OpusSettings *OpusSettings

	// Required when you set Codec, under AudioDescriptions>CodecSettings, to the value
	// Vorbis.
	VorbisSettings *VorbisSettings

	// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
	// value WAV.
	WavSettings *WavSettings
}

// Description of audio output
type AudioDescription struct {

	// When you mimic a multi-channel audio layout with multiple mono-channel tracks,
	// you can tag each channel layout manually. For example, you would tag the tracks
	// that contain your left, right, and center audio with Left (L), Right (R), and
	// Center (C), respectively. When you don't specify a value, MediaConvert labels
	// your track as Center (C) by default. To use audio layout tagging, your output
	// must be in a QuickTime (.mov) container; your audio codec must be AAC, WAV, or
	// AIFF; and you must set up your audio track to have only one channel.
	AudioChannelTaggingSettings *AudioChannelTaggingSettings

	// Advanced audio normalization settings. Ignore these settings unless you need to
	// comply with a loudness standard.
	AudioNormalizationSettings *AudioNormalizationSettings

	// Specifies which audio data to use from each input. In the simplest case, specify
	// an "Audio Selector":#inputs-audio_selector by name based on its order within
	// each input. For example if you specify "Audio Selector 3", then the third audio
	// selector will be used from each input. If an input does not have an "Audio
	// Selector 3", then the audio selector marked as "default" in that input will be
	// used. If there is no audio selector marked as "default", silence will be
	// inserted for the duration of that input. Alternatively, an "Audio Selector
	// Group":#inputs-audio_selector_group name may be specified, with similar
	// default/silence behavior. If no audio_source_name is specified, then "Audio
	// Selector 1" will be chosen automatically.
	AudioSourceName *string

	// Applies only if Follow Input Audio Type is unchecked (false). A number between 0
	// and 255. The following are defined in ISO-IEC 13818-1: 0 = Undefined, 1 = Clean
	// Effects, 2 = Hearing Impaired, 3 = Visually Impaired Commentary, 4-255 =
	// Reserved.
	AudioType int32

	// When set to FOLLOW_INPUT, if the input contains an ISO 639 audio_type, then that
	// value is passed through to the output. If the input contains no ISO 639
	// audio_type, the value in Audio Type is included in the output. Otherwise the
	// value in Audio Type is included in the output. Note that this field and
	// audioType are both ignored if audioDescriptionBroadcasterMix is set to
	// BROADCASTER_MIXED_AD.
	AudioTypeControl AudioTypeControl

	// Audio codec settings (CodecSettings) under (AudioDescriptions) contains the
	// group of settings related to audio encoding. The settings in this group vary
	// depending on the value that you choose for Audio codec (Codec). For each codec
	// enum that you choose, define the corresponding settings object. The following
	// lists the codec enum, settings object pairs. * AAC, AacSettings * MP2,
	// Mp2Settings * MP3, Mp3Settings * WAV, WavSettings * AIFF, AiffSettings * AC3,
	// Ac3Settings * EAC3, Eac3Settings * EAC3_ATMOS, Eac3AtmosSettings * VORBIS,
	// VorbisSettings * OPUS, OpusSettings
	CodecSettings *AudioCodecSettings

	// Specify the language for this audio output track. The service puts this language
	// code into your output audio track when you set Language code control
	// (AudioLanguageCodeControl) to Use configured (USE_CONFIGURED). The service also
	// uses your specified custom language code when you set Language code control
	// (AudioLanguageCodeControl) to Follow input (FOLLOW_INPUT), but your input file
	// doesn't specify a language code. For all outputs, you can use an ISO 639-2 or
	// ISO 639-3 code. For streaming outputs, you can also use any other code in the
	// full RFC-5646 specification. Streaming outputs are those that are in one of the
	// following output groups: CMAF, DASH ISO, Apple HLS, or Microsoft Smooth
	// Streaming.
	CustomLanguageCode *string

	// Indicates the language of the audio output track. The ISO 639 language specified
	// in the 'Language Code' drop down will be used when 'Follow Input Language Code'
	// is not selected or when 'Follow Input Language Code' is selected but there is no
	// ISO 639 language code specified by the input.
	LanguageCode LanguageCode

	// Specify which source for language code takes precedence for this audio track.
	// When you choose Follow input (FOLLOW_INPUT), the service uses the language code
	// from the input track if it's present. If there's no languge code on the input
	// track, the service uses the code that you specify in the setting Language code
	// (languageCode or customLanguageCode). When you choose Use configured
	// (USE_CONFIGURED), the service uses the language code that you specify.
	LanguageCodeControl AudioLanguageCodeControl

	// Advanced audio remixing settings.
	RemixSettings *RemixSettings

	// Specify a label for this output audio stream. For example, "English", "Director
	// commentary", or "track_2". For streaming outputs, MediaConvert passes this
	// information into destination manifests for display on the end-viewer's player
	// device. For outputs in other output groups, the service ignores this setting.
	StreamName *string
}

// Advanced audio normalization settings. Ignore these settings unless you need to
// comply with a loudness standard.
type AudioNormalizationSettings struct {

	// Choose one of the following audio normalization algorithms: ITU-R BS.1770-1:
	// Ungated loudness. A measurement of ungated average loudness for an entire piece
	// of content, suitable for measurement of short-form content under ATSC
	// recommendation A/85. Supports up to 5.1 audio channels. ITU-R BS.1770-2: Gated
	// loudness. A measurement of gated average loudness compliant with the
	// requirements of EBU-R128. Supports up to 5.1 audio channels. ITU-R BS.1770-3:
	// Modified peak. The same loudness measurement algorithm as 1770-2, with an
	// updated true peak measurement. ITU-R BS.1770-4: Higher channel count. Allows for
	// more audio channels than the other algorithms, including configurations such as
	// 7.1.
	Algorithm AudioNormalizationAlgorithm

	// When enabled the output audio is corrected using the chosen algorithm. If
	// disabled, the audio will be measured but not adjusted.
	AlgorithmControl AudioNormalizationAlgorithmControl

	// Content measuring above this level will be corrected to the target level.
	// Content measuring below this level will not be corrected.
	CorrectionGateLevel int32

	// If set to LOG, log each output's audio track loudness to a CSV file.
	LoudnessLogging AudioNormalizationLoudnessLogging

	// If set to TRUE_PEAK, calculate and log the TruePeak for each output's audio
	// track loudness.
	PeakCalculation AudioNormalizationPeakCalculation

	// When you use Audio normalization (AudioNormalizationSettings), optionally use
	// this setting to specify a target loudness. If you don't specify a value here,
	// the encoder chooses a value for you, based on the algorithm that you choose for
	// Algorithm (algorithm). If you choose algorithm 1770-1, the encoder will choose
	// -24 LKFS; otherwise, the encoder will choose -23 LKFS.
	TargetLkfs float64
}

// Selector for Audio
type AudioSelector struct {

	// Selects a specific language code from within an audio source, using the ISO
	// 639-2 or ISO 639-3 three-letter language code
	CustomLanguageCode *string

	// Enable this setting on one audio selector to set it as the default for the job.
	// The service uses this default for outputs where it can't find the specified
	// input audio. If you don't set a default, those outputs have no audio.
	DefaultSelection AudioDefaultSelection

	// Specifies audio data from an external file source.
	ExternalAudioFileInput *string

	// Selects a specific language code from within an audio source.
	LanguageCode LanguageCode

	// Specifies a time delta in milliseconds to offset the audio from the input video.
	Offset int32

	// Selects a specific PID from within an audio source (e.g. 257 selects PID 0x101).
	Pids []int32

	// Use this setting for input streams that contain Dolby E, to have the service
	// extract specific program data from the track. To select multiple programs,
	// create multiple selectors with the same Track and different Program numbers. In
	// the console, this setting is visible when you set Selector type to Track. Choose
	// the program number from the dropdown list. If you are sending a JSON file,
	// provide the program ID, which is part of the audio metadata. If your input file
	// has incorrect metadata, you can choose All channels instead of a program number
	// to have the service ignore the program IDs and include all the programs in the
	// track.
	ProgramSelection int32

	// Use these settings to reorder the audio channels of one input to match those of
	// another input. This allows you to combine the two files into a single output,
	// one after the other.
	RemixSettings *RemixSettings

	// Specifies the type of the audio selector.
	SelectorType AudioSelectorType

	// Identify a track from the input audio to include in this selector by entering
	// the track index number. To include several tracks in a single audio selector,
	// specify multiple tracks as follows. Using the console, enter a comma-separated
	// list. For examle, type "1,2,3" to include tracks 1 through 3. Specifying
	// directly in your JSON job file, provide the track numbers in an array. For
	// example, "tracks": [1,2,3].
	Tracks []int32
}

// Group of Audio Selectors
type AudioSelectorGroup struct {

	// Name of an Audio Selector within the same input to include in the group. Audio
	// selector names are standardized, based on their order within the input (e.g.,
	// "Audio Selector 1"). The audio selector name parameter can be repeated to add
	// any number of audio selectors to the group.
	AudioSelectorNames []string
}

// Use automated ABR to have MediaConvert set up the renditions in your ABR package
// for you automatically, based on characteristics of your input video. This
// feature optimizes video quality while minimizing the overall size of your ABR
// package.
type AutomatedAbrSettings struct {

	// Optional. The maximum target bit rate used in your automated ABR stack. Use this
	// value to set an upper limit on the bandwidth consumed by the highest-quality
	// rendition. This is the rendition that is delivered to viewers with the fastest
	// internet connections. If you don't specify a value, MediaConvert uses 8,000,000
	// (8 mb/s) by default.
	MaxAbrBitrate int32

	// Optional. The maximum number of renditions that MediaConvert will create in your
	// automated ABR stack. The number of renditions is determined automatically, based
	// on analysis of each job, but will never exceed this limit. When you set this to
	// Auto in the console, which is equivalent to excluding it from your JSON job
	// specification, MediaConvert defaults to a limit of 15.
	MaxRenditions int32

	// Optional. The minimum target bitrate used in your automated ABR stack. Use this
	// value to set a lower limit on the bitrate of video delivered to viewers with
	// slow internet connections. If you don't specify a value, MediaConvert uses
	// 600,000 (600 kb/s) by default.
	MinAbrBitrate int32
}

// Use automated encoding to have MediaConvert choose your encoding settings for
// you, based on characteristics of your input video.
type AutomatedEncodingSettings struct {

	// Use automated ABR to have MediaConvert set up the renditions in your ABR package
	// for you automatically, based on characteristics of your input video. This
	// feature optimizes video quality while minimizing the overall size of your ABR
	// package.
	AbrSettings *AutomatedAbrSettings
}

// Settings for quality-defined variable bitrate encoding with the AV1 codec.
// Required when you set Rate control mode to QVBR. Not valid when you set Rate
// control mode to a value other than QVBR, or when you don't define Rate control
// mode.
type Av1QvbrSettings struct {

	// Required when you use QVBR rate control mode. That is, when you specify
	// qvbrSettings within av1Settings. Specify the general target quality level for
	// this output, from 1 to 10. Use higher numbers for greater quality. Level 10
	// results in nearly lossless compression. The quality level for most
	// broadcast-quality transcodes is between 6 and 9. Optionally, to specify a value
	// between whole numbers, also provide a value for the setting
	// qvbrQualityLevelFineTune. For example, if you want your QVBR quality level to be
	// 7.33, set qvbrQualityLevel to 7 and set qvbrQualityLevelFineTune to .33.
	QvbrQualityLevel int32

	// Optional. Specify a value here to set the QVBR quality to a level that is
	// between whole numbers. For example, if you want your QVBR quality level to be
	// 7.33, set qvbrQualityLevel to 7 and set qvbrQualityLevelFineTune to .33.
	// MediaConvert rounds your QVBR quality level to the nearest third of a whole
	// number. For example, if you set qvbrQualityLevel to 7 and you set
	// qvbrQualityLevelFineTune to .25, your actual QVBR quality level is 7.33.
	QvbrQualityLevelFineTune float64
}

// Required when you set Codec, under VideoDescription>CodecSettings to the value
// AV1.
type Av1Settings struct {

	// Specify the strength of any adaptive quantization filters that you enable. The
	// value that you choose here applies to Spatial adaptive quantization
	// (spatialAdaptiveQuantization).
	AdaptiveQuantization Av1AdaptiveQuantization

	// If you are using the console, use the Framerate setting to specify the frame
	// rate for this output. If you want to keep the same frame rate as the input
	// video, choose Follow source. If you want to do frame rate conversion, choose a
	// frame rate from the dropdown list or choose Custom. The framerates shown in the
	// dropdown list are decimal approximations of fractions. If you choose Custom,
	// specify your frame rate as a fraction. If you are creating your transcoding job
	// specification as a JSON file without the console, use FramerateControl to
	// specify which value the service uses for the frame rate for this output. Choose
	// INITIALIZE_FROM_SOURCE if you want the service to use the frame rate from the
	// input. Choose SPECIFIED if you want the service to use the frame rate you
	// specify in the settings FramerateNumerator and FramerateDenominator.
	FramerateControl Av1FramerateControl

	// Choose the method that you want MediaConvert to use when increasing or
	// decreasing the frame rate. We recommend using drop duplicate (DUPLICATE_DROP)
	// for numerically simple conversions, such as 60 fps to 30 fps. For numerically
	// complex conversions, you can use interpolate (INTERPOLATE) to avoid stutter.
	// This results in a smooth picture, but might introduce undesirable video
	// artifacts. For complex frame rate conversions, especially if your source video
	// has already been converted from its original cadence, use FrameFormer
	// (FRAMEFORMER) to do motion-compensated interpolation. FrameFormer chooses the
	// best conversion method frame by frame. Note that using FrameFormer increases the
	// transcoding time and incurs a significant add-on cost.
	FramerateConversionAlgorithm Av1FramerateConversionAlgorithm

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateDenominator to specify the denominator of this fraction. In this
	// example, use 1001 for the value of FramerateDenominator. When you use the
	// console for transcode jobs that use frame rate conversion, provide the value as
	// a decimal number for Framerate. In this example, specify 23.976.
	FramerateDenominator int32

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateNumerator to specify the numerator of this fraction. In this example,
	// use 24000 for the value of FramerateNumerator. When you use the console for
	// transcode jobs that use frame rate conversion, provide the value as a decimal
	// number for Framerate. In this example, specify 23.976.
	FramerateNumerator int32

	// Specify the GOP length (keyframe interval) in frames. With AV1, MediaConvert
	// doesn't support GOP length in seconds. This value must be greater than zero and
	// preferably equal to 1 + ((numberBFrames + 1) * x), where x is an integer value.
	GopSize float64

	// Maximum bitrate in bits/second. For example, enter five megabits per second as
	// 5000000. Required when Rate control mode is QVBR.
	MaxBitrate int32

	// Specify the number of B-frames. With AV1, MediaConvert supports only 7 or 15.
	NumberBFramesBetweenReferenceFrames int32

	// Settings for quality-defined variable bitrate encoding with the AV1 codec.
	// Required when you set Rate control mode to QVBR. Not valid when you set Rate
	// control mode to a value other than QVBR, or when you don't define Rate control
	// mode.
	QvbrSettings *Av1QvbrSettings

	// 'With AV1 outputs, for rate control mode, MediaConvert supports only
	// quality-defined variable bitrate (QVBR). You can''t use CBR or VBR.'
	RateControlMode Av1RateControlMode

	// Specify the number of slices per picture. This value must be 1, 2, 4, 8, 16, or
	// 32. For progressive pictures, this value must be less than or equal to the
	// number of macroblock rows. For interlaced pictures, this value must be less than
	// or equal to half the number of macroblock rows.
	Slices int32

	// Keep the default value, Enabled (ENABLED), to adjust quantization within each
	// frame based on spatial variation of content complexity. When you enable this
	// feature, the encoder uses fewer bits on areas that can sustain more distortion
	// with no noticeable visual degradation and uses more bits on areas where any
	// small distortion will be noticeable. For example, complex textured blocks are
	// encoded with fewer bits and smooth textured blocks are encoded with more bits.
	// Enabling this feature will almost always improve your video quality. Note,
	// though, that this feature doesn't take into account where the viewer's attention
	// is likely to be. If viewers are likely to be focusing their attention on a part
	// of the screen with a lot of complex texture, you might choose to disable this
	// feature. Related setting: When you enable spatial adaptive quantization, set the
	// value for Adaptive quantization (adaptiveQuantization) depending on your
	// content. For homogeneous content, such as cartoons and video games, set it to
	// Low. For content with a wider variety of textures, set it to High or Higher.
	SpatialAdaptiveQuantization Av1SpatialAdaptiveQuantization
}

// Settings for Avail Blanking
type AvailBlanking struct {

	// Blanking image to be used. Leave empty for solid black. Only bmp and png images
	// are supported.
	AvailBlankingImage *string
}

// Required when you set your output video codec to AVC-Intra. For more information
// about the AVC-I settings, see the relevant specification. For detailed
// information about SD and HD in AVC-I, see
// https://ieeexplore.ieee.org/document/7290936.
type AvcIntraSettings struct {

	// Specify the AVC-Intra class of your output. The AVC-Intra class selection
	// determines the output video bit rate depending on the frame rate of the output.
	// Outputs with higher class values have higher bitrates and improved image
	// quality.
	AvcIntraClass AvcIntraClass

	// If you are using the console, use the Framerate setting to specify the frame
	// rate for this output. If you want to keep the same frame rate as the input
	// video, choose Follow source. If you want to do frame rate conversion, choose a
	// frame rate from the dropdown list or choose Custom. The framerates shown in the
	// dropdown list are decimal approximations of fractions. If you choose Custom,
	// specify your frame rate as a fraction. If you are creating your transcoding job
	// specification as a JSON file without the console, use FramerateControl to
	// specify which value the service uses for the frame rate for this output. Choose
	// INITIALIZE_FROM_SOURCE if you want the service to use the frame rate from the
	// input. Choose SPECIFIED if you want the service to use the frame rate you
	// specify in the settings FramerateNumerator and FramerateDenominator.
	FramerateControl AvcIntraFramerateControl

	// Choose the method that you want MediaConvert to use when increasing or
	// decreasing the frame rate. We recommend using drop duplicate (DUPLICATE_DROP)
	// for numerically simple conversions, such as 60 fps to 30 fps. For numerically
	// complex conversions, you can use interpolate (INTERPOLATE) to avoid stutter.
	// This results in a smooth picture, but might introduce undesirable video
	// artifacts. For complex frame rate conversions, especially if your source video
	// has already been converted from its original cadence, use FrameFormer
	// (FRAMEFORMER) to do motion-compensated interpolation. FrameFormer chooses the
	// best conversion method frame by frame. Note that using FrameFormer increases the
	// transcoding time and incurs a significant add-on cost.
	FramerateConversionAlgorithm AvcIntraFramerateConversionAlgorithm

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateDenominator to specify the denominator of this fraction. In this
	// example, use 1001 for the value of FramerateDenominator. When you use the
	// console for transcode jobs that use frame rate conversion, provide the value as
	// a decimal number for Framerate. In this example, specify 23.976.
	FramerateDenominator int32

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateNumerator to specify the numerator of this fraction. In this example,
	// use 24000 for the value of FramerateNumerator. When you use the console for
	// transcode jobs that use frame rate conversion, provide the value as a decimal
	// number for Framerate. In this example, specify 23.976.
	FramerateNumerator int32

	// Choose the scan line type for the output. Keep the default value, Progressive
	// (PROGRESSIVE) to create a progressive output, regardless of the scan type of
	// your input. Use Top field first (TOP_FIELD) or Bottom field first (BOTTOM_FIELD)
	// to create an output that's interlaced with the same field polarity throughout.
	// Use Follow, default top (FOLLOW_TOP_FIELD) or Follow, default bottom
	// (FOLLOW_BOTTOM_FIELD) to produce outputs with the same field polarity as the
	// source. For jobs that have multiple inputs, the output field polarity might
	// change over the course of the output. Follow behavior depends on the input scan
	// type. If the source is interlaced, the output will be interlaced with the same
	// polarity as the source. If the source is progressive, the output will be
	// interlaced with top field bottom field first, depending on which of the Follow
	// options you choose.
	InterlaceMode AvcIntraInterlaceMode

	// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
	// second (fps). Enable slow PAL to create a 25 fps output. When you enable slow
	// PAL, MediaConvert relabels the video frames to 25 fps and resamples your audio
	// to keep it synchronized with the video. Note that enabling this setting will
	// slightly reduce the duration of your video. Required settings: You must also set
	// Framerate to 25. In your JSON job specification, set (framerateControl) to
	// (SPECIFIED), (framerateNumerator) to 25 and (framerateDenominator) to 1.
	SlowPal AvcIntraSlowPal

	// When you do frame rate conversion from 23.976 frames per second (fps) to 29.97
	// fps, and your output scan type is interlaced, you can optionally enable hard
	// telecine (HARD) to create a smoother picture. When you keep the default value,
	// None (NONE), MediaConvert does a standard frame rate conversion to 29.97 without
	// doing anything with the field polarity to create a smoother picture.
	Telecine AvcIntraTelecine
}

// Burn-In Destination Settings.
type BurninDestinationSettings struct {

	// If no explicit x_position or y_position is provided, setting alignment to
	// centered will place the captions at the bottom center of the output. Similarly,
	// setting a left alignment will align captions to the bottom left of the output.
	// If x and y positions are given in conjunction with the alignment parameter, the
	// font will be justified (either left or centered) relative to those coordinates.
	// This option is not valid for source captions that are STL, 608/embedded or
	// teletext. These source settings are already pre-defined by the caption stream.
	// All burn-in and DVB-Sub font settings must match.
	Alignment BurninSubtitleAlignment

	// Specifies the color of the rectangle behind the captions. All burn-in and
	// DVB-Sub font settings must match.
	BackgroundColor BurninSubtitleBackgroundColor

	// Specifies the opacity of the background rectangle. 255 is opaque; 0 is
	// transparent. Leaving this parameter blank is equivalent to setting it to 0
	// (transparent). All burn-in and DVB-Sub font settings must match.
	BackgroundOpacity int32

	// Specifies the color of the burned-in captions. This option is not valid for
	// source captions that are STL, 608/embedded or teletext. These source settings
	// are already pre-defined by the caption stream. All burn-in and DVB-Sub font
	// settings must match.
	FontColor BurninSubtitleFontColor

	// Specifies the opacity of the burned-in captions. 255 is opaque; 0 is
	// transparent. All burn-in and DVB-Sub font settings must match.
	FontOpacity int32

	// Font resolution in DPI (dots per inch); default is 96 dpi. All burn-in and
	// DVB-Sub font settings must match.
	FontResolution int32

	// Provide the font script, using an ISO 15924 script code, if the LanguageCode is
	// not sufficient for determining the script type. Where LanguageCode or
	// CustomLanguageCode is sufficient, use "AUTOMATIC" or leave unset. This is used
	// to help determine the appropriate font for rendering burn-in captions.
	FontScript FontScript

	// A positive integer indicates the exact font size in points. Set to 0 for
	// automatic font size selection. All burn-in and DVB-Sub font settings must match.
	FontSize int32

	// Specifies font outline color. This option is not valid for source captions that
	// are either 608/embedded or teletext. These source settings are already
	// pre-defined by the caption stream. All burn-in and DVB-Sub font settings must
	// match.
	OutlineColor BurninSubtitleOutlineColor

	// Specifies font outline size in pixels. This option is not valid for source
	// captions that are either 608/embedded or teletext. These source settings are
	// already pre-defined by the caption stream. All burn-in and DVB-Sub font settings
	// must match.
	OutlineSize int32

	// Specifies the color of the shadow cast by the captions. All burn-in and DVB-Sub
	// font settings must match.
	ShadowColor BurninSubtitleShadowColor

	// Specifies the opacity of the shadow. 255 is opaque; 0 is transparent. Leaving
	// this parameter blank is equivalent to setting it to 0 (transparent). All burn-in
	// and DVB-Sub font settings must match.
	ShadowOpacity int32

	// Specifies the horizontal offset of the shadow relative to the captions in
	// pixels. A value of -2 would result in a shadow offset 2 pixels to the left. All
	// burn-in and DVB-Sub font settings must match.
	ShadowXOffset int32

	// Specifies the vertical offset of the shadow relative to the captions in pixels.
	// A value of -2 would result in a shadow offset 2 pixels above the text. All
	// burn-in and DVB-Sub font settings must match.
	ShadowYOffset int32

	// Only applies to jobs with input captions in Teletext or STL formats. Specify
	// whether the spacing between letters in your captions is set by the captions grid
	// or varies depending on letter width. Choose fixed grid to conform to the spacing
	// specified in the captions file more accurately. Choose proportional to make the
	// text easier to read if the captions are closed caption.
	TeletextSpacing BurninSubtitleTeletextSpacing

	// Specifies the horizontal position of the caption relative to the left side of
	// the output in pixels. A value of 10 would result in the captions starting 10
	// pixels from the left of the output. If no explicit x_position is provided, the
	// horizontal caption position will be determined by the alignment parameter. This
	// option is not valid for source captions that are STL, 608/embedded or teletext.
	// These source settings are already pre-defined by the caption stream. All burn-in
	// and DVB-Sub font settings must match.
	XPosition int32

	// Specifies the vertical position of the caption relative to the top of the output
	// in pixels. A value of 10 would result in the captions starting 10 pixels from
	// the top of the output. If no explicit y_position is provided, the caption will
	// be positioned towards the bottom of the output. This option is not valid for
	// source captions that are STL, 608/embedded or teletext. These source settings
	// are already pre-defined by the caption stream. All burn-in and DVB-Sub font
	// settings must match.
	YPosition int32
}

// Description of Caption output
type CaptionDescription struct {

	// Specifies which "Caption Selector":#inputs-caption_selector to use from each
	// input when generating captions. The name should be of the format "Caption
	// Selector ", which denotes that the Nth Caption Selector will be used from each
	// input.
	CaptionSelectorName *string

	// Specify the language for this captions output track. For most captions output
	// formats, the encoder puts this language information in the output captions
	// metadata. If your output captions format is DVB-Sub or Burn in, the encoder uses
	// this language information when automatically selecting the font script for
	// rendering the captions text. For all outputs, you can use an ISO 639-2 or ISO
	// 639-3 code. For streaming outputs, you can also use any other code in the full
	// RFC-5646 specification. Streaming outputs are those that are in one of the
	// following output groups: CMAF, DASH ISO, Apple HLS, or Microsoft Smooth
	// Streaming.
	CustomLanguageCode *string

	// Specific settings required by destination type. Note that
	// burnin_destination_settings are not available if the source of the caption data
	// is Embedded or Teletext.
	DestinationSettings *CaptionDestinationSettings

	// Specify the language of this captions output track. For most captions output
	// formats, the encoder puts this language information in the output captions
	// metadata. If your output captions format is DVB-Sub or Burn in, the encoder uses
	// this language information to choose the font language for rendering the captions
	// text.
	LanguageCode LanguageCode

	// Specify a label for this set of output captions. For example, "English",
	// "Director commentary", or "track_2". For streaming outputs, MediaConvert passes
	// this information into destination manifests for display on the end-viewer's
	// player device. For outputs in other output groups, the service ignores this
	// setting.
	LanguageDescription *string
}

// Caption Description for preset
type CaptionDescriptionPreset struct {

	// Specify the language for this captions output track. For most captions output
	// formats, the encoder puts this language information in the output captions
	// metadata. If your output captions format is DVB-Sub or Burn in, the encoder uses
	// this language information when automatically selecting the font script for
	// rendering the captions text. For all outputs, you can use an ISO 639-2 or ISO
	// 639-3 code. For streaming outputs, you can also use any other code in the full
	// RFC-5646 specification. Streaming outputs are those that are in one of the
	// following output groups: CMAF, DASH ISO, Apple HLS, or Microsoft Smooth
	// Streaming.
	CustomLanguageCode *string

	// Specific settings required by destination type. Note that
	// burnin_destination_settings are not available if the source of the caption data
	// is Embedded or Teletext.
	DestinationSettings *CaptionDestinationSettings

	// Specify the language of this captions output track. For most captions output
	// formats, the encoder puts this language information in the output captions
	// metadata. If your output captions format is DVB-Sub or Burn in, the encoder uses
	// this language information to choose the font language for rendering the captions
	// text.
	LanguageCode LanguageCode

	// Specify a label for this set of output captions. For example, "English",
	// "Director commentary", or "track_2". For streaming outputs, MediaConvert passes
	// this information into destination manifests for display on the end-viewer's
	// player device. For outputs in other output groups, the service ignores this
	// setting.
	LanguageDescription *string
}

// Specific settings required by destination type. Note that
// burnin_destination_settings are not available if the source of the caption data
// is Embedded or Teletext.
type CaptionDestinationSettings struct {

	// Burn-In Destination Settings.
	BurninDestinationSettings *BurninDestinationSettings

	// Specify the format for this set of captions on this output. The default format
	// is embedded without SCTE-20. Other options are embedded with SCTE-20, burn-in,
	// DVB-sub, IMSC, SCC, SRT, teletext, TTML, and web-VTT. If you are using SCTE-20,
	// choose SCTE-20 plus embedded (SCTE20_PLUS_EMBEDDED) to create an output that
	// complies with the SCTE-43 spec. To create a non-compliant output where the
	// embedded captions come first, choose Embedded plus SCTE-20
	// (EMBEDDED_PLUS_SCTE20).
	DestinationType CaptionDestinationType

	// DVB-Sub Destination Settings
	DvbSubDestinationSettings *DvbSubDestinationSettings

	// Settings specific to embedded/ancillary caption outputs, including 608/708
	// Channel destination number.
	EmbeddedDestinationSettings *EmbeddedDestinationSettings

	// Settings specific to IMSC caption outputs.
	ImscDestinationSettings *ImscDestinationSettings

	// Settings for SCC caption output.
	SccDestinationSettings *SccDestinationSettings

	// Settings for Teletext caption output
	TeletextDestinationSettings *TeletextDestinationSettings

	// Settings specific to TTML caption outputs, including Pass style information
	// (TtmlStylePassthrough).
	TtmlDestinationSettings *TtmlDestinationSettings
}

// Set up captions in your outputs by first selecting them from your input here.
type CaptionSelector struct {

	// The specific language to extract from source, using the ISO 639-2 or ISO 639-3
	// three-letter language code. If input is SCTE-27, complete this field and/or PID
	// to select the caption language to extract. If input is DVB-Sub and output is
	// Burn-in or SMPTE-TT, complete this field and/or PID to select the caption
	// language to extract. If input is DVB-Sub that is being passed through, omit this
	// field (and PID field); there is no way to extract a specific language with
	// pass-through captions.
	CustomLanguageCode *string

	// The specific language to extract from source. If input is SCTE-27, complete this
	// field and/or PID to select the caption language to extract. If input is DVB-Sub
	// and output is Burn-in or SMPTE-TT, complete this field and/or PID to select the
	// caption language to extract. If input is DVB-Sub that is being passed through,
	// omit this field (and PID field); there is no way to extract a specific language
	// with pass-through captions.
	LanguageCode LanguageCode

	// If your input captions are SCC, TTML, STL, SMI, SRT, or IMSC in an xml file,
	// specify the URI of the input captions source file. If your input captions are
	// IMSC in an IMF package, use TrackSourceSettings instead of FileSoureSettings.
	SourceSettings *CaptionSourceSettings
}

// Ignore this setting unless your input captions format is SCC. To have the
// service compensate for differing frame rates between your input captions and
// input video, specify the frame rate of the captions file. Specify this value as
// a fraction, using the settings Framerate numerator (framerateNumerator) and
// Framerate denominator (framerateDenominator). For example, you might specify 24
// / 1 for 24 fps, 25 / 1 for 25 fps, 24000 / 1001 for 23.976 fps, or 30000 / 1001
// for 29.97 fps.
type CaptionSourceFramerate struct {

	// Specify the denominator of the fraction that represents the frame rate for the
	// setting Caption source frame rate (CaptionSourceFramerate). Use this setting
	// along with the setting Framerate numerator (framerateNumerator).
	FramerateDenominator int32

	// Specify the numerator of the fraction that represents the frame rate for the
	// setting Caption source frame rate (CaptionSourceFramerate). Use this setting
	// along with the setting Framerate denominator (framerateDenominator).
	FramerateNumerator int32
}

// If your input captions are SCC, TTML, STL, SMI, SRT, or IMSC in an xml file,
// specify the URI of the input captions source file. If your input captions are
// IMSC in an IMF package, use TrackSourceSettings instead of FileSoureSettings.
type CaptionSourceSettings struct {

	// Settings for ancillary captions source.
	AncillarySourceSettings *AncillarySourceSettings

	// DVB Sub Source Settings
	DvbSubSourceSettings *DvbSubSourceSettings

	// Settings for embedded captions Source
	EmbeddedSourceSettings *EmbeddedSourceSettings

	// If your input captions are SCC, SMI, SRT, STL, TTML, or IMSC 1.1 in an xml file,
	// specify the URI of the input caption source file. If your caption source is IMSC
	// in an IMF package, use TrackSourceSettings instead of FileSoureSettings.
	FileSourceSettings *FileSourceSettings

	// Use Source (SourceType) to identify the format of your input captions. The
	// service cannot auto-detect caption format.
	SourceType CaptionSourceType

	// Settings specific to Teletext caption sources, including Page number.
	TeletextSourceSettings *TeletextSourceSettings

	// Settings specific to caption sources that are specified by track number.
	// Currently, this is only IMSC captions in an IMF package. If your caption source
	// is IMSC 1.1 in a separate xml file, use FileSourceSettings instead of
	// TrackSourceSettings.
	TrackSourceSettings *TrackSourceSettings
}

// Channel mapping (ChannelMapping) contains the group of fields that hold the
// remixing value for each channel. Units are in dB. Acceptable values are within
// the range from -60 (mute) through 6. A setting of 0 passes the input channel
// unchanged to the output channel (no attenuation or amplification).
type ChannelMapping struct {

	// List of output channels
	OutputChannels []OutputChannelMapping
}

// Specify the details for each pair of HLS and DASH additional manifests that you
// want the service to generate for this CMAF output group. Each pair of manifests
// can reference a different subset of outputs in the group.
type CmafAdditionalManifest struct {

	// Specify a name modifier that the service adds to the name of this manifest to
	// make it different from the file names of the other main manifests in the output
	// group. For example, say that the default main manifest for your HLS group is
	// film-name.m3u8. If you enter "-no-premium" for this setting, then the file name
	// the service generates for this top-level manifest is film-name-no-premium.m3u8.
	// For HLS output groups, specify a manifestNameModifier that is different from the
	// nameModifier of the output. The service uses the output name modifier to create
	// unique names for the individual variant manifests.
	ManifestNameModifier *string

	// Specify the outputs that you want this additional top-level manifest to
	// reference.
	SelectedOutputs []string
}

// Settings for CMAF encryption
type CmafEncryptionSettings struct {

	// This is a 128-bit, 16-byte hex value represented by a 32-character text string.
	// If this parameter is not set then the Initialization Vector will follow the
	// segment number by default.
	ConstantInitializationVector *string

	// Specify the encryption scheme that you want the service to use when encrypting
	// your CMAF segments. Choose AES-CBC subsample (SAMPLE-AES) or AES_CTR (AES-CTR).
	EncryptionMethod CmafEncryptionType

	// When you use DRM with CMAF outputs, choose whether the service writes the
	// 128-bit encryption initialization vector in the HLS and DASH manifests.
	InitializationVectorInManifest CmafInitializationVectorInManifest

	// If your output group type is CMAF, use these settings when doing DRM encryption
	// with a SPEKE-compliant key provider. If your output group type is HLS, DASH, or
	// Microsoft Smooth, use the SpekeKeyProvider settings instead.
	SpekeKeyProvider *SpekeKeyProviderCmaf

	// Use these settings to set up encryption with a static key provider.
	StaticKeyProvider *StaticKeyProvider

	// Specify whether your DRM encryption key is static or from a key provider that
	// follows the SPEKE standard. For more information about SPEKE, see
	// https://docs.aws.amazon.com/speke/latest/documentation/what-is-speke.html.
	Type CmafKeyProviderType
}

// Required when you set (Type) under (OutputGroups)>(OutputGroupSettings) to
// CMAF_GROUP_SETTINGS. Each output in a CMAF Output Group may only contain a
// single video, audio, or caption output.
type CmafGroupSettings struct {

	// By default, the service creates one top-level .m3u8 HLS manifest and one top
	// -level .mpd DASH manifest for each CMAF output group in your job. These default
	// manifests reference every output in the output group. To create additional
	// top-level manifests that reference a subset of the outputs in the output group,
	// specify a list of them here. For each additional manifest that you specify, the
	// service creates one HLS manifest and one DASH manifest.
	AdditionalManifests []CmafAdditionalManifest

	// A partial URI prefix that will be put in the manifest file at the top level
	// BaseURL element. Can be used if streams are delivered from a different URL than
	// the manifest file.
	BaseUrl *string

	// When set to ENABLED, sets #EXT-X-ALLOW-CACHE:no tag, which prevents client from
	// saving media segments for later replay.
	ClientCache CmafClientCache

	// Specification to use (RFC-6381 or the default RFC-4281) during m3u8 playlist
	// generation.
	CodecSpecification CmafCodecSpecification

	// Use Destination (Destination) to specify the S3 output location and the output
	// filename base. Destination accepts format identifiers. If you do not specify the
	// base filename in the URI, the service will use the filename of the input file.
	// If your job has multiple inputs, the service uses the filename of the first
	// input file.
	Destination *string

	// Settings associated with the destination. Will vary based on the type of
	// destination
	DestinationSettings *DestinationSettings

	// DRM settings.
	Encryption *CmafEncryptionSettings

	// Length of fragments to generate (in seconds). Fragment length must be compatible
	// with GOP size and Framerate. Note that fragments will end on the next keyframe
	// after this number of seconds, so actual fragment length may be longer. When Emit
	// Single File is checked, the fragmentation is internal to a single output file
	// and it does not cause the creation of many output files as in other output
	// types.
	FragmentLength int32

	// When set to GZIP, compresses HLS playlist.
	ManifestCompression CmafManifestCompression

	// Indicates whether the output manifest should use floating point values for
	// segment duration.
	ManifestDurationFormat CmafManifestDurationFormat

	// Minimum time of initially buffered media that is needed to ensure smooth
	// playout.
	MinBufferTime int32

	// Keep this setting at the default value of 0, unless you are troubleshooting a
	// problem with how devices play back the end of your video asset. If you know that
	// player devices are hanging on the final segment of your video because the length
	// of your final segment is too short, use this setting to specify a minimum final
	// segment length, in seconds. Choose a value that is greater than or equal to 1
	// and less than your segment length. When you specify a value for this setting,
	// the encoder will combine any final segment that is shorter than the length that
	// you specify with the previous segment. For example, your segment length is 3
	// seconds and your final segment is .5 seconds without a minimum final segment
	// length; when you set the minimum final segment length to 1, your final segment
	// is 3.5 seconds.
	MinFinalSegmentLength float64

	// Specify whether your DASH profile is on-demand or main. When you choose Main
	// profile (MAIN_PROFILE), the service signals
	// urn:mpeg:dash:profile:isoff-main:2011 in your .mpd DASH manifest. When you
	// choose On-demand (ON_DEMAND_PROFILE), the service signals
	// urn:mpeg:dash:profile:isoff-on-demand:2011 in your .mpd. When you choose
	// On-demand, you must also set the output group setting Segment control
	// (SegmentControl) to Single file (SINGLE_FILE).
	MpdProfile CmafMpdProfile

	// When set to SINGLE_FILE, a single output file is generated, which is internally
	// segmented using the Fragment Length and Segment Length. When set to
	// SEGMENTED_FILES, separate segment files will be created.
	SegmentControl CmafSegmentControl

	// Use this setting to specify the length, in seconds, of each individual CMAF
	// segment. This value applies to the whole package; that is, to every output in
	// the output group. Note that segments end on the first keyframe after this number
	// of seconds, so the actual segment length might be slightly longer. If you set
	// Segment control (CmafSegmentControl) to single file, the service puts the
	// content of each output in a single file that has metadata that marks these
	// segments. If you set it to segmented files, the service creates multiple files
	// for each output, each with the content of one segment.
	SegmentLength int32

	// Include or exclude RESOLUTION attribute for video in EXT-X-STREAM-INF tag of
	// variant manifest.
	StreamInfResolution CmafStreamInfResolution

	// When set to ENABLED, a DASH MPD manifest will be generated for this output.
	WriteDashManifest CmafWriteDASHManifest

	// When set to ENABLED, an Apple HLS manifest will be generated for this output.
	WriteHlsManifest CmafWriteHLSManifest

	// When you enable Precise segment duration in DASH manifests
	// (writeSegmentTimelineInRepresentation), your DASH manifest shows precise segment
	// durations. The segment duration information appears inside the SegmentTimeline
	// element, inside SegmentTemplate at the Representation level. When this feature
	// isn't enabled, the segment durations in your DASH manifest are approximate. The
	// segment duration information appears in the duration attribute of the
	// SegmentTemplate element.
	WriteSegmentTimelineInRepresentation CmafWriteSegmentTimelineInRepresentation
}

// Settings for MP4 segments in CMAF
type CmfcSettings struct {

	// Use this setting only when you specify SCTE-35 markers from ESAM. Choose INSERT
	// to put SCTE-35 markers in this output at the insertion points that you specify
	// in an ESAM XML document. Provide the document in the setting SCC XML (sccXml).
	Scte35Esam CmfcScte35Esam

	// Ignore this setting unless you have SCTE-35 markers in your input video file.
	// Choose Passthrough (PASSTHROUGH) if you want SCTE-35 markers that appear in your
	// input to also appear in this output. Choose None (NONE) if you don't want those
	// SCTE-35 markers in this output.
	Scte35Source CmfcScte35Source
}

// Settings for color correction.
type ColorCorrector struct {

	// Brightness level.
	Brightness int32

	// Specify the color space you want for this output. The service supports
	// conversion between HDR formats, between SDR formats, from SDR to HDR, and from
	// HDR to SDR. SDR to HDR conversion doesn't upgrade the dynamic range. The
	// converted video has an HDR format, but visually appears the same as an
	// unconverted output. HDR to SDR conversion uses Elemental tone mapping technology
	// to approximate the outcome of manually regrading from HDR to SDR.
	ColorSpaceConversion ColorSpaceConversion

	// Contrast level.
	Contrast int32

	// Use these settings when you convert to the HDR 10 color space. Specify the SMPTE
	// ST 2086 Mastering Display Color Volume static metadata that you want signaled in
	// the output. These values don't affect the pixel values that are encoded in the
	// video stream. They are intended to help the downstream video player display
	// content in a way that reflects the intentions of the the content creator. When
	// you set Color space conversion (ColorSpaceConversion) to HDR 10 (FORCE_HDR10),
	// these settings are required. You must set values for Max frame average light
	// level (maxFrameAverageLightLevel) and Max content light level
	// (maxContentLightLevel); these settings don't have a default value. The default
	// values for the other HDR 10 metadata settings are defined by the P3D65 color
	// space. For more information about MediaConvert HDR jobs, see
	// https://docs.aws.amazon.com/console/mediaconvert/hdr.
	Hdr10Metadata *Hdr10Metadata

	// Hue in degrees.
	Hue int32

	// Saturation level.
	Saturation int32
}

// Container specific settings.
type ContainerSettings struct {

	// Settings for MP4 segments in CMAF
	CmfcSettings *CmfcSettings

	// Container for this output. Some containers require a container settings object.
	// If not specified, the default object will be created.
	Container ContainerType

	// Settings for F4v container
	F4vSettings *F4vSettings

	// MPEG-2 TS container settings. These apply to outputs in a File output group when
	// the output's container (ContainerType) is MPEG-2 Transport Stream (M2TS). In
	// these assets, data is organized by the program map table (PMT). Each transport
	// stream program contains subsets of data, including audio, video, and metadata.
	// Each of these subsets of data has a numerical label called a packet identifier
	// (PID). Each transport stream program corresponds to one MediaConvert output. The
	// PMT lists the types of data in a program along with their PID. Downstream
	// systems and players use the program map table to look up the PID for each type
	// of data it accesses and then uses the PIDs to locate specific data within the
	// asset.
	M2tsSettings *M2tsSettings

	// Settings for TS segments in HLS
	M3u8Settings *M3u8Settings

	// Settings for MOV Container.
	MovSettings *MovSettings

	// Settings for MP4 container. You can create audio-only AAC outputs with this
	// container.
	Mp4Settings *Mp4Settings

	// Settings for MP4 segments in DASH
	MpdSettings *MpdSettings

	// MXF settings
	MxfSettings *MxfSettings
}

// Specify the details for each additional DASH manifest that you want the service
// to generate for this output group. Each manifest can reference a different
// subset of outputs in the group.
type DashAdditionalManifest struct {

	// Specify a name modifier that the service adds to the name of this manifest to
	// make it different from the file names of the other main manifests in the output
	// group. For example, say that the default main manifest for your DASH group is
	// film-name.mpd. If you enter "-no-premium" for this setting, then the file name
	// the service generates for this top-level manifest is film-name-no-premium.mpd.
	ManifestNameModifier *string

	// Specify the outputs that you want this additional top-level manifest to
	// reference.
	SelectedOutputs []string
}

// Specifies DRM settings for DASH outputs.
type DashIsoEncryptionSettings struct {

	// This setting can improve the compatibility of your output with video players on
	// obsolete devices. It applies only to DASH H.264 outputs with DRM encryption.
	// Choose Unencrypted SEI (UNENCRYPTED_SEI) only to correct problems with playback
	// on older devices. Otherwise, keep the default setting CENC v1 (CENC_V1). If you
	// choose Unencrypted SEI, for that output, the service will exclude the access
	// unit delimiter and will leave the SEI NAL units unencrypted.
	PlaybackDeviceCompatibility DashIsoPlaybackDeviceCompatibility

	// If your output group type is HLS, DASH, or Microsoft Smooth, use these settings
	// when doing DRM encryption with a SPEKE-compliant key provider. If your output
	// group type is CMAF, use the SpekeKeyProviderCmaf settings instead.
	SpekeKeyProvider *SpekeKeyProvider
}

// Required when you set (Type) under (OutputGroups)>(OutputGroupSettings) to
// DASH_ISO_GROUP_SETTINGS.
type DashIsoGroupSettings struct {

	// By default, the service creates one .mpd DASH manifest for each DASH ISO output
	// group in your job. This default manifest references every output in the output
	// group. To create additional DASH manifests that reference a subset of the
	// outputs in the output group, specify a list of them here.
	AdditionalManifests []DashAdditionalManifest

	// A partial URI prefix that will be put in the manifest (.mpd) file at the top
	// level BaseURL element. Can be used if streams are delivered from a different URL
	// than the manifest file.
	BaseUrl *string

	// Use Destination (Destination) to specify the S3 output location and the output
	// filename base. Destination accepts format identifiers. If you do not specify the
	// base filename in the URI, the service will use the filename of the input file.
	// If your job has multiple inputs, the service uses the filename of the first
	// input file.
	Destination *string

	// Settings associated with the destination. Will vary based on the type of
	// destination
	DestinationSettings *DestinationSettings

	// DRM settings.
	Encryption *DashIsoEncryptionSettings

	// Length of fragments to generate (in seconds). Fragment length must be compatible
	// with GOP size and Framerate. Note that fragments will end on the next keyframe
	// after this number of seconds, so actual fragment length may be longer. When Emit
	// Single File is checked, the fragmentation is internal to a single output file
	// and it does not cause the creation of many output files as in other output
	// types.
	FragmentLength int32

	// Supports HbbTV specification as indicated
	HbbtvCompliance DashIsoHbbtvCompliance

	// Minimum time of initially buffered media that is needed to ensure smooth
	// playout.
	MinBufferTime int32

	// Specify whether your DASH profile is on-demand or main. When you choose Main
	// profile (MAIN_PROFILE), the service signals
	// urn:mpeg:dash:profile:isoff-main:2011 in your .mpd DASH manifest. When you
	// choose On-demand (ON_DEMAND_PROFILE), the service signals
	// urn:mpeg:dash:profile:isoff-on-demand:2011 in your .mpd. When you choose
	// On-demand, you must also set the output group setting Segment control
	// (SegmentControl) to Single file (SINGLE_FILE).
	MpdProfile DashIsoMpdProfile

	// When set to SINGLE_FILE, a single output file is generated, which is internally
	// segmented using the Fragment Length and Segment Length. When set to
	// SEGMENTED_FILES, separate segment files will be created.
	SegmentControl DashIsoSegmentControl

	// Length of mpd segments to create (in seconds). Note that segments will end on
	// the next keyframe after this number of seconds, so actual segment length may be
	// longer. When Emit Single File is checked, the segmentation is internal to a
	// single output file and it does not cause the creation of many output files as in
	// other output types.
	SegmentLength int32

	// If you get an HTTP error in the 400 range when you play back your DASH output,
	// enable this setting and run your transcoding job again. When you enable this
	// setting, the service writes precise segment durations in the DASH manifest. The
	// segment duration information appears inside the SegmentTimeline element, inside
	// SegmentTemplate at the Representation level. When you don't enable this setting,
	// the service writes approximate segment durations in your DASH manifest.
	WriteSegmentTimelineInRepresentation DashIsoWriteSegmentTimelineInRepresentation
}

// Settings for deinterlacer
type Deinterlacer struct {

	// Only applies when you set Deinterlacer (DeinterlaceMode) to Deinterlace
	// (DEINTERLACE) or Adaptive (ADAPTIVE). Motion adaptive interpolate (INTERPOLATE)
	// produces sharper pictures, while blend (BLEND) produces smoother motion. Use
	// (INTERPOLATE_TICKER) OR (BLEND_TICKER) if your source file includes a ticker,
	// such as a scrolling headline at the bottom of the frame.
	Algorithm DeinterlaceAlgorithm

	// * When set to NORMAL (default), the deinterlacer does not convert frames that
	// are tagged in metadata as progressive. It will only convert those that are
	// tagged as some other type. - When set to FORCE_ALL_FRAMES, the deinterlacer
	// converts every frame to progressive - even those that are already tagged as
	// progressive. Turn Force mode on only if there is a good chance that the metadata
	// has tagged frames as progressive when they are not progressive. Do not turn on
	// otherwise; processing frames that are already progressive into progressive will
	// probably result in lower quality video.
	Control DeinterlacerControl

	// Use Deinterlacer (DeinterlaceMode) to choose how the service will do
	// deinterlacing. Default is Deinterlace. - Deinterlace converts interlaced to
	// progressive. - Inverse telecine converts Hard Telecine 29.97i to progressive
	// 23.976p. - Adaptive auto-detects and converts to progressive.
	Mode DeinterlacerMode
}

// Settings associated with the destination. Will vary based on the type of
// destination
type DestinationSettings struct {

	// Settings associated with S3 destination
	S3Settings *S3DestinationSettings
}

// Settings for Dolby Vision
type DolbyVision struct {

	// Use these settings when you set DolbyVisionLevel6Mode to SPECIFY to override the
	// MaxCLL and MaxFALL values in your input with new values.
	L6Metadata *DolbyVisionLevel6Metadata

	// Use Dolby Vision Mode to choose how the service will handle Dolby Vision MaxCLL
	// and MaxFALL properies.
	L6Mode DolbyVisionLevel6Mode

	// In the current MediaConvert implementation, the Dolby Vision profile is always 5
	// (PROFILE_5). Therefore, all of your inputs must contain Dolby Vision frame
	// interleaved data.
	Profile DolbyVisionProfile
}

// Use these settings when you set DolbyVisionLevel6Mode to SPECIFY to override the
// MaxCLL and MaxFALL values in your input with new values.
type DolbyVisionLevel6Metadata struct {

	// Maximum Content Light Level. Static HDR metadata that corresponds to the
	// brightest pixel in the entire stream. Measured in nits.
	MaxCll int32

	// Maximum Frame-Average Light Level. Static HDR metadata that corresponds to the
	// highest frame-average brightness in the entire stream. Measured in nits.
	MaxFall int32
}

// Inserts DVB Network Information Table (NIT) at the specified table repetition
// interval.
type DvbNitSettings struct {

	// The numeric value placed in the Network Information Table (NIT).
	NetworkId int32

	// The network name text placed in the network_name_descriptor inside the Network
	// Information Table. Maximum length is 256 characters.
	NetworkName *string

	// The number of milliseconds between instances of this table in the output
	// transport stream.
	NitInterval int32
}

// Inserts DVB Service Description Table (NIT) at the specified table repetition
// interval.
type DvbSdtSettings struct {

	// Selects method of inserting SDT information into output stream. "Follow input
	// SDT" copies SDT information from input stream to output stream. "Follow input
	// SDT if present" copies SDT information from input stream to output stream if SDT
	// information is present in the input, otherwise it will fall back on the
	// user-defined values. Enter "SDT Manually" means user will enter the SDT
	// information. "No SDT" means output stream will not contain SDT information.
	OutputSdt OutputSdt

	// The number of milliseconds between instances of this table in the output
	// transport stream.
	SdtInterval int32

	// The service name placed in the service_descriptor in the Service Description
	// Table. Maximum length is 256 characters.
	ServiceName *string

	// The service provider name placed in the service_descriptor in the Service
	// Description Table. Maximum length is 256 characters.
	ServiceProviderName *string
}

// DVB-Sub Destination Settings
type DvbSubDestinationSettings struct {

	// If no explicit x_position or y_position is provided, setting alignment to
	// centered will place the captions at the bottom center of the output. Similarly,
	// setting a left alignment will align captions to the bottom left of the output.
	// If x and y positions are given in conjunction with the alignment parameter, the
	// font will be justified (either left or centered) relative to those coordinates.
	// This option is not valid for source captions that are STL, 608/embedded or
	// teletext. These source settings are already pre-defined by the caption stream.
	// All burn-in and DVB-Sub font settings must match.
	Alignment DvbSubtitleAlignment

	// Specifies the color of the rectangle behind the captions. All burn-in and
	// DVB-Sub font settings must match.
	BackgroundColor DvbSubtitleBackgroundColor

	// Specifies the opacity of the background rectangle. 255 is opaque; 0 is
	// transparent. Leaving this parameter blank is equivalent to setting it to 0
	// (transparent). All burn-in and DVB-Sub font settings must match.
	BackgroundOpacity int32

	// Specifies the color of the burned-in captions. This option is not valid for
	// source captions that are STL, 608/embedded or teletext. These source settings
	// are already pre-defined by the caption stream. All burn-in and DVB-Sub font
	// settings must match.
	FontColor DvbSubtitleFontColor

	// Specifies the opacity of the burned-in captions. 255 is opaque; 0 is
	// transparent. All burn-in and DVB-Sub font settings must match.
	FontOpacity int32

	// Font resolution in DPI (dots per inch); default is 96 dpi. All burn-in and
	// DVB-Sub font settings must match.
	FontResolution int32

	// Provide the font script, using an ISO 15924 script code, if the LanguageCode is
	// not sufficient for determining the script type. Where LanguageCode or
	// CustomLanguageCode is sufficient, use "AUTOMATIC" or leave unset. This is used
	// to help determine the appropriate font for rendering DVB-Sub captions.
	FontScript FontScript

	// A positive integer indicates the exact font size in points. Set to 0 for
	// automatic font size selection. All burn-in and DVB-Sub font settings must match.
	FontSize int32

	// Specifies font outline color. This option is not valid for source captions that
	// are either 608/embedded or teletext. These source settings are already
	// pre-defined by the caption stream. All burn-in and DVB-Sub font settings must
	// match.
	OutlineColor DvbSubtitleOutlineColor

	// Specifies font outline size in pixels. This option is not valid for source
	// captions that are either 608/embedded or teletext. These source settings are
	// already pre-defined by the caption stream. All burn-in and DVB-Sub font settings
	// must match.
	OutlineSize int32

	// Specifies the color of the shadow cast by the captions. All burn-in and DVB-Sub
	// font settings must match.
	ShadowColor DvbSubtitleShadowColor

	// Specifies the opacity of the shadow. 255 is opaque; 0 is transparent. Leaving
	// this parameter blank is equivalent to setting it to 0 (transparent). All burn-in
	// and DVB-Sub font settings must match.
	ShadowOpacity int32

	// Specifies the horizontal offset of the shadow relative to the captions in
	// pixels. A value of -2 would result in a shadow offset 2 pixels to the left. All
	// burn-in and DVB-Sub font settings must match.
	ShadowXOffset int32

	// Specifies the vertical offset of the shadow relative to the captions in pixels.
	// A value of -2 would result in a shadow offset 2 pixels above the text. All
	// burn-in and DVB-Sub font settings must match.
	ShadowYOffset int32

	// Specify whether your DVB subtitles are standard or for hearing impaired. Choose
	// hearing impaired if your subtitles include audio descriptions and dialogue.
	// Choose standard if your subtitles include only dialogue.
	SubtitlingType DvbSubtitlingType

	// Only applies to jobs with input captions in Teletext or STL formats. Specify
	// whether the spacing between letters in your captions is set by the captions grid
	// or varies depending on letter width. Choose fixed grid to conform to the spacing
	// specified in the captions file more accurately. Choose proportional to make the
	// text easier to read if the captions are closed caption.
	TeletextSpacing DvbSubtitleTeletextSpacing

	// Specifies the horizontal position of the caption relative to the left side of
	// the output in pixels. A value of 10 would result in the captions starting 10
	// pixels from the left of the output. If no explicit x_position is provided, the
	// horizontal caption position will be determined by the alignment parameter. This
	// option is not valid for source captions that are STL, 608/embedded or teletext.
	// These source settings are already pre-defined by the caption stream. All burn-in
	// and DVB-Sub font settings must match.
	XPosition int32

	// Specifies the vertical position of the caption relative to the top of the output
	// in pixels. A value of 10 would result in the captions starting 10 pixels from
	// the top of the output. If no explicit y_position is provided, the caption will
	// be positioned towards the bottom of the output. This option is not valid for
	// source captions that are STL, 608/embedded or teletext. These source settings
	// are already pre-defined by the caption stream. All burn-in and DVB-Sub font
	// settings must match.
	YPosition int32
}

// DVB Sub Source Settings
type DvbSubSourceSettings struct {

	// When using DVB-Sub with Burn-In or SMPTE-TT, use this PID for the source
	// content. Unused for DVB-Sub passthrough. All DVB-Sub content is passed through,
	// regardless of selectors.
	Pid int32
}

// Inserts DVB Time and Date Table (TDT) at the specified table repetition
// interval.
type DvbTdtSettings struct {

	// The number of milliseconds between instances of this table in the output
	// transport stream.
	TdtInterval int32
}

// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
// value EAC3_ATMOS.
type Eac3AtmosSettings struct {

	// Specify the average bitrate in bits per second. Valid values: 384k, 448k, 640k,
	// 768k
	Bitrate int32

	// Specify the bitstream mode for the E-AC-3 stream that the encoder emits. For
	// more information about the EAC3 bitstream mode, see ATSC A/52-2012 (Annex E).
	BitstreamMode Eac3AtmosBitstreamMode

	// The coding mode for Dolby Digital Plus JOC (Atmos) is always 9.1.6
	// (CODING_MODE_9_1_6).
	CodingMode Eac3AtmosCodingMode

	// Enable Dolby Dialogue Intelligence to adjust loudness based on dialogue
	// analysis.
	DialogueIntelligence Eac3AtmosDialogueIntelligence

	// Specify the absolute peak level for a signal with dynamic range compression.
	DynamicRangeCompressionLine Eac3AtmosDynamicRangeCompressionLine

	// Specify how the service limits the audio dynamic range when compressing the
	// audio.
	DynamicRangeCompressionRf Eac3AtmosDynamicRangeCompressionRf

	// Specify a value for the following Dolby Atmos setting: Left only/Right only
	// center mix (Lo/Ro center). MediaConvert uses this value for downmixing. How the
	// service uses this value depends on the value that you choose for Stereo downmix
	// (Eac3AtmosStereoDownmix). Valid values: 3.0, 1.5, 0.0, -1.5, -3.0, -4.5, and
	// -6.0.
	LoRoCenterMixLevel float64

	// Specify a value for the following Dolby Atmos setting: Left only/Right only
	// (Lo/Ro surround). MediaConvert uses this value for downmixing. How the service
	// uses this value depends on the value that you choose for Stereo downmix
	// (Eac3AtmosStereoDownmix). Valid values: -1.5, -3.0, -4.5, -6.0, and -60. The
	// value -60 mutes the channel.
	LoRoSurroundMixLevel float64

	// Specify a value for the following Dolby Atmos setting: Left total/Right total
	// center mix (Lt/Rt center). MediaConvert uses this value for downmixing. How the
	// service uses this value depends on the value that you choose for Stereo downmix
	// (Eac3AtmosStereoDownmix). Valid values: 3.0, 1.5, 0.0, -1.5, -3.0, -4.5, and
	// -6.0.
	LtRtCenterMixLevel float64

	// Specify a value for the following Dolby Atmos setting: Left total/Right total
	// surround mix (Lt/Rt surround). MediaConvert uses this value for downmixing. How
	// the service uses this value depends on the value that you choose for Stereo
	// downmix (Eac3AtmosStereoDownmix). Valid values: -1.5, -3.0, -4.5, -6.0, and -60.
	// The value -60 mutes the channel.
	LtRtSurroundMixLevel float64

	// Choose how the service meters the loudness of your audio.
	MeteringMode Eac3AtmosMeteringMode

	// This value is always 48000. It represents the sample rate in Hz.
	SampleRate int32

	// Specify the percentage of audio content that must be speech before the encoder
	// uses the measured speech loudness as the overall program loudness.
	SpeechThreshold int32

	// Choose how the service does stereo downmixing.
	StereoDownmix Eac3AtmosStereoDownmix

	// Specify whether your input audio has an additional center rear surround channel
	// matrix encoded into your left and right surround channels.
	SurroundExMode Eac3AtmosSurroundExMode
}

// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
// value EAC3.
type Eac3Settings struct {

	// If set to ATTENUATE_3_DB, applies a 3 dB attenuation to the surround channels.
	// Only used for 3/2 coding mode.
	AttenuationControl Eac3AttenuationControl

	// Specify the average bitrate in bits per second. Valid bitrates depend on the
	// coding mode.
	Bitrate int32

	// Specify the bitstream mode for the E-AC-3 stream that the encoder emits. For
	// more information about the EAC3 bitstream mode, see ATSC A/52-2012 (Annex E).
	BitstreamMode Eac3BitstreamMode

	// Dolby Digital Plus coding mode. Determines number of channels.
	CodingMode Eac3CodingMode

	// Activates a DC highpass filter for all input channels.
	DcFilter Eac3DcFilter

	// Sets the dialnorm for the output. If blank and input audio is Dolby Digital
	// Plus, dialnorm will be passed through.
	Dialnorm int32

	// Specify the absolute peak level for a signal with dynamic range compression.
	DynamicRangeCompressionLine Eac3DynamicRangeCompressionLine

	// Specify how the service limits the audio dynamic range when compressing the
	// audio.
	DynamicRangeCompressionRf Eac3DynamicRangeCompressionRf

	// When encoding 3/2 audio, controls whether the LFE channel is enabled
	LfeControl Eac3LfeControl

	// Applies a 120Hz lowpass filter to the LFE channel prior to encoding. Only valid
	// with 3_2_LFE coding mode.
	LfeFilter Eac3LfeFilter

	// Specify a value for the following Dolby Digital Plus setting: Left only/Right
	// only center mix (Lo/Ro center). MediaConvert uses this value for downmixing. How
	// the service uses this value depends on the value that you choose for Stereo
	// downmix (Eac3StereoDownmix). Valid values: 3.0, 1.5, 0.0, -1.5, -3.0, -4.5,
	// -6.0, and -60. The value -60 mutes the channel. This setting applies only if you
	// keep the default value of 3/2 - L, R, C, Ls, Rs (CODING_MODE_3_2) for the
	// setting Coding mode (Eac3CodingMode). If you choose a different value for Coding
	// mode, the service ignores Left only/Right only center (loRoCenterMixLevel).
	LoRoCenterMixLevel float64

	// Specify a value for the following Dolby Digital Plus setting: Left only/Right
	// only (Lo/Ro surround). MediaConvert uses this value for downmixing. How the
	// service uses this value depends on the value that you choose for Stereo downmix
	// (Eac3StereoDownmix). Valid values: -1.5, -3.0, -4.5, -6.0, and -60. The value
	// -60 mutes the channel. This setting applies only if you keep the default value
	// of 3/2 - L, R, C, Ls, Rs (CODING_MODE_3_2) for the setting Coding mode
	// (Eac3CodingMode). If you choose a different value for Coding mode, the service
	// ignores Left only/Right only surround (loRoSurroundMixLevel).
	LoRoSurroundMixLevel float64

	// Specify a value for the following Dolby Digital Plus setting: Left total/Right
	// total center mix (Lt/Rt center). MediaConvert uses this value for downmixing.
	// How the service uses this value depends on the value that you choose for Stereo
	// downmix (Eac3StereoDownmix). Valid values: 3.0, 1.5, 0.0, -1.5, -3.0, -4.5,
	// -6.0, and -60. The value -60 mutes the channel. This setting applies only if you
	// keep the default value of 3/2 - L, R, C, Ls, Rs (CODING_MODE_3_2) for the
	// setting Coding mode (Eac3CodingMode). If you choose a different value for Coding
	// mode, the service ignores Left total/Right total center (ltRtCenterMixLevel).
	LtRtCenterMixLevel float64

	// Specify a value for the following Dolby Digital Plus setting: Left total/Right
	// total surround mix (Lt/Rt surround). MediaConvert uses this value for
	// downmixing. How the service uses this value depends on the value that you choose
	// for Stereo downmix (Eac3StereoDownmix). Valid values: -1.5, -3.0, -4.5, -6.0,
	// and -60. The value -60 mutes the channel. This setting applies only if you keep
	// the default value of 3/2 - L, R, C, Ls, Rs (CODING_MODE_3_2) for the setting
	// Coding mode (Eac3CodingMode). If you choose a different value for Coding mode,
	// the service ignores Left total/Right total surround (ltRtSurroundMixLevel).
	LtRtSurroundMixLevel float64

	// When set to FOLLOW_INPUT, encoder metadata will be sourced from the DD, DD+, or
	// DolbyE decoder that supplied this audio data. If audio was not supplied from one
	// of these streams, then the static metadata settings will be used.
	MetadataControl Eac3MetadataControl

	// When set to WHEN_POSSIBLE, input DD+ audio will be passed through if it is
	// present on the input. this detection is dynamic over the life of the transcode.
	// Inputs that alternate between DD+ and non-DD+ content will have a consistent DD+
	// output as the system alternates between passthrough and encoding.
	PassthroughControl Eac3PassthroughControl

	// Controls the amount of phase-shift applied to the surround channels. Only used
	// for 3/2 coding mode.
	PhaseControl Eac3PhaseControl

	// This value is always 48000. It represents the sample rate in Hz.
	SampleRate int32

	// Choose how the service does stereo downmixing. This setting only applies if you
	// keep the default value of 3/2 - L, R, C, Ls, Rs (CODING_MODE_3_2) for the
	// setting Coding mode (Eac3CodingMode). If you choose a different value for Coding
	// mode, the service ignores Stereo downmix (Eac3StereoDownmix).
	StereoDownmix Eac3StereoDownmix

	// When encoding 3/2 audio, sets whether an extra center back surround channel is
	// matrix encoded into the left and right surround channels.
	SurroundExMode Eac3SurroundExMode

	// When encoding 2/0 audio, sets whether Dolby Surround is matrix encoded into the
	// two channels.
	SurroundMode Eac3SurroundMode
}

// Settings specific to embedded/ancillary caption outputs, including 608/708
// Channel destination number.
type EmbeddedDestinationSettings struct {

	// Ignore this setting unless your input captions are SCC format and your output
	// captions are embedded in the video stream. Specify a CC number for each captions
	// channel in this output. If you have two channels, choose CC numbers that aren't
	// in the same field. For example, choose 1 and 3. For more information, see
	// https://docs.aws.amazon.com/console/mediaconvert/dual-scc-to-embedded.
	Destination608ChannelNumber int32

	// Ignore this setting unless your input captions are SCC format and you want both
	// 608 and 708 captions embedded in your output stream. Optionally, specify the 708
	// service number for each output captions channel. Choose a different number for
	// each channel. To use this setting, also set Force 608 to 708 upconvert
	// (Convert608To708) to Upconvert (UPCONVERT) in your input captions selector
	// settings. If you choose to upconvert but don't specify a 708 service number,
	// MediaConvert uses the number that you specify for CC channel number
	// (destination608ChannelNumber) for the 708 service number. For more information,
	// see https://docs.aws.amazon.com/console/mediaconvert/dual-scc-to-embedded.
	Destination708ServiceNumber int32
}

// Settings for embedded captions Source
type EmbeddedSourceSettings struct {

	// Specify whether this set of input captions appears in your outputs in both 608
	// and 708 format. If you choose Upconvert (UPCONVERT), MediaConvert includes the
	// captions data in two ways: it passes the 608 data through using the 608
	// compatibility bytes fields of the 708 wrapper, and it also translates the 608
	// data into 708.
	Convert608To708 EmbeddedConvert608To708

	// Specifies the 608/708 channel number within the video track from which to
	// extract captions. Unused for passthrough.
	Source608ChannelNumber int32

	// Specifies the video track index used for extracting captions. The system only
	// supports one input video track, so this should always be set to '1'.
	Source608TrackNumber int32

	// By default, the service terminates any unterminated captions at the end of each
	// input. If you want the caption to continue onto your next input, disable this
	// setting.
	TerminateCaptions EmbeddedTerminateCaptions
}

// Describes an account-specific API endpoint.
type Endpoint struct {

	// URL of endpoint
	Url *string
}

// ESAM ManifestConfirmConditionNotification defined by OC-SP-ESAM-API-I03-131025.
type EsamManifestConfirmConditionNotification struct {

	// Provide your ESAM ManifestConfirmConditionNotification XML document inside your
	// JSON job settings. Form the XML document as per OC-SP-ESAM-API-I03-131025. The
	// transcoder will use the Manifest Conditioning instructions in the message that
	// you supply.
	MccXml *string
}

// Settings for Event Signaling And Messaging (ESAM). If you don't do ad insertion,
// you can ignore these settings.
type EsamSettings struct {

	// Specifies an ESAM ManifestConfirmConditionNotification XML as per
	// OC-SP-ESAM-API-I03-131025. The transcoder uses the manifest conditioning
	// instructions that you provide in the setting MCC XML (mccXml).
	ManifestConfirmConditionNotification *EsamManifestConfirmConditionNotification

	// Specifies the stream distance, in milliseconds, between the SCTE 35 messages
	// that the transcoder places and the splice points that they refer to. If the time
	// between the start of the asset and the SCTE-35 message is less than this value,
	// then the transcoder places the SCTE-35 marker at the beginning of the stream.
	ResponseSignalPreroll int32

	// Specifies an ESAM SignalProcessingNotification XML as per
	// OC-SP-ESAM-API-I03-131025. The transcoder uses the signal processing
	// instructions that you provide in the setting SCC XML (sccXml).
	SignalProcessingNotification *EsamSignalProcessingNotification
}

// ESAM SignalProcessingNotification data defined by OC-SP-ESAM-API-I03-131025.
type EsamSignalProcessingNotification struct {

	// Provide your ESAM SignalProcessingNotification XML document inside your JSON job
	// settings. Form the XML document as per OC-SP-ESAM-API-I03-131025. The transcoder
	// will use the signal processing instructions in the message that you supply.
	// Provide your ESAM SignalProcessingNotification XML document inside your JSON job
	// settings. For your MPEG2-TS file outputs, if you want the service to place
	// SCTE-35 markers at the insertion points you specify in the XML document, you
	// must also enable SCTE-35 ESAM (scte35Esam). Note that you can either specify an
	// ESAM XML document or enable SCTE-35 passthrough. You can't do both.
	SccXml *string
}

// Settings for F4v container
type F4vSettings struct {

	// If set to PROGRESSIVE_DOWNLOAD, the MOOV atom is relocated to the beginning of
	// the archive as required for progressive downloading. Otherwise it is placed
	// normally at the end.
	MoovPlacement F4vMoovPlacement
}

// Required when you set (Type) under (OutputGroups)>(OutputGroupSettings) to
// FILE_GROUP_SETTINGS.
type FileGroupSettings struct {

	// Use Destination (Destination) to specify the S3 output location and the output
	// filename base. Destination accepts format identifiers. If you do not specify the
	// base filename in the URI, the service will use the filename of the input file.
	// If your job has multiple inputs, the service uses the filename of the first
	// input file.
	Destination *string

	// Settings associated with the destination. Will vary based on the type of
	// destination
	DestinationSettings *DestinationSettings
}

// If your input captions are SCC, SMI, SRT, STL, TTML, or IMSC 1.1 in an xml file,
// specify the URI of the input caption source file. If your caption source is IMSC
// in an IMF package, use TrackSourceSettings instead of FileSoureSettings.
type FileSourceSettings struct {

	// Specify whether this set of input captions appears in your outputs in both 608
	// and 708 format. If you choose Upconvert (UPCONVERT), MediaConvert includes the
	// captions data in two ways: it passes the 608 data through using the 608
	// compatibility bytes fields of the 708 wrapper, and it also translates the 608
	// data into 708.
	Convert608To708 FileSourceConvert608To708

	// Ignore this setting unless your input captions format is SCC. To have the
	// service compensate for differing frame rates between your input captions and
	// input video, specify the frame rate of the captions file. Specify this value as
	// a fraction, using the settings Framerate numerator (framerateNumerator) and
	// Framerate denominator (framerateDenominator). For example, you might specify 24
	// / 1 for 24 fps, 25 / 1 for 25 fps, 24000 / 1001 for 23.976 fps, or 30000 / 1001
	// for 29.97 fps.
	Framerate *CaptionSourceFramerate

	// External caption file used for loading captions. Accepted file extensions are
	// 'scc', 'ttml', 'dfxp', 'stl', 'srt', 'xml', and 'smi'.
	SourceFile *string

	// Specifies a time delta in seconds to offset the captions from the source file.
	TimeDelta int32
}

// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
// value FRAME_CAPTURE.
type FrameCaptureSettings struct {

	// Frame capture will encode the first frame of the output stream, then one frame
	// every framerateDenominator/framerateNumerator seconds. For example, settings of
	// framerateNumerator = 1 and framerateDenominator = 3 (a rate of 1/3 frame per
	// second) will capture the first frame, then 1 frame every 3s. Files will be named
	// as filename.n.jpg where n is the 0-based sequence number of each Capture.
	FramerateDenominator int32

	// Frame capture will encode the first frame of the output stream, then one frame
	// every framerateDenominator/framerateNumerator seconds. For example, settings of
	// framerateNumerator = 1 and framerateDenominator = 3 (a rate of 1/3 frame per
	// second) will capture the first frame, then 1 frame every 3s. Files will be named
	// as filename.NNNNNNN.jpg where N is the 0-based frame sequence number zero padded
	// to 7 decimal places.
	FramerateNumerator int32

	// Maximum number of captures (encoded jpg output files).
	MaxCaptures int32

	// JPEG Quality - a higher value equals higher quality.
	Quality int32
}

// Settings for quality-defined variable bitrate encoding with the H.264 codec.
// Required when you set Rate control mode to QVBR. Not valid when you set Rate
// control mode to a value other than QVBR, or when you don't define Rate control
// mode.
type H264QvbrSettings struct {

	// Use this setting only when Rate control mode is QVBR and Quality tuning level is
	// Multi-pass HQ. For Max average bitrate values suited to the complexity of your
	// input video, the service limits the average bitrate of the video part of this
	// output to the value that you choose. That is, the total size of the video
	// element is less than or equal to the value you set multiplied by the number of
	// seconds of encoded output.
	MaxAverageBitrate int32

	// Required when you use QVBR rate control mode. That is, when you specify
	// qvbrSettings within h264Settings. Specify the general target quality level for
	// this output, from 1 to 10. Use higher numbers for greater quality. Level 10
	// results in nearly lossless compression. The quality level for most
	// broadcast-quality transcodes is between 6 and 9. Optionally, to specify a value
	// between whole numbers, also provide a value for the setting
	// qvbrQualityLevelFineTune. For example, if you want your QVBR quality level to be
	// 7.33, set qvbrQualityLevel to 7 and set qvbrQualityLevelFineTune to .33.
	QvbrQualityLevel int32

	// Optional. Specify a value here to set the QVBR quality to a level that is
	// between whole numbers. For example, if you want your QVBR quality level to be
	// 7.33, set qvbrQualityLevel to 7 and set qvbrQualityLevelFineTune to .33.
	// MediaConvert rounds your QVBR quality level to the nearest third of a whole
	// number. For example, if you set qvbrQualityLevel to 7 and you set
	// qvbrQualityLevelFineTune to .25, your actual QVBR quality level is 7.33.
	QvbrQualityLevelFineTune float64
}

// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
// value H_264.
type H264Settings struct {

	// Keep the default value, Auto (AUTO), for this setting to have MediaConvert
	// automatically apply the best types of quantization for your video content. When
	// you want to apply your quantization settings manually, you must set
	// H264AdaptiveQuantization to a value other than Auto (AUTO). Use this setting to
	// specify the strength of any adaptive quantization filters that you enable. If
	// you don't want MediaConvert to do any adaptive quantization in this transcode,
	// set Adaptive quantization (H264AdaptiveQuantization) to Off (OFF). Related
	// settings: The value that you choose here applies to the following settings:
	// H264FlickerAdaptiveQuantization, H264SpatialAdaptiveQuantization, and
	// H264TemporalAdaptiveQuantization.
	AdaptiveQuantization H264AdaptiveQuantization

	// Specify the average bitrate in bits per second. Required for VBR and CBR. For MS
	// Smooth outputs, bitrates must be unique when rounded down to the nearest
	// multiple of 1000.
	Bitrate int32

	// Specify an H.264 level that is consistent with your output video settings. If
	// you aren't sure what level to specify, choose Auto (AUTO).
	CodecLevel H264CodecLevel

	// H.264 Profile. High 4:2:2 and 10-bit profiles are only available with the AVC-I
	// License.
	CodecProfile H264CodecProfile

	// Choose Adaptive to improve subjective video quality for high-motion content.
	// This will cause the service to use fewer B-frames (which infer information based
	// on other frames) for high-motion portions of the video and more B-frames for
	// low-motion portions. The maximum number of B-frames is limited by the value you
	// provide for the setting B frames between reference frames
	// (numberBFramesBetweenReferenceFrames).
	DynamicSubGop H264DynamicSubGop

	// Entropy encoding mode. Use CABAC (must be in Main or High profile) or CAVLC.
	EntropyEncoding H264EntropyEncoding

	// Keep the default value, PAFF, to have MediaConvert use PAFF encoding for
	// interlaced outputs. Choose Force field (FORCE_FIELD) to disable PAFF encoding
	// and create separate interlaced fields.
	FieldEncoding H264FieldEncoding

	// Only use this setting when you change the default value, AUTO, for the setting
	// H264AdaptiveQuantization. When you keep all defaults, excluding
	// H264AdaptiveQuantization and all other adaptive quantization from your JSON job
	// specification, MediaConvert automatically applies the best types of quantization
	// for your video content. When you set H264AdaptiveQuantization to a value other
	// than AUTO, the default value for H264FlickerAdaptiveQuantization is Disabled
	// (DISABLED). Change this value to Enabled (ENABLED) to reduce I-frame pop.
	// I-frame pop appears as a visual flicker that can arise when the encoder saves
	// bits by copying some macroblocks many times from frame to frame, and then
	// refreshes them at the I-frame. When you enable this setting, the encoder updates
	// these macroblocks slightly more often to smooth out the flicker. To manually
	// enable or disable H264FlickerAdaptiveQuantization, you must set Adaptive
	// quantization (H264AdaptiveQuantization) to a value other than AUTO.
	FlickerAdaptiveQuantization H264FlickerAdaptiveQuantization

	// If you are using the console, use the Framerate setting to specify the frame
	// rate for this output. If you want to keep the same frame rate as the input
	// video, choose Follow source. If you want to do frame rate conversion, choose a
	// frame rate from the dropdown list or choose Custom. The framerates shown in the
	// dropdown list are decimal approximations of fractions. If you choose Custom,
	// specify your frame rate as a fraction. If you are creating your transcoding job
	// specification as a JSON file without the console, use FramerateControl to
	// specify which value the service uses for the frame rate for this output. Choose
	// INITIALIZE_FROM_SOURCE if you want the service to use the frame rate from the
	// input. Choose SPECIFIED if you want the service to use the frame rate you
	// specify in the settings FramerateNumerator and FramerateDenominator.
	FramerateControl H264FramerateControl

	// Choose the method that you want MediaConvert to use when increasing or
	// decreasing the frame rate. We recommend using drop duplicate (DUPLICATE_DROP)
	// for numerically simple conversions, such as 60 fps to 30 fps. For numerically
	// complex conversions, you can use interpolate (INTERPOLATE) to avoid stutter.
	// This results in a smooth picture, but might introduce undesirable video
	// artifacts. For complex frame rate conversions, especially if your source video
	// has already been converted from its original cadence, use FrameFormer
	// (FRAMEFORMER) to do motion-compensated interpolation. FrameFormer chooses the
	// best conversion method frame by frame. Note that using FrameFormer increases the
	// transcoding time and incurs a significant add-on cost.
	FramerateConversionAlgorithm H264FramerateConversionAlgorithm

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateDenominator to specify the denominator of this fraction. In this
	// example, use 1001 for the value of FramerateDenominator. When you use the
	// console for transcode jobs that use frame rate conversion, provide the value as
	// a decimal number for Framerate. In this example, specify 23.976.
	FramerateDenominator int32

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateNumerator to specify the numerator of this fraction. In this example,
	// use 24000 for the value of FramerateNumerator. When you use the console for
	// transcode jobs that use frame rate conversion, provide the value as a decimal
	// number for Framerate. In this example, specify 23.976.
	FramerateNumerator int32

	// If enable, use reference B frames for GOP structures that have B frames > 1.
	GopBReference H264GopBReference

	// Frequency of closed GOPs. In streaming applications, it is recommended that this
	// be set to 1 so a decoder joining mid-stream will receive an IDR frame as quickly
	// as possible. Setting this value to 0 will break output segmenting.
	GopClosedCadence int32

	// GOP Length (keyframe interval) in frames or seconds. Must be greater than zero.
	GopSize float64

	// Indicates if the GOP Size in H264 is specified in frames or seconds. If seconds
	// the system will convert the GOP Size into a frame count at run time.
	GopSizeUnits H264GopSizeUnits

	// Percentage of the buffer that should initially be filled (HRD buffer model).
	HrdBufferInitialFillPercentage int32

	// Size of buffer (HRD buffer model) in bits. For example, enter five megabits as
	// 5000000.
	HrdBufferSize int32

	// Choose the scan line type for the output. Keep the default value, Progressive
	// (PROGRESSIVE) to create a progressive output, regardless of the scan type of
	// your input. Use Top field first (TOP_FIELD) or Bottom field first (BOTTOM_FIELD)
	// to create an output that's interlaced with the same field polarity throughout.
	// Use Follow, default top (FOLLOW_TOP_FIELD) or Follow, default bottom
	// (FOLLOW_BOTTOM_FIELD) to produce outputs with the same field polarity as the
	// source. For jobs that have multiple inputs, the output field polarity might
	// change over the course of the output. Follow behavior depends on the input scan
	// type. If the source is interlaced, the output will be interlaced with the same
	// polarity as the source. If the source is progressive, the output will be
	// interlaced with top field bottom field first, depending on which of the Follow
	// options you choose.
	InterlaceMode H264InterlaceMode

	// Maximum bitrate in bits/second. For example, enter five megabits per second as
	// 5000000. Required when Rate control mode is QVBR.
	MaxBitrate int32

	// Enforces separation between repeated (cadence) I-frames and I-frames inserted by
	// Scene Change Detection. If a scene change I-frame is within I-interval frames of
	// a cadence I-frame, the GOP is shrunk and/or stretched to the scene change
	// I-frame. GOP stretch requires enabling lookahead as well as setting I-interval.
	// The normal cadence resumes for the next GOP. This setting is only used when
	// Scene Change Detect is enabled. Note: Maximum GOP stretch = GOP size +
	// Min-I-interval - 1
	MinIInterval int32

	// Number of B-frames between reference frames.
	NumberBFramesBetweenReferenceFrames int32

	// Number of reference frames to use. The encoder may use more than requested if
	// using B-frames and/or interlaced encoding.
	NumberReferenceFrames int32

	// Optional. Specify how the service determines the pixel aspect ratio (PAR) for
	// this output. The default behavior, Follow source (INITIALIZE_FROM_SOURCE), uses
	// the PAR from your input video for your output. To specify a different PAR in the
	// console, choose any value other than Follow source. To specify a different PAR
	// by editing the JSON job specification, choose SPECIFIED. When you choose
	// SPECIFIED for this setting, you must also specify values for the parNumerator
	// and parDenominator settings.
	ParControl H264ParControl

	// Required when you set Pixel aspect ratio (parControl) to SPECIFIED. On the
	// console, this corresponds to any value other than Follow source. When you
	// specify an output pixel aspect ratio (PAR) that is different from your input
	// video PAR, provide your output PAR as a ratio. For example, for D1/DV NTSC
	// widescreen, you would specify the ratio 40:33. In this example, the value for
	// parDenominator is 33.
	ParDenominator int32

	// Required when you set Pixel aspect ratio (parControl) to SPECIFIED. On the
	// console, this corresponds to any value other than Follow source. When you
	// specify an output pixel aspect ratio (PAR) that is different from your input
	// video PAR, provide your output PAR as a ratio. For example, for D1/DV NTSC
	// widescreen, you would specify the ratio 40:33. In this example, the value for
	// parNumerator is 40.
	ParNumerator int32

	// Optional. Use Quality tuning level (qualityTuningLevel) to choose how you want
	// to trade off encoding speed for output video quality. The default behavior is
	// faster, lower quality, single-pass encoding.
	QualityTuningLevel H264QualityTuningLevel

	// Settings for quality-defined variable bitrate encoding with the H.264 codec.
	// Required when you set Rate control mode to QVBR. Not valid when you set Rate
	// control mode to a value other than QVBR, or when you don't define Rate control
	// mode.
	QvbrSettings *H264QvbrSettings

	// Use this setting to specify whether this output has a variable bitrate (VBR),
	// constant bitrate (CBR) or quality-defined variable bitrate (QVBR).
	RateControlMode H264RateControlMode

	// Places a PPS header on each encoded picture, even if repeated.
	RepeatPps H264RepeatPps

	// Enable this setting to insert I-frames at scene changes that the service
	// automatically detects. This improves video quality and is enabled by default. If
	// this output uses QVBR, choose Transition detection (TRANSITION_DETECTION) for
	// further video quality improvement. For more information about QVBR, see
	// https://docs.aws.amazon.com/console/mediaconvert/cbr-vbr-qvbr.
	SceneChangeDetect H264SceneChangeDetect

	// Number of slices per picture. Must be less than or equal to the number of
	// macroblock rows for progressive pictures, and less than or equal to half the
	// number of macroblock rows for interlaced pictures.
	Slices int32

	// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
	// second (fps). Enable slow PAL to create a 25 fps output. When you enable slow
	// PAL, MediaConvert relabels the video frames to 25 fps and resamples your audio
	// to keep it synchronized with the video. Note that enabling this setting will
	// slightly reduce the duration of your video. Required settings: You must also set
	// Framerate to 25. In your JSON job specification, set (framerateControl) to
	// (SPECIFIED), (framerateNumerator) to 25 and (framerateDenominator) to 1.
	SlowPal H264SlowPal

	// Ignore this setting unless you need to comply with a specification that requires
	// a specific value. If you don't have a specification requirement, we recommend
	// that you adjust the softness of your output by using a lower value for the
	// setting Sharpness (sharpness) or by enabling a noise reducer filter
	// (noiseReducerFilter). The Softness (softness) setting specifies the quantization
	// matrices that the encoder uses. Keep the default value, 0, for flat
	// quantization. Choose the value 1 or 16 to use the default JVT softening
	// quantization matricies from the H.264 specification. Choose a value from 17 to
	// 128 to use planar interpolation. Increasing values from 17 to 128 result in
	// increasing reduction of high-frequency data. The value 128 results in the
	// softest video.
	Softness int32

	// Only use this setting when you change the default value, Auto (AUTO), for the
	// setting H264AdaptiveQuantization. When you keep all defaults, excluding
	// H264AdaptiveQuantization and all other adaptive quantization from your JSON job
	// specification, MediaConvert automatically applies the best types of quantization
	// for your video content. When you set H264AdaptiveQuantization to a value other
	// than AUTO, the default value for H264SpatialAdaptiveQuantization is Enabled
	// (ENABLED). Keep this default value to adjust quantization within each frame
	// based on spatial variation of content complexity. When you enable this feature,
	// the encoder uses fewer bits on areas that can sustain more distortion with no
	// noticeable visual degradation and uses more bits on areas where any small
	// distortion will be noticeable. For example, complex textured blocks are encoded
	// with fewer bits and smooth textured blocks are encoded with more bits. Enabling
	// this feature will almost always improve your video quality. Note, though, that
	// this feature doesn't take into account where the viewer's attention is likely to
	// be. If viewers are likely to be focusing their attention on a part of the screen
	// with a lot of complex texture, you might choose to set
	// H264SpatialAdaptiveQuantization to Disabled (DISABLED). Related setting: When
	// you enable spatial adaptive quantization, set the value for Adaptive
	// quantization (H264AdaptiveQuantization) depending on your content. For
	// homogeneous content, such as cartoons and video games, set it to Low. For
	// content with a wider variety of textures, set it to High or Higher. To manually
	// enable or disable H264SpatialAdaptiveQuantization, you must set Adaptive
	// quantization (H264AdaptiveQuantization) to a value other than AUTO.
	SpatialAdaptiveQuantization H264SpatialAdaptiveQuantization

	// Produces a bitstream compliant with SMPTE RP-2027.
	Syntax H264Syntax

	// When you do frame rate conversion from 23.976 frames per second (fps) to 29.97
	// fps, and your output scan type is interlaced, you can optionally enable hard or
	// soft telecine to create a smoother picture. Hard telecine (HARD) produces a
	// 29.97i output. Soft telecine (SOFT) produces an output with a 23.976 output that
	// signals to the video player device to do the conversion during play back. When
	// you keep the default value, None (NONE), MediaConvert does a standard frame rate
	// conversion to 29.97 without doing anything with the field polarity to create a
	// smoother picture.
	Telecine H264Telecine

	// Only use this setting when you change the default value, AUTO, for the setting
	// H264AdaptiveQuantization. When you keep all defaults, excluding
	// H264AdaptiveQuantization and all other adaptive quantization from your JSON job
	// specification, MediaConvert automatically applies the best types of quantization
	// for your video content. When you set H264AdaptiveQuantization to a value other
	// than AUTO, the default value for H264TemporalAdaptiveQuantization is Enabled
	// (ENABLED). Keep this default value to adjust quantization within each frame
	// based on temporal variation of content complexity. When you enable this feature,
	// the encoder uses fewer bits on areas of the frame that aren't moving and uses
	// more bits on complex objects with sharp edges that move a lot. For example, this
	// feature improves the readability of text tickers on newscasts and scoreboards on
	// sports matches. Enabling this feature will almost always improve your video
	// quality. Note, though, that this feature doesn't take into account where the
	// viewer's attention is likely to be. If viewers are likely to be focusing their
	// attention on a part of the screen that doesn't have moving objects with sharp
	// edges, such as sports athletes' faces, you might choose to set
	// H264TemporalAdaptiveQuantization to Disabled (DISABLED). Related setting: When
	// you enable temporal quantization, adjust the strength of the filter with the
	// setting Adaptive quantization (adaptiveQuantization). To manually enable or
	// disable H264TemporalAdaptiveQuantization, you must set Adaptive quantization
	// (H264AdaptiveQuantization) to a value other than AUTO.
	TemporalAdaptiveQuantization H264TemporalAdaptiveQuantization

	// Inserts timecode for each frame as 4 bytes of an unregistered SEI message.
	UnregisteredSeiTimecode H264UnregisteredSeiTimecode
}

// Settings for quality-defined variable bitrate encoding with the H.265 codec.
// Required when you set Rate control mode to QVBR. Not valid when you set Rate
// control mode to a value other than QVBR, or when you don't define Rate control
// mode.
type H265QvbrSettings struct {

	// Use this setting only when Rate control mode is QVBR and Quality tuning level is
	// Multi-pass HQ. For Max average bitrate values suited to the complexity of your
	// input video, the service limits the average bitrate of the video part of this
	// output to the value that you choose. That is, the total size of the video
	// element is less than or equal to the value you set multiplied by the number of
	// seconds of encoded output.
	MaxAverageBitrate int32

	// Required when you use QVBR rate control mode. That is, when you specify
	// qvbrSettings within h265Settings. Specify the general target quality level for
	// this output, from 1 to 10. Use higher numbers for greater quality. Level 10
	// results in nearly lossless compression. The quality level for most
	// broadcast-quality transcodes is between 6 and 9. Optionally, to specify a value
	// between whole numbers, also provide a value for the setting
	// qvbrQualityLevelFineTune. For example, if you want your QVBR quality level to be
	// 7.33, set qvbrQualityLevel to 7 and set qvbrQualityLevelFineTune to .33.
	QvbrQualityLevel int32

	// Optional. Specify a value here to set the QVBR quality to a level that is
	// between whole numbers. For example, if you want your QVBR quality level to be
	// 7.33, set qvbrQualityLevel to 7 and set qvbrQualityLevelFineTune to .33.
	// MediaConvert rounds your QVBR quality level to the nearest third of a whole
	// number. For example, if you set qvbrQualityLevel to 7 and you set
	// qvbrQualityLevelFineTune to .25, your actual QVBR quality level is 7.33.
	QvbrQualityLevelFineTune float64
}

// Settings for H265 codec
type H265Settings struct {

	// Specify the strength of any adaptive quantization filters that you enable. The
	// value that you choose here applies to the following settings: Flicker adaptive
	// quantization (flickerAdaptiveQuantization), Spatial adaptive quantization
	// (spatialAdaptiveQuantization), and Temporal adaptive quantization
	// (temporalAdaptiveQuantization).
	AdaptiveQuantization H265AdaptiveQuantization

	// Enables Alternate Transfer Function SEI message for outputs using Hybrid Log
	// Gamma (HLG) Electro-Optical Transfer Function (EOTF).
	AlternateTransferFunctionSei H265AlternateTransferFunctionSei

	// Specify the average bitrate in bits per second. Required for VBR and CBR. For MS
	// Smooth outputs, bitrates must be unique when rounded down to the nearest
	// multiple of 1000.
	Bitrate int32

	// H.265 Level.
	CodecLevel H265CodecLevel

	// Represents the Profile and Tier, per the HEVC (H.265) specification. Selections
	// are grouped as [Profile] / [Tier], so "Main/High" represents Main Profile with
	// High Tier. 4:2:2 profiles are only available with the HEVC 4:2:2 License.
	CodecProfile H265CodecProfile

	// Choose Adaptive to improve subjective video quality for high-motion content.
	// This will cause the service to use fewer B-frames (which infer information based
	// on other frames) for high-motion portions of the video and more B-frames for
	// low-motion portions. The maximum number of B-frames is limited by the value you
	// provide for the setting B frames between reference frames
	// (numberBFramesBetweenReferenceFrames).
	DynamicSubGop H265DynamicSubGop

	// Enable this setting to have the encoder reduce I-frame pop. I-frame pop appears
	// as a visual flicker that can arise when the encoder saves bits by copying some
	// macroblocks many times from frame to frame, and then refreshes them at the
	// I-frame. When you enable this setting, the encoder updates these macroblocks
	// slightly more often to smooth out the flicker. This setting is disabled by
	// default. Related setting: In addition to enabling this setting, you must also
	// set adaptiveQuantization to a value other than Off (OFF).
	FlickerAdaptiveQuantization H265FlickerAdaptiveQuantization

	// If you are using the console, use the Framerate setting to specify the frame
	// rate for this output. If you want to keep the same frame rate as the input
	// video, choose Follow source. If you want to do frame rate conversion, choose a
	// frame rate from the dropdown list or choose Custom. The framerates shown in the
	// dropdown list are decimal approximations of fractions. If you choose Custom,
	// specify your frame rate as a fraction. If you are creating your transcoding job
	// specification as a JSON file without the console, use FramerateControl to
	// specify which value the service uses for the frame rate for this output. Choose
	// INITIALIZE_FROM_SOURCE if you want the service to use the frame rate from the
	// input. Choose SPECIFIED if you want the service to use the frame rate you
	// specify in the settings FramerateNumerator and FramerateDenominator.
	FramerateControl H265FramerateControl

	// Choose the method that you want MediaConvert to use when increasing or
	// decreasing the frame rate. We recommend using drop duplicate (DUPLICATE_DROP)
	// for numerically simple conversions, such as 60 fps to 30 fps. For numerically
	// complex conversions, you can use interpolate (INTERPOLATE) to avoid stutter.
	// This results in a smooth picture, but might introduce undesirable video
	// artifacts. For complex frame rate conversions, especially if your source video
	// has already been converted from its original cadence, use FrameFormer
	// (FRAMEFORMER) to do motion-compensated interpolation. FrameFormer chooses the
	// best conversion method frame by frame. Note that using FrameFormer increases the
	// transcoding time and incurs a significant add-on cost.
	FramerateConversionAlgorithm H265FramerateConversionAlgorithm

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateDenominator to specify the denominator of this fraction. In this
	// example, use 1001 for the value of FramerateDenominator. When you use the
	// console for transcode jobs that use frame rate conversion, provide the value as
	// a decimal number for Framerate. In this example, specify 23.976.
	FramerateDenominator int32

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateNumerator to specify the numerator of this fraction. In this example,
	// use 24000 for the value of FramerateNumerator. When you use the console for
	// transcode jobs that use frame rate conversion, provide the value as a decimal
	// number for Framerate. In this example, specify 23.976.
	FramerateNumerator int32

	// If enable, use reference B frames for GOP structures that have B frames > 1.
	GopBReference H265GopBReference

	// Frequency of closed GOPs. In streaming applications, it is recommended that this
	// be set to 1 so a decoder joining mid-stream will receive an IDR frame as quickly
	// as possible. Setting this value to 0 will break output segmenting.
	GopClosedCadence int32

	// GOP Length (keyframe interval) in frames or seconds. Must be greater than zero.
	GopSize float64

	// Indicates if the GOP Size in H265 is specified in frames or seconds. If seconds
	// the system will convert the GOP Size into a frame count at run time.
	GopSizeUnits H265GopSizeUnits

	// Percentage of the buffer that should initially be filled (HRD buffer model).
	HrdBufferInitialFillPercentage int32

	// Size of buffer (HRD buffer model) in bits. For example, enter five megabits as
	// 5000000.
	HrdBufferSize int32

	// Choose the scan line type for the output. Keep the default value, Progressive
	// (PROGRESSIVE) to create a progressive output, regardless of the scan type of
	// your input. Use Top field first (TOP_FIELD) or Bottom field first (BOTTOM_FIELD)
	// to create an output that's interlaced with the same field polarity throughout.
	// Use Follow, default top (FOLLOW_TOP_FIELD) or Follow, default bottom
	// (FOLLOW_BOTTOM_FIELD) to produce outputs with the same field polarity as the
	// source. For jobs that have multiple inputs, the output field polarity might
	// change over the course of the output. Follow behavior depends on the input scan
	// type. If the source is interlaced, the output will be interlaced with the same
	// polarity as the source. If the source is progressive, the output will be
	// interlaced with top field bottom field first, depending on which of the Follow
	// options you choose.
	InterlaceMode H265InterlaceMode

	// Maximum bitrate in bits/second. For example, enter five megabits per second as
	// 5000000. Required when Rate control mode is QVBR.
	MaxBitrate int32

	// Enforces separation between repeated (cadence) I-frames and I-frames inserted by
	// Scene Change Detection. If a scene change I-frame is within I-interval frames of
	// a cadence I-frame, the GOP is shrunk and/or stretched to the scene change
	// I-frame. GOP stretch requires enabling lookahead as well as setting I-interval.
	// The normal cadence resumes for the next GOP. This setting is only used when
	// Scene Change Detect is enabled. Note: Maximum GOP stretch = GOP size +
	// Min-I-interval - 1
	MinIInterval int32

	// Number of B-frames between reference frames.
	NumberBFramesBetweenReferenceFrames int32

	// Number of reference frames to use. The encoder may use more than requested if
	// using B-frames and/or interlaced encoding.
	NumberReferenceFrames int32

	// Optional. Specify how the service determines the pixel aspect ratio (PAR) for
	// this output. The default behavior, Follow source (INITIALIZE_FROM_SOURCE), uses
	// the PAR from your input video for your output. To specify a different PAR in the
	// console, choose any value other than Follow source. To specify a different PAR
	// by editing the JSON job specification, choose SPECIFIED. When you choose
	// SPECIFIED for this setting, you must also specify values for the parNumerator
	// and parDenominator settings.
	ParControl H265ParControl

	// Required when you set Pixel aspect ratio (parControl) to SPECIFIED. On the
	// console, this corresponds to any value other than Follow source. When you
	// specify an output pixel aspect ratio (PAR) that is different from your input
	// video PAR, provide your output PAR as a ratio. For example, for D1/DV NTSC
	// widescreen, you would specify the ratio 40:33. In this example, the value for
	// parDenominator is 33.
	ParDenominator int32

	// Required when you set Pixel aspect ratio (parControl) to SPECIFIED. On the
	// console, this corresponds to any value other than Follow source. When you
	// specify an output pixel aspect ratio (PAR) that is different from your input
	// video PAR, provide your output PAR as a ratio. For example, for D1/DV NTSC
	// widescreen, you would specify the ratio 40:33. In this example, the value for
	// parNumerator is 40.
	ParNumerator int32

	// Optional. Use Quality tuning level (qualityTuningLevel) to choose how you want
	// to trade off encoding speed for output video quality. The default behavior is
	// faster, lower quality, single-pass encoding.
	QualityTuningLevel H265QualityTuningLevel

	// Settings for quality-defined variable bitrate encoding with the H.265 codec.
	// Required when you set Rate control mode to QVBR. Not valid when you set Rate
	// control mode to a value other than QVBR, or when you don't define Rate control
	// mode.
	QvbrSettings *H265QvbrSettings

	// Use this setting to specify whether this output has a variable bitrate (VBR),
	// constant bitrate (CBR) or quality-defined variable bitrate (QVBR).
	RateControlMode H265RateControlMode

	// Specify Sample Adaptive Offset (SAO) filter strength. Adaptive mode dynamically
	// selects best strength based on content
	SampleAdaptiveOffsetFilterMode H265SampleAdaptiveOffsetFilterMode

	// Enable this setting to insert I-frames at scene changes that the service
	// automatically detects. This improves video quality and is enabled by default. If
	// this output uses QVBR, choose Transition detection (TRANSITION_DETECTION) for
	// further video quality improvement. For more information about QVBR, see
	// https://docs.aws.amazon.com/console/mediaconvert/cbr-vbr-qvbr.
	SceneChangeDetect H265SceneChangeDetect

	// Number of slices per picture. Must be less than or equal to the number of
	// macroblock rows for progressive pictures, and less than or equal to half the
	// number of macroblock rows for interlaced pictures.
	Slices int32

	// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
	// second (fps). Enable slow PAL to create a 25 fps output. When you enable slow
	// PAL, MediaConvert relabels the video frames to 25 fps and resamples your audio
	// to keep it synchronized with the video. Note that enabling this setting will
	// slightly reduce the duration of your video. Required settings: You must also set
	// Framerate to 25. In your JSON job specification, set (framerateControl) to
	// (SPECIFIED), (framerateNumerator) to 25 and (framerateDenominator) to 1.
	SlowPal H265SlowPal

	// Keep the default value, Enabled (ENABLED), to adjust quantization within each
	// frame based on spatial variation of content complexity. When you enable this
	// feature, the encoder uses fewer bits on areas that can sustain more distortion
	// with no noticeable visual degradation and uses more bits on areas where any
	// small distortion will be noticeable. For example, complex textured blocks are
	// encoded with fewer bits and smooth textured blocks are encoded with more bits.
	// Enabling this feature will almost always improve your video quality. Note,
	// though, that this feature doesn't take into account where the viewer's attention
	// is likely to be. If viewers are likely to be focusing their attention on a part
	// of the screen with a lot of complex texture, you might choose to disable this
	// feature. Related setting: When you enable spatial adaptive quantization, set the
	// value for Adaptive quantization (adaptiveQuantization) depending on your
	// content. For homogeneous content, such as cartoons and video games, set it to
	// Low. For content with a wider variety of textures, set it to High or Higher.
	SpatialAdaptiveQuantization H265SpatialAdaptiveQuantization

	// This field applies only if the Streams > Advanced > Framerate (framerate) field
	// is set to 29.970. This field works with the Streams > Advanced > Preprocessors >
	// Deinterlacer field (deinterlace_mode) and the Streams > Advanced > Interlaced
	// Mode field (interlace_mode) to identify the scan type for the output:
	// Progressive, Interlaced, Hard Telecine or Soft Telecine. - Hard: produces 29.97i
	// output from 23.976 input. - Soft: produces 23.976; the player converts this
	// output to 29.97i.
	Telecine H265Telecine

	// Keep the default value, Enabled (ENABLED), to adjust quantization within each
	// frame based on temporal variation of content complexity. When you enable this
	// feature, the encoder uses fewer bits on areas of the frame that aren't moving
	// and uses more bits on complex objects with sharp edges that move a lot. For
	// example, this feature improves the readability of text tickers on newscasts and
	// scoreboards on sports matches. Enabling this feature will almost always improve
	// your video quality. Note, though, that this feature doesn't take into account
	// where the viewer's attention is likely to be. If viewers are likely to be
	// focusing their attention on a part of the screen that doesn't have moving
	// objects with sharp edges, such as sports athletes' faces, you might choose to
	// disable this feature. Related setting: When you enable temporal quantization,
	// adjust the strength of the filter with the setting Adaptive quantization
	// (adaptiveQuantization).
	TemporalAdaptiveQuantization H265TemporalAdaptiveQuantization

	// Enables temporal layer identifiers in the encoded bitstream. Up to 3 layers are
	// supported depending on GOP structure: I- and P-frames form one layer, reference
	// B-frames can form a second layer and non-reference b-frames can form a third
	// layer. Decoders can optionally decode only the lower temporal layers to generate
	// a lower frame rate output. For example, given a bitstream with temporal IDs and
	// with b-frames = 1 (i.e. IbPbPb display order), a decoder could decode all the
	// frames for full frame rate output or only the I and P frames (lowest temporal
	// layer) for a half frame rate output.
	TemporalIds H265TemporalIds

	// Enable use of tiles, allowing horizontal as well as vertical subdivision of the
	// encoded pictures.
	Tiles H265Tiles

	// Inserts timecode for each frame as 4 bytes of an unregistered SEI message.
	UnregisteredSeiTimecode H265UnregisteredSeiTimecode

	// If the location of parameter set NAL units doesn't matter in your workflow,
	// ignore this setting. Use this setting only with CMAF or DASH outputs, or with
	// standalone file outputs in an MPEG-4 container (MP4 outputs). Choose HVC1 to
	// mark your output as HVC1. This makes your output compliant with the following
	// specification: ISO IECJTC1 SC29 N13798 Text ISO/IEC FDIS 14496-15 3rd Edition.
	// For these outputs, the service stores parameter set NAL units in the sample
	// headers but not in the samples directly. For MP4 outputs, when you choose HVC1,
	// your output video might not work properly with some downstream systems and video
	// players. The service defaults to marking your output as HEV1. For these outputs,
	// the service writes parameter set NAL units directly into the samples.
	WriteMp4PackagingType H265WriteMp4PackagingType
}

// Use these settings to specify static color calibration metadata, as defined by
// SMPTE ST 2086. These values don't affect the pixel values that are encoded in
// the video stream. They are intended to help the downstream video player display
// content in a way that reflects the intentions of the the content creator.
type Hdr10Metadata struct {

	// HDR Master Display Information must be provided by a color grader, using color
	// grading tools. Range is 0 to 50,000, each increment represents 0.00002 in
	// CIE1931 color coordinate. Note that this setting is not for color correction.
	BluePrimaryX int32

	// HDR Master Display Information must be provided by a color grader, using color
	// grading tools. Range is 0 to 50,000, each increment represents 0.00002 in
	// CIE1931 color coordinate. Note that this setting is not for color correction.
	BluePrimaryY int32

	// HDR Master Display Information must be provided by a color grader, using color
	// grading tools. Range is 0 to 50,000, each increment represents 0.00002 in
	// CIE1931 color coordinate. Note that this setting is not for color correction.
	GreenPrimaryX int32

	// HDR Master Display Information must be provided by a color grader, using color
	// grading tools. Range is 0 to 50,000, each increment represents 0.00002 in
	// CIE1931 color coordinate. Note that this setting is not for color correction.
	GreenPrimaryY int32

	// Maximum light level among all samples in the coded video sequence, in units of
	// candelas per square meter. This setting doesn't have a default value; you must
	// specify a value that is suitable for the content.
	MaxContentLightLevel int32

	// Maximum average light level of any frame in the coded video sequence, in units
	// of candelas per square meter. This setting doesn't have a default value; you
	// must specify a value that is suitable for the content.
	MaxFrameAverageLightLevel int32

	// Nominal maximum mastering display luminance in units of of 0.0001 candelas per
	// square meter.
	MaxLuminance int32

	// Nominal minimum mastering display luminance in units of of 0.0001 candelas per
	// square meter
	MinLuminance int32

	// HDR Master Display Information must be provided by a color grader, using color
	// grading tools. Range is 0 to 50,000, each increment represents 0.00002 in
	// CIE1931 color coordinate. Note that this setting is not for color correction.
	RedPrimaryX int32

	// HDR Master Display Information must be provided by a color grader, using color
	// grading tools. Range is 0 to 50,000, each increment represents 0.00002 in
	// CIE1931 color coordinate. Note that this setting is not for color correction.
	RedPrimaryY int32

	// HDR Master Display Information must be provided by a color grader, using color
	// grading tools. Range is 0 to 50,000, each increment represents 0.00002 in
	// CIE1931 color coordinate. Note that this setting is not for color correction.
	WhitePointX int32

	// HDR Master Display Information must be provided by a color grader, using color
	// grading tools. Range is 0 to 50,000, each increment represents 0.00002 in
	// CIE1931 color coordinate. Note that this setting is not for color correction.
	WhitePointY int32
}

// Specify the details for each additional HLS manifest that you want the service
// to generate for this output group. Each manifest can reference a different
// subset of outputs in the group.
type HlsAdditionalManifest struct {

	// Specify a name modifier that the service adds to the name of this manifest to
	// make it different from the file names of the other main manifests in the output
	// group. For example, say that the default main manifest for your HLS group is
	// film-name.m3u8. If you enter "-no-premium" for this setting, then the file name
	// the service generates for this top-level manifest is film-name-no-premium.m3u8.
	// For HLS output groups, specify a manifestNameModifier that is different from the
	// nameModifier of the output. The service uses the output name modifier to create
	// unique names for the individual variant manifests.
	ManifestNameModifier *string

	// Specify the outputs that you want this additional top-level manifest to
	// reference.
	SelectedOutputs []string
}

// Caption Language Mapping
type HlsCaptionLanguageMapping struct {

	// Caption channel.
	CaptionChannel int32

	// Specify the language for this captions channel, using the ISO 639-2 or ISO 639-3
	// three-letter language code
	CustomLanguageCode *string

	// Specify the language, using the ISO 639-2 three-letter code listed at
	// https://www.loc.gov/standards/iso639-2/php/code_list.php.
	LanguageCode LanguageCode

	// Caption language description.
	LanguageDescription *string
}

// Settings for HLS encryption
type HlsEncryptionSettings struct {

	// This is a 128-bit, 16-byte hex value represented by a 32-character text string.
	// If this parameter is not set then the Initialization Vector will follow the
	// segment number by default.
	ConstantInitializationVector *string

	// Encrypts the segments with the given encryption scheme. Leave blank to disable.
	// Selecting 'Disabled' in the web interface also disables encryption.
	EncryptionMethod HlsEncryptionType

	// The Initialization Vector is a 128-bit number used in conjunction with the key
	// for encrypting blocks. If set to INCLUDE, Initialization Vector is listed in the
	// manifest. Otherwise Initialization Vector is not in the manifest.
	InitializationVectorInManifest HlsInitializationVectorInManifest

	// Enable this setting to insert the EXT-X-SESSION-KEY element into the master
	// playlist. This allows for offline Apple HLS FairPlay content protection.
	OfflineEncrypted HlsOfflineEncrypted

	// If your output group type is HLS, DASH, or Microsoft Smooth, use these settings
	// when doing DRM encryption with a SPEKE-compliant key provider. If your output
	// group type is CMAF, use the SpekeKeyProviderCmaf settings instead.
	SpekeKeyProvider *SpekeKeyProvider

	// Use these settings to set up encryption with a static key provider.
	StaticKeyProvider *StaticKeyProvider

	// Specify whether your DRM encryption key is static or from a key provider that
	// follows the SPEKE standard. For more information about SPEKE, see
	// https://docs.aws.amazon.com/speke/latest/documentation/what-is-speke.html.
	Type HlsKeyProviderType
}

// Required when you set (Type) under (OutputGroups)>(OutputGroupSettings) to
// HLS_GROUP_SETTINGS.
type HlsGroupSettings struct {

	// Choose one or more ad marker types to decorate your Apple HLS manifest. This
	// setting does not determine whether SCTE-35 markers appear in the outputs
	// themselves.
	AdMarkers []HlsAdMarkers

	// By default, the service creates one top-level .m3u8 HLS manifest for each HLS
	// output group in your job. This default manifest references every output in the
	// output group. To create additional top-level manifests that reference a subset
	// of the outputs in the output group, specify a list of them here.
	AdditionalManifests []HlsAdditionalManifest

	// Ignore this setting unless you are using FairPlay DRM with Verimatrix and you
	// encounter playback issues. Keep the default value, Include (INCLUDE), to output
	// audio-only headers. Choose Exclude (EXCLUDE) to remove the audio-only headers
	// from your audio segments.
	AudioOnlyHeader HlsAudioOnlyHeader

	// A partial URI prefix that will be prepended to each output in the media .m3u8
	// file. Can be used if base manifest is delivered from a different URL than the
	// main .m3u8 file.
	BaseUrl *string

	// Language to be used on Caption outputs
	CaptionLanguageMappings []HlsCaptionLanguageMapping

	// Applies only to 608 Embedded output captions. Insert: Include CLOSED-CAPTIONS
	// lines in the manifest. Specify at least one language in the CC1 Language Code
	// field. One CLOSED-CAPTION line is added for each Language Code you specify. Make
	// sure to specify the languages in the order in which they appear in the original
	// source (if the source is embedded format) or the order of the caption selectors
	// (if the source is other than embedded). Otherwise, languages in the manifest
	// will not match up properly with the output captions. None: Include
	// CLOSED-CAPTIONS=NONE line in the manifest. Omit: Omit any CLOSED-CAPTIONS line
	// from the manifest.
	CaptionLanguageSetting HlsCaptionLanguageSetting

	// When set to ENABLED, sets #EXT-X-ALLOW-CACHE:no tag, which prevents client from
	// saving media segments for later replay.
	ClientCache HlsClientCache

	// Specification to use (RFC-6381 or the default RFC-4281) during m3u8 playlist
	// generation.
	CodecSpecification HlsCodecSpecification

	// Use Destination (Destination) to specify the S3 output location and the output
	// filename base. Destination accepts format identifiers. If you do not specify the
	// base filename in the URI, the service will use the filename of the input file.
	// If your job has multiple inputs, the service uses the filename of the first
	// input file.
	Destination *string

	// Settings associated with the destination. Will vary based on the type of
	// destination
	DestinationSettings *DestinationSettings

	// Indicates whether segments should be placed in subdirectories.
	DirectoryStructure HlsDirectoryStructure

	// DRM settings.
	Encryption *HlsEncryptionSettings

	// When set to GZIP, compresses HLS playlist.
	ManifestCompression HlsManifestCompression

	// Indicates whether the output manifest should use floating point values for
	// segment duration.
	ManifestDurationFormat HlsManifestDurationFormat

	// Keep this setting at the default value of 0, unless you are troubleshooting a
	// problem with how devices play back the end of your video asset. If you know that
	// player devices are hanging on the final segment of your video because the length
	// of your final segment is too short, use this setting to specify a minimum final
	// segment length, in seconds. Choose a value that is greater than or equal to 1
	// and less than your segment length. When you specify a value for this setting,
	// the encoder will combine any final segment that is shorter than the length that
	// you specify with the previous segment. For example, your segment length is 3
	// seconds and your final segment is .5 seconds without a minimum final segment
	// length; when you set the minimum final segment length to 1, your final segment
	// is 3.5 seconds.
	MinFinalSegmentLength float64

	// When set, Minimum Segment Size is enforced by looking ahead and back within the
	// specified range for a nearby avail and extending the segment size if needed.
	MinSegmentLength int32

	// Indicates whether the .m3u8 manifest file should be generated for this HLS
	// output group.
	OutputSelection HlsOutputSelection

	// Includes or excludes EXT-X-PROGRAM-DATE-TIME tag in .m3u8 manifest files. The
	// value is calculated as follows: either the program date and time are initialized
	// using the input timecode source, or the time is initialized using the input
	// timecode source and the date is initialized using the timestamp_offset.
	ProgramDateTime HlsProgramDateTime

	// Period of insertion of EXT-X-PROGRAM-DATE-TIME entry, in seconds.
	ProgramDateTimePeriod int32

	// When set to SINGLE_FILE, emits program as a single media resource (.ts) file,
	// uses #EXT-X-BYTERANGE tags to index segment for playback.
	SegmentControl HlsSegmentControl

	// Length of MPEG-2 Transport Stream segments to create (in seconds). Note that
	// segments will end on the next keyframe after this number of seconds, so actual
	// segment length may be longer.
	SegmentLength int32

	// Number of segments to write to a subdirectory before starting a new one.
	// directoryStructure must be SINGLE_DIRECTORY for this setting to have an effect.
	SegmentsPerSubdirectory int32

	// Include or exclude RESOLUTION attribute for video in EXT-X-STREAM-INF tag of
	// variant manifest.
	StreamInfResolution HlsStreamInfResolution

	// Indicates ID3 frame that has the timecode.
	TimedMetadataId3Frame HlsTimedMetadataId3Frame

	// Timed Metadata interval in seconds.
	TimedMetadataId3Period int32

	// Provides an extra millisecond delta offset to fine tune the timestamps.
	TimestampDeltaMilliseconds int32
}

// Settings for HLS output groups
type HlsSettings struct {

	// Specifies the group to which the audio Rendition belongs.
	AudioGroupId *string

	// Use this setting only in audio-only outputs. Choose MPEG-2 Transport Stream
	// (M2TS) to create a file in an MPEG2-TS container. Keep the default value
	// Automatic (AUTOMATIC) to create an audio-only file in a raw container.
	// Regardless of the value that you specify here, if this output has video, the
	// service will place the output into an MPEG2-TS container.
	AudioOnlyContainer HlsAudioOnlyContainer

	// List all the audio groups that are used with the video output stream. Input all
	// the audio GROUP-IDs that are associated to the video, separate by ','.
	AudioRenditionSets *string

	// Four types of audio-only tracks are supported: Audio-Only Variant Stream The
	// client can play back this audio-only stream instead of video in low-bandwidth
	// scenarios. Represented as an EXT-X-STREAM-INF in the HLS manifest. Alternate
	// Audio, Auto Select, Default Alternate rendition that the client should try to
	// play back by default. Represented as an EXT-X-MEDIA in the HLS manifest with
	// DEFAULT=YES, AUTOSELECT=YES Alternate Audio, Auto Select, Not Default Alternate
	// rendition that the client may try to play back by default. Represented as an
	// EXT-X-MEDIA in the HLS manifest with DEFAULT=NO, AUTOSELECT=YES Alternate Audio,
	// not Auto Select Alternate rendition that the client will not try to play back by
	// default. Represented as an EXT-X-MEDIA in the HLS manifest with DEFAULT=NO,
	// AUTOSELECT=NO
	AudioTrackType HlsAudioTrackType

	// When set to INCLUDE, writes I-Frame Only Manifest in addition to the HLS
	// manifest
	IFrameOnlyManifest HlsIFrameOnlyManifest

	// Use this setting to add an identifying string to the filename of each segment.
	// The service adds this string between the name modifier and segment index number.
	// You can use format identifiers in the string. For more information, see
	// https://docs.aws.amazon.com/mediaconvert/latest/ug/using-variables-in-your-job-settings.html
	SegmentModifier *string
}

// Optional. Configuration for a destination queue to which the job can hop once a
// customer-defined minimum wait time has passed.
type HopDestination struct {

	// Optional. When you set up a job to use queue hopping, you can specify a
	// different relative priority for the job in the destination queue. If you don't
	// specify, the relative priority will remain the same as in the previous queue.
	Priority int32

	// Optional unless the job is submitted on the default queue. When you set up a job
	// to use queue hopping, you can specify a destination queue. This queue cannot be
	// the original queue to which the job is submitted. If the original queue isn't
	// the default queue and you don't specify the destination queue, the job will move
	// to the default queue.
	Queue *string

	// Required for setting up a job to use queue hopping. Minimum wait time in minutes
	// until the job can hop to the destination queue. Valid range is 1 to 1440
	// minutes, inclusive.
	WaitMinutes int32
}

// To insert ID3 tags in your output, specify two values. Use ID3 tag (Id3) to
// specify the base 64 encoded string and use Timecode (TimeCode) to specify the
// time when the tag should be inserted. To insert multiple ID3 tags in your
// output, create multiple instances of ID3 insertion (Id3Insertion).
type Id3Insertion struct {

	// Use ID3 tag (Id3) to provide a tag value in base64-encode format.
	Id3 *string

	// Provide a Timecode (TimeCode) in HH:MM:SS:FF or HH:MM:SS;FF format.
	Timecode *string
}

// Enable the image inserter feature to include a graphic overlay on your video.
// Enable or disable this feature for each input or output individually. This
// setting is disabled by default.
type ImageInserter struct {

	// Specify the images that you want to overlay on your video. The images must be
	// PNG or TGA files.
	InsertableImages []InsertableImage
}

// Settings specific to IMSC caption outputs.
type ImscDestinationSettings struct {

	// Keep this setting enabled to have MediaConvert use the font style and position
	// information from the captions source in the output. This option is available
	// only when your input captions are IMSC, SMPTE-TT, or TTML. Disable this setting
	// for simplified output captions.
	StylePassthrough ImscStylePassthrough
}

// Specifies media input
type Input struct {

	// Specifies set of audio selectors within an input to combine. An input may have
	// multiple audio selector groups. See "Audio Selector
	// Group":#inputs-audio_selector_group for more information.
	AudioSelectorGroups map[string]AudioSelectorGroup

	// Use Audio selectors (AudioSelectors) to specify a track or set of tracks from
	// the input that you will use in your outputs. You can use multiple Audio
	// selectors per input.
	AudioSelectors map[string]AudioSelector

	// Use captions selectors to specify the captions data from your input that you use
	// in your outputs. You can use up to 20 captions selectors per input.
	CaptionSelectors map[string]CaptionSelector

	// Use Cropping selection (crop) to specify the video area that the service will
	// include in the output video frame. If you specify a value here, it will override
	// any value that you specify in the output setting Cropping selection (crop).
	Crop *Rectangle

	// Enable Deblock (InputDeblockFilter) to produce smoother motion in the output.
	// Default is disabled. Only manually controllable for MPEG2 and uncompressed video
	// inputs.
	DeblockFilter InputDeblockFilter

	// Settings for decrypting any input files that you encrypt before you upload them
	// to Amazon S3. MediaConvert can decrypt files only when you use AWS Key
	// Management Service (KMS) to encrypt the data key that you use to encrypt your
	// content.
	DecryptionSettings *InputDecryptionSettings

	// Enable Denoise (InputDenoiseFilter) to filter noise from the input. Default is
	// disabled. Only applicable to MPEG2, H.264, H.265, and uncompressed video inputs.
	DenoiseFilter InputDenoiseFilter

	// Specify the source file for your transcoding job. You can use multiple inputs in
	// a single job. The service concatenates these inputs, in the order that you
	// specify them in the job, to create the outputs. If your input format is IMF,
	// specify your input by providing the path to your CPL. For example,
	// "s3://bucket/vf/cpl.xml". If the CPL is in an incomplete IMP, make sure to use
	// Supplemental IMPs (SupplementalImps) to specify any supplemental IMPs that
	// contain assets referenced by the CPL.
	FileInput *string

	// Specify how the transcoding service applies the denoise and deblock filters. You
	// must also enable the filters separately, with Denoise (InputDenoiseFilter) and
	// Deblock (InputDeblockFilter). * Auto - The transcoding service determines
	// whether to apply filtering, depending on input type and quality. * Disable - The
	// input is not filtered. This is true even if you use the API to enable them in
	// (InputDeblockFilter) and (InputDeblockFilter). * Force - The input is filtered
	// regardless of input type.
	FilterEnable InputFilterEnable

	// Use Filter strength (FilterStrength) to adjust the magnitude the input filter
	// settings (Deblock and Denoise). The range is -5 to 5. Default is 0.
	FilterStrength int32

	// Enable the image inserter feature to include a graphic overlay on your video.
	// Enable or disable this feature for each input individually. This setting is
	// disabled by default.
	ImageInserter *ImageInserter

	// (InputClippings) contains sets of start and end times that together specify a
	// portion of the input to be used in the outputs. If you provide only a start
	// time, the clip will be the entire input from that point to the end. If you
	// provide only an end time, it will be the entire input up to that point. When you
	// specify more than one input clip, the transcoding service creates the job
	// outputs by stringing the clips together in the order you specify them.
	InputClippings []InputClipping

	// When you have a progressive segmented frame (PsF) input, use this setting to
	// flag the input as PsF. MediaConvert doesn't automatically detect PsF. Therefore,
	// flagging your input as PsF results in better preservation of video quality when
	// you do deinterlacing and frame rate conversion. If you don't specify, the
	// default value is Auto (AUTO). Auto is the correct setting for all inputs that
	// are not PsF. Don't set this value to PsF when your input is interlaced. Doing so
	// creates horizontal interlacing artifacts.
	InputScanType InputScanType

	// Use Selection placement (position) to define the video area in your output
	// frame. The area outside of the rectangle that you specify here is black. If you
	// specify a value here, it will override any value that you specify in the output
	// setting Selection placement (position). If you specify a value here, this will
	// override any AFD values in your input, even if you set Respond to AFD
	// (RespondToAfd) to Respond (RESPOND). If you specify a value here, this will
	// ignore anything that you specify for the setting Scaling Behavior
	// (scalingBehavior).
	Position *Rectangle

	// Use Program (programNumber) to select a specific program from within a
	// multi-program transport stream. Note that Quad 4K is not currently supported.
	// Default is the first program within the transport stream. If the program you
	// specify doesn't exist, the transcoding service will use this default.
	ProgramNumber int32

	// Set PSI control (InputPsiControl) for transport stream inputs to specify which
	// data the demux process to scans. * Ignore PSI - Scan all PIDs for audio and
	// video. * Use PSI - Scan only PSI data.
	PsiControl InputPsiControl

	// Provide a list of any necessary supplemental IMPs. You need supplemental IMPs if
	// the CPL that you're using for your input is in an incomplete IMP. Specify either
	// the supplemental IMP directories with a trailing slash or the ASSETMAP.xml
	// files. For example ["s3://bucket/ov/", "s3://bucket/vf2/ASSETMAP.xml"]. You
	// don't need to specify the IMP that contains your input CPL, because the service
	// automatically detects it.
	SupplementalImps []string

	// Use this Timecode source setting, located under the input settings
	// (InputTimecodeSource), to specify how the service counts input video frames.
	// This input frame count affects only the behavior of features that apply to a
	// single input at a time, such as input clipping and synchronizing some captions
	// formats. Choose Embedded (EMBEDDED) to use the timecodes in your input video.
	// Choose Start at zero (ZEROBASED) to start the first frame at zero. Choose
	// Specified start (SPECIFIEDSTART) to start the first frame at the timecode that
	// you specify in the setting Start timecode (timecodeStart). If you don't specify
	// a value for Timecode source, the service will use Embedded by default. For more
	// information about timecodes, see
	// https://docs.aws.amazon.com/console/mediaconvert/timecode.
	TimecodeSource InputTimecodeSource

	// Specify the timecode that you want the service to use for this input's initial
	// frame. To use this setting, you must set the Timecode source setting, located
	// under the input settings (InputTimecodeSource), to Specified start
	// (SPECIFIEDSTART). For more information about timecodes, see
	// https://docs.aws.amazon.com/console/mediaconvert/timecode.
	TimecodeStart *string

	// Selector for video.
	VideoSelector *VideoSelector
}

// To transcode only portions of your input (clips), include one Input clipping
// (one instance of InputClipping in the JSON job file) for each input clip. All
// input clips you specify will be included in every output of the job.
type InputClipping struct {

	// Set End timecode (EndTimecode) to the end of the portion of the input you are
	// clipping. The frame corresponding to the End timecode value is included in the
	// clip. Start timecode or End timecode may be left blank, but not both. Use the
	// format HH:MM:SS:FF or HH:MM:SS;FF, where HH is the hour, MM is the minute, SS is
	// the second, and FF is the frame number. When choosing this value, take into
	// account your setting for timecode source under input settings
	// (InputTimecodeSource). For example, if you have embedded timecodes that start at
	// 01:00:00:00 and you want your clip to end six minutes into the video, use
	// 01:06:00:00.
	EndTimecode *string

	// Set Start timecode (StartTimecode) to the beginning of the portion of the input
	// you are clipping. The frame corresponding to the Start timecode value is
	// included in the clip. Start timecode or End timecode may be left blank, but not
	// both. Use the format HH:MM:SS:FF or HH:MM:SS;FF, where HH is the hour, MM is the
	// minute, SS is the second, and FF is the frame number. When choosing this value,
	// take into account your setting for Input timecode source. For example, if you
	// have embedded timecodes that start at 01:00:00:00 and you want your clip to
	// begin five minutes into the video, use 01:05:00:00.
	StartTimecode *string
}

// Settings for decrypting any input files that you encrypt before you upload them
// to Amazon S3. MediaConvert can decrypt files only when you use AWS Key
// Management Service (KMS) to encrypt the data key that you use to encrypt your
// content.
type InputDecryptionSettings struct {

	// Specify the encryption mode that you used to encrypt your input files.
	DecryptionMode DecryptionMode

	// Warning! Don't provide your encryption key in plaintext. Your job settings could
	// be intercepted, making your encrypted content vulnerable. Specify the encrypted
	// version of the data key that you used to encrypt your content. The data key must
	// be encrypted by AWS Key Management Service (KMS). The key can be 128, 192, or
	// 256 bits.
	EncryptedDecryptionKey *string

	// Specify the initialization vector that you used when you encrypted your content
	// before uploading it to Amazon S3. You can use a 16-byte initialization vector
	// with any encryption mode. Or, you can use a 12-byte initialization vector with
	// GCM or CTR. MediaConvert accepts only initialization vectors that are
	// base64-encoded.
	InitializationVector *string

	// Specify the AWS Region for AWS Key Management Service (KMS) that you used to
	// encrypt your data key, if that Region is different from the one you are using
	// for AWS Elemental MediaConvert.
	KmsKeyRegion *string
}

// Specified video input in a template.
type InputTemplate struct {

	// Specifies set of audio selectors within an input to combine. An input may have
	// multiple audio selector groups. See "Audio Selector
	// Group":#inputs-audio_selector_group for more information.
	AudioSelectorGroups map[string]AudioSelectorGroup

	// Use Audio selectors (AudioSelectors) to specify a track or set of tracks from
	// the input that you will use in your outputs. You can use multiple Audio
	// selectors per input.
	AudioSelectors map[string]AudioSelector

	// Use captions selectors to specify the captions data from your input that you use
	// in your outputs. You can use up to 20 captions selectors per input.
	CaptionSelectors map[string]CaptionSelector

	// Use Cropping selection (crop) to specify the video area that the service will
	// include in the output video frame. If you specify a value here, it will override
	// any value that you specify in the output setting Cropping selection (crop).
	Crop *Rectangle

	// Enable Deblock (InputDeblockFilter) to produce smoother motion in the output.
	// Default is disabled. Only manually controllable for MPEG2 and uncompressed video
	// inputs.
	DeblockFilter InputDeblockFilter

	// Enable Denoise (InputDenoiseFilter) to filter noise from the input. Default is
	// disabled. Only applicable to MPEG2, H.264, H.265, and uncompressed video inputs.
	DenoiseFilter InputDenoiseFilter

	// Specify how the transcoding service applies the denoise and deblock filters. You
	// must also enable the filters separately, with Denoise (InputDenoiseFilter) and
	// Deblock (InputDeblockFilter). * Auto - The transcoding service determines
	// whether to apply filtering, depending on input type and quality. * Disable - The
	// input is not filtered. This is true even if you use the API to enable them in
	// (InputDeblockFilter) and (InputDeblockFilter). * Force - The input is filtered
	// regardless of input type.
	FilterEnable InputFilterEnable

	// Use Filter strength (FilterStrength) to adjust the magnitude the input filter
	// settings (Deblock and Denoise). The range is -5 to 5. Default is 0.
	FilterStrength int32

	// Enable the image inserter feature to include a graphic overlay on your video.
	// Enable or disable this feature for each input individually. This setting is
	// disabled by default.
	ImageInserter *ImageInserter

	// (InputClippings) contains sets of start and end times that together specify a
	// portion of the input to be used in the outputs. If you provide only a start
	// time, the clip will be the entire input from that point to the end. If you
	// provide only an end time, it will be the entire input up to that point. When you
	// specify more than one input clip, the transcoding service creates the job
	// outputs by stringing the clips together in the order you specify them.
	InputClippings []InputClipping

	// When you have a progressive segmented frame (PsF) input, use this setting to
	// flag the input as PsF. MediaConvert doesn't automatically detect PsF. Therefore,
	// flagging your input as PsF results in better preservation of video quality when
	// you do deinterlacing and frame rate conversion. If you don't specify, the
	// default value is Auto (AUTO). Auto is the correct setting for all inputs that
	// are not PsF. Don't set this value to PsF when your input is interlaced. Doing so
	// creates horizontal interlacing artifacts.
	InputScanType InputScanType

	// Use Selection placement (position) to define the video area in your output
	// frame. The area outside of the rectangle that you specify here is black. If you
	// specify a value here, it will override any value that you specify in the output
	// setting Selection placement (position). If you specify a value here, this will
	// override any AFD values in your input, even if you set Respond to AFD
	// (RespondToAfd) to Respond (RESPOND). If you specify a value here, this will
	// ignore anything that you specify for the setting Scaling Behavior
	// (scalingBehavior).
	Position *Rectangle

	// Use Program (programNumber) to select a specific program from within a
	// multi-program transport stream. Note that Quad 4K is not currently supported.
	// Default is the first program within the transport stream. If the program you
	// specify doesn't exist, the transcoding service will use this default.
	ProgramNumber int32

	// Set PSI control (InputPsiControl) for transport stream inputs to specify which
	// data the demux process to scans. * Ignore PSI - Scan all PIDs for audio and
	// video. * Use PSI - Scan only PSI data.
	PsiControl InputPsiControl

	// Use this Timecode source setting, located under the input settings
	// (InputTimecodeSource), to specify how the service counts input video frames.
	// This input frame count affects only the behavior of features that apply to a
	// single input at a time, such as input clipping and synchronizing some captions
	// formats. Choose Embedded (EMBEDDED) to use the timecodes in your input video.
	// Choose Start at zero (ZEROBASED) to start the first frame at zero. Choose
	// Specified start (SPECIFIEDSTART) to start the first frame at the timecode that
	// you specify in the setting Start timecode (timecodeStart). If you don't specify
	// a value for Timecode source, the service will use Embedded by default. For more
	// information about timecodes, see
	// https://docs.aws.amazon.com/console/mediaconvert/timecode.
	TimecodeSource InputTimecodeSource

	// Specify the timecode that you want the service to use for this input's initial
	// frame. To use this setting, you must set the Timecode source setting, located
	// under the input settings (InputTimecodeSource), to Specified start
	// (SPECIFIEDSTART). For more information about timecodes, see
	// https://docs.aws.amazon.com/console/mediaconvert/timecode.
	TimecodeStart *string

	// Selector for video.
	VideoSelector *VideoSelector
}

// Settings that specify how your still graphic overlay appears.
type InsertableImage struct {

	// Specify the time, in milliseconds, for the image to remain on the output video.
	// This duration includes fade-in time but not fade-out time.
	Duration int32

	// Specify the length of time, in milliseconds, between the Start time that you
	// specify for the image insertion and the time that the image appears at full
	// opacity. Full opacity is the level that you specify for the opacity setting. If
	// you don't specify a value for Fade-in, the image will appear abruptly at the
	// overlay start time.
	FadeIn int32

	// Specify the length of time, in milliseconds, between the end of the time that
	// you have specified for the image overlay Duration and when the overlaid image
	// has faded to total transparency. If you don't specify a value for Fade-out, the
	// image will disappear abruptly at the end of the inserted image duration.
	FadeOut int32

	// Specify the height of the inserted image in pixels. If you specify a value
	// that's larger than the video resolution height, the service will crop your
	// overlaid image to fit. To use the native height of the image, keep this setting
	// blank.
	Height int32

	// Specify the HTTP, HTTPS, or Amazon S3 location of the image that you want to
	// overlay on the video. Use a PNG or TGA file.
	ImageInserterInput *string

	// Specify the distance, in pixels, between the inserted image and the left edge of
	// the video frame. Required for any image overlay that you specify.
	ImageX int32

	// Specify the distance, in pixels, between the overlaid image and the top edge of
	// the video frame. Required for any image overlay that you specify.
	ImageY int32

	// Specify how overlapping inserted images appear. Images with higher values for
	// Layer appear on top of images with lower values for Layer.
	Layer int32

	// Use Opacity (Opacity) to specify how much of the underlying video shows through
	// the inserted image. 0 is transparent and 100 is fully opaque. Default is 50.
	Opacity int32

	// Specify the timecode of the frame that you want the overlay to first appear on.
	// This must be in timecode (HH:MM:SS:FF or HH:MM:SS;FF) format. Remember to take
	// into account your timecode source settings.
	StartTime *string

	// Specify the width of the inserted image in pixels. If you specify a value that's
	// larger than the video resolution width, the service will crop your overlaid
	// image to fit. To use the native width of the image, keep this setting blank.
	Width int32
}

// Each job converts an input file into an output file or files. For more
// information, see the User Guide at
// https://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
type Job struct {

	// The IAM role you use for creating this job. For details about permissions, see
	// the User Guide topic at the User Guide at
	// https://docs.aws.amazon.com/mediaconvert/latest/ug/iam-role.html
	//
	// This member is required.
	Role *string

	// JobSettings contains all the transcode settings for a job.
	//
	// This member is required.
	Settings *JobSettings

	// Accelerated transcoding can significantly speed up jobs with long, visually
	// complex content.
	AccelerationSettings *AccelerationSettings

	// Describes whether the current job is running with accelerated transcoding. For
	// jobs that have Acceleration (AccelerationMode) set to DISABLED,
	// AccelerationStatus is always NOT_APPLICABLE. For jobs that have Acceleration
	// (AccelerationMode) set to ENABLED or PREFERRED, AccelerationStatus is one of the
	// other states. AccelerationStatus is IN_PROGRESS initially, while the service
	// determines whether the input files and job settings are compatible with
	// accelerated transcoding. If they are, AcclerationStatus is ACCELERATED. If your
	// input files and job settings aren't compatible with accelerated transcoding, the
	// service either fails your job or runs it without accelerated transcoding,
	// depending on how you set Acceleration (AccelerationMode). When the service runs
	// your job without accelerated transcoding, AccelerationStatus is NOT_ACCELERATED.
	AccelerationStatus AccelerationStatus

	// An identifier for this resource that is unique within all of AWS.
	Arn *string

	// The tag type that AWS Billing and Cost Management will use to sort your AWS
	// Elemental MediaConvert costs on any billing report that you set up.
	BillingTagsSource BillingTagsSource

	// The time, in Unix epoch format in seconds, when the job got created.
	CreatedAt *time.Time

	// A job's phase can be PROBING, TRANSCODING OR UPLOADING
	CurrentPhase JobPhase

	// Error code for the job
	ErrorCode int32

	// Error message of Job
	ErrorMessage *string

	// Optional list of hop destinations.
	HopDestinations []HopDestination

	// A portion of the job's ARN, unique within your AWS Elemental MediaConvert
	// resources
	Id *string

	// An estimate of how far your job has progressed. This estimate is shown as a
	// percentage of the total time from when your job leaves its queue to when your
	// output files appear in your output Amazon S3 bucket. AWS Elemental MediaConvert
	// provides jobPercentComplete in CloudWatch STATUS_UPDATE events and in the
	// response to GetJob and ListJobs requests. The jobPercentComplete estimate is
	// reliable for the following input containers: Quicktime, Transport Stream, MP4,
	// and MXF. For some jobs, the service can't provide information about job
	// progress. In those cases, jobPercentComplete returns a null value.
	JobPercentComplete int32

	// The job template that the job is created from, if it is created from a job
	// template.
	JobTemplate *string

	// Provides messages from the service about jobs that you have already successfully
	// submitted.
	Messages *JobMessages

	// List of output group details
	OutputGroupDetails []OutputGroupDetail

	// Relative priority on the job.
	Priority int32

	// When you create a job, you can specify a queue to send it to. If you don't
	// specify, the job will go to the default queue. For more about queues, see the
	// User Guide topic at
	// https://docs.aws.amazon.com/mediaconvert/latest/ug/what-is.html
	Queue *string

	// The job's queue hopping history.
	QueueTransitions []QueueTransition

	// The number of times that the service automatically attempted to process your job
	// after encountering an error.
	RetryCount int32

	// Enable this setting when you run a test job to estimate how many reserved
	// transcoding slots (RTS) you need. When this is enabled, MediaConvert runs your
	// job from an on-demand queue with similar performance to what you will see with
	// one RTS in a reserved queue. This setting is disabled by default.
	SimulateReservedQueue SimulateReservedQueue

	// A job's status can be SUBMITTED, PROGRESSING, COMPLETE, CANCELED, or ERROR.
	Status JobStatus

	// Specify how often MediaConvert sends STATUS_UPDATE events to Amazon CloudWatch
	// Events. Set the interval, in seconds, between status updates. MediaConvert sends
	// an update at this interval from the time the service begins processing your job
	// to the time it completes the transcode or encounters an error.
	StatusUpdateInterval StatusUpdateInterval

	// Information about when jobs are submitted, started, and finished is specified in
	// Unix epoch format in seconds.
	Timing *Timing

	// User-defined metadata that you want to associate with an MediaConvert job. You
	// specify metadata in key/value pairs.
	UserMetadata map[string]string
}

// Provides messages from the service about jobs that you have already successfully
// submitted.
type JobMessages struct {

	// List of messages that are informational only and don't indicate a problem with
	// your job.
	Info []string

	// List of messages that warn about conditions that might cause your job not to run
	// or to fail.
	Warning []string
}

// JobSettings contains all the transcode settings for a job.
type JobSettings struct {

	// When specified, this offset (in milliseconds) is added to the input Ad Avail PTS
	// time.
	AdAvailOffset int32

	// Settings for ad avail blanking. Video can be blanked or overlaid with an image,
	// and audio muted during SCTE-35 triggered ad avails.
	AvailBlanking *AvailBlanking

	// Settings for Event Signaling And Messaging (ESAM).
	Esam *EsamSettings

	// Use Inputs (inputs) to define source file used in the transcode job. There can
	// be multiple inputs add in a job. These inputs will be concantenated together to
	// create the output.
	Inputs []Input

	// Overlay motion graphics on top of your video. The motion graphics that you
	// specify here appear on all outputs in all output groups.
	MotionImageInserter *MotionImageInserter

	// Settings for your Nielsen configuration. If you don't do Nielsen measurement and
	// analytics, ignore these settings. When you enable Nielsen configuration
	// (nielsenConfiguration), MediaConvert enables PCM to ID3 tagging for all outputs
	// in the job. To enable Nielsen configuration programmatically, include an
	// instance of nielsenConfiguration in your JSON job specification. Even if you
	// don't include any children of nielsenConfiguration, you still enable the
	// setting.
	NielsenConfiguration *NielsenConfiguration

	// Ignore these settings unless you are using Nielsen non-linear watermarking.
	// Specify the values that MediaConvert uses to generate and place Nielsen
	// watermarks in your output audio. In addition to specifying these values, you
	// also need to set up your cloud TIC server. These settings apply to every output
	// in your job. The MediaConvert implementation is currently with the following
	// Nielsen versions: Nielsen Watermark SDK Version 5.2.1 Nielsen NLM Watermark
	// Engine Version 1.2.7 Nielsen Watermark Authenticator [SID_TIC] Version [5.0.0]
	NielsenNonLinearWatermark *NielsenNonLinearWatermarkSettings

	// (OutputGroups) contains one group of settings for each set of outputs that share
	// a common package type. All unpackaged files (MPEG-4, MPEG-2 TS, Quicktime, MXF,
	// and no container) are grouped in a single output group as well. Required in
	// (OutputGroups) is a group of settings that apply to the whole group. This
	// required object depends on the value you set for (Type) under
	// (OutputGroups)>(OutputGroupSettings). Type, settings object pairs are as
	// follows. * FILE_GROUP_SETTINGS, FileGroupSettings * HLS_GROUP_SETTINGS,
	// HlsGroupSettings * DASH_ISO_GROUP_SETTINGS, DashIsoGroupSettings *
	// MS_SMOOTH_GROUP_SETTINGS, MsSmoothGroupSettings * CMAF_GROUP_SETTINGS,
	// CmafGroupSettings
	OutputGroups []OutputGroup

	// Contains settings used to acquire and adjust timecode information from inputs.
	TimecodeConfig *TimecodeConfig

	// Enable Timed metadata insertion (TimedMetadataInsertion) to include ID3 tags in
	// any HLS outputs. To include timed metadata, you must enable it here, enable it
	// in each output container, and specify tags and timecodes in ID3 insertion
	// (Id3Insertion) objects.
	TimedMetadataInsertion *TimedMetadataInsertion
}

// A job template is a pre-made set of encoding instructions that you can use to
// quickly create a job.
type JobTemplate struct {

	// A name you create for each job template. Each name must be unique within your
	// account.
	//
	// This member is required.
	Name *string

	// JobTemplateSettings contains all the transcode settings saved in the template
	// that will be applied to jobs created from it.
	//
	// This member is required.
	Settings *JobTemplateSettings

	// Accelerated transcoding can significantly speed up jobs with long, visually
	// complex content.
	AccelerationSettings *AccelerationSettings

	// An identifier for this resource that is unique within all of AWS.
	Arn *string

	// An optional category you create to organize your job templates.
	Category *string

	// The timestamp in epoch seconds for Job template creation.
	CreatedAt *time.Time

	// An optional description you create for each job template.
	Description *string

	// Optional list of hop destinations.
	HopDestinations []HopDestination

	// The timestamp in epoch seconds when the Job template was last updated.
	LastUpdated *time.Time

	// Relative priority on the job.
	Priority int32

	// Optional. The queue that jobs created from this template are assigned to. If you
	// don't specify this, jobs will go to the default queue.
	Queue *string

	// Specify how often MediaConvert sends STATUS_UPDATE events to Amazon CloudWatch
	// Events. Set the interval, in seconds, between status updates. MediaConvert sends
	// an update at this interval from the time the service begins processing your job
	// to the time it completes the transcode or encounters an error.
	StatusUpdateInterval StatusUpdateInterval

	// A job template can be of two types: system or custom. System or built-in job
	// templates can't be modified or deleted by the user.
	Type Type
}

// JobTemplateSettings contains all the transcode settings saved in the template
// that will be applied to jobs created from it.
type JobTemplateSettings struct {

	// When specified, this offset (in milliseconds) is added to the input Ad Avail PTS
	// time.
	AdAvailOffset int32

	// Settings for ad avail blanking. Video can be blanked or overlaid with an image,
	// and audio muted during SCTE-35 triggered ad avails.
	AvailBlanking *AvailBlanking

	// Settings for Event Signaling And Messaging (ESAM).
	Esam *EsamSettings

	// Use Inputs (inputs) to define the source file used in the transcode job. There
	// can only be one input in a job template. Using the API, you can include multiple
	// inputs when referencing a job template.
	Inputs []InputTemplate

	// Overlay motion graphics on top of your video. The motion graphics that you
	// specify here appear on all outputs in all output groups.
	MotionImageInserter *MotionImageInserter

	// Settings for your Nielsen configuration. If you don't do Nielsen measurement and
	// analytics, ignore these settings. When you enable Nielsen configuration
	// (nielsenConfiguration), MediaConvert enables PCM to ID3 tagging for all outputs
	// in the job. To enable Nielsen configuration programmatically, include an
	// instance of nielsenConfiguration in your JSON job specification. Even if you
	// don't include any children of nielsenConfiguration, you still enable the
	// setting.
	NielsenConfiguration *NielsenConfiguration

	// Ignore these settings unless you are using Nielsen non-linear watermarking.
	// Specify the values that MediaConvert uses to generate and place Nielsen
	// watermarks in your output audio. In addition to specifying these values, you
	// also need to set up your cloud TIC server. These settings apply to every output
	// in your job. The MediaConvert implementation is currently with the following
	// Nielsen versions: Nielsen Watermark SDK Version 5.2.1 Nielsen NLM Watermark
	// Engine Version 1.2.7 Nielsen Watermark Authenticator [SID_TIC] Version [5.0.0]
	NielsenNonLinearWatermark *NielsenNonLinearWatermarkSettings

	// (OutputGroups) contains one group of settings for each set of outputs that share
	// a common package type. All unpackaged files (MPEG-4, MPEG-2 TS, Quicktime, MXF,
	// and no container) are grouped in a single output group as well. Required in
	// (OutputGroups) is a group of settings that apply to the whole group. This
	// required object depends on the value you set for (Type) under
	// (OutputGroups)>(OutputGroupSettings). Type, settings object pairs are as
	// follows. * FILE_GROUP_SETTINGS, FileGroupSettings * HLS_GROUP_SETTINGS,
	// HlsGroupSettings * DASH_ISO_GROUP_SETTINGS, DashIsoGroupSettings *
	// MS_SMOOTH_GROUP_SETTINGS, MsSmoothGroupSettings * CMAF_GROUP_SETTINGS,
	// CmafGroupSettings
	OutputGroups []OutputGroup

	// Contains settings used to acquire and adjust timecode information from inputs.
	TimecodeConfig *TimecodeConfig

	// Enable Timed metadata insertion (TimedMetadataInsertion) to include ID3 tags in
	// any HLS outputs. To include timed metadata, you must enable it here, enable it
	// in each output container, and specify tags and timecodes in ID3 insertion
	// (Id3Insertion) objects.
	TimedMetadataInsertion *TimedMetadataInsertion
}

// Settings for SCTE-35 signals from ESAM. Include this in your job settings to put
// SCTE-35 markers in your HLS and transport stream outputs at the insertion points
// that you specify in an ESAM XML document. Provide the document in the setting
// SCC XML (sccXml).
type M2tsScte35Esam struct {

	// Packet Identifier (PID) of the SCTE-35 stream in the transport stream generated
	// by ESAM.
	Scte35EsamPid int32
}

// MPEG-2 TS container settings. These apply to outputs in a File output group when
// the output's container (ContainerType) is MPEG-2 Transport Stream (M2TS). In
// these assets, data is organized by the program map table (PMT). Each transport
// stream program contains subsets of data, including audio, video, and metadata.
// Each of these subsets of data has a numerical label called a packet identifier
// (PID). Each transport stream program corresponds to one MediaConvert output. The
// PMT lists the types of data in a program along with their PID. Downstream
// systems and players use the program map table to look up the PID for each type
// of data it accesses and then uses the PIDs to locate specific data within the
// asset.
type M2tsSettings struct {

	// Selects between the DVB and ATSC buffer models for Dolby Digital audio.
	AudioBufferModel M2tsAudioBufferModel

	// The number of audio frames to insert for each PES packet.
	AudioFramesPerPes int32

	// Specify the packet identifiers (PIDs) for any elementary audio streams you
	// include in this output. Specify multiple PIDs as a JSON array. Default is the
	// range 482-492.
	AudioPids []int32

	// Specify the output bitrate of the transport stream in bits per second. Setting
	// to 0 lets the muxer automatically determine the appropriate bitrate. Other
	// common values are 3750000, 7500000, and 15000000.
	Bitrate int32

	// Controls what buffer model to use for accurate interleaving. If set to
	// MULTIPLEX, use multiplex buffer model. If set to NONE, this can lead to lower
	// latency, but low-memory devices may not be able to play back the stream without
	// interruptions.
	BufferModel M2tsBufferModel

	// Inserts DVB Network Information Table (NIT) at the specified table repetition
	// interval.
	DvbNitSettings *DvbNitSettings

	// Inserts DVB Service Description Table (NIT) at the specified table repetition
	// interval.
	DvbSdtSettings *DvbSdtSettings

	// Specify the packet identifiers (PIDs) for DVB subtitle data included in this
	// output. Specify multiple PIDs as a JSON array. Default is the range 460-479.
	DvbSubPids []int32

	// Inserts DVB Time and Date Table (TDT) at the specified table repetition
	// interval.
	DvbTdtSettings *DvbTdtSettings

	// Specify the packet identifier (PID) for DVB teletext data you include in this
	// output. Default is 499.
	DvbTeletextPid int32

	// When set to VIDEO_AND_FIXED_INTERVALS, audio EBP markers will be added to
	// partitions 3 and 4. The interval between these additional markers will be fixed,
	// and will be slightly shorter than the video EBP marker interval. When set to
	// VIDEO_INTERVAL, these additional markers will not be inserted. Only applicable
	// when EBP segmentation markers are is selected (segmentationMarkers is EBP or
	// EBP_LEGACY).
	EbpAudioInterval M2tsEbpAudioInterval

	// Selects which PIDs to place EBP markers on. They can either be placed only on
	// the video PID, or on both the video PID and all audio PIDs. Only applicable when
	// EBP segmentation markers are is selected (segmentationMarkers is EBP or
	// EBP_LEGACY).
	EbpPlacement M2tsEbpPlacement

	// Controls whether to include the ES Rate field in the PES header.
	EsRateInPes M2tsEsRateInPes

	// Keep the default value (DEFAULT) unless you know that your audio EBP markers are
	// incorrectly appearing before your video EBP markers. To correct this problem,
	// set this value to Force (FORCE).
	ForceTsVideoEbpOrder M2tsForceTsVideoEbpOrder

	// The length, in seconds, of each fragment. Only used with EBP markers.
	FragmentTime float64

	// Specify the maximum time, in milliseconds, between Program Clock References
	// (PCRs) inserted into the transport stream.
	MaxPcrInterval int32

	// When set, enforces that Encoder Boundary Points do not come within the specified
	// time interval of each other by looking ahead at input video. If another EBP is
	// going to come in within the specified time interval, the current EBP is not
	// emitted, and the segment is "stretched" to the next marker. The lookahead value
	// does not add latency to the system. The Live Event must be configured elsewhere
	// to create sufficient latency to make the lookahead accurate.
	MinEbpInterval int32

	// If INSERT, Nielsen inaudible tones for media tracking will be detected in the
	// input audio and an equivalent ID3 tag will be inserted in the output.
	NielsenId3 M2tsNielsenId3

	// Value in bits per second of extra null packets to insert into the transport
	// stream. This can be used if a downstream encryption system requires periodic
	// null packets.
	NullPacketBitrate float64

	// The number of milliseconds between instances of this table in the output
	// transport stream.
	PatInterval int32

	// When set to PCR_EVERY_PES_PACKET, a Program Clock Reference value is inserted
	// for every Packetized Elementary Stream (PES) header. This is effective only when
	// the PCR PID is the same as the video or audio elementary stream.
	PcrControl M2tsPcrControl

	// Specify the packet identifier (PID) for the program clock reference (PCR) in
	// this output. If you do not specify a value, the service will use the value for
	// Video PID (VideoPid).
	PcrPid int32

	// Specify the number of milliseconds between instances of the program map table
	// (PMT) in the output transport stream.
	PmtInterval int32

	// Specify the packet identifier (PID) for the program map table (PMT) itself.
	// Default is 480.
	PmtPid int32

	// Specify the packet identifier (PID) of the private metadata stream. Default is
	// 503.
	PrivateMetadataPid int32

	// Use Program number (programNumber) to specify the program number used in the
	// program map table (PMT) for this output. Default is 1. Program numbers and
	// program map tables are parts of MPEG-2 transport stream containers, used for
	// organizing data.
	ProgramNumber int32

	// When set to CBR, inserts null packets into transport stream to fill specified
	// bitrate. When set to VBR, the bitrate setting acts as the maximum bitrate, but
	// the output will not be padded up to that bitrate.
	RateMode M2tsRateMode

	// Include this in your job settings to put SCTE-35 markers in your HLS and
	// transport stream outputs at the insertion points that you specify in an ESAM XML
	// document. Provide the document in the setting SCC XML (sccXml).
	Scte35Esam *M2tsScte35Esam

	// Specify the packet identifier (PID) of the SCTE-35 stream in the transport
	// stream.
	Scte35Pid int32

	// For SCTE-35 markers from your input-- Choose Passthrough (PASSTHROUGH) if you
	// want SCTE-35 markers that appear in your input to also appear in this output.
	// Choose None (NONE) if you don't want SCTE-35 markers in this output. For SCTE-35
	// markers from an ESAM XML document-- Choose None (NONE). Also provide the ESAM
	// XML as a string in the setting Signal processing notification XML (sccXml). Also
	// enable ESAM SCTE-35 (include the property scte35Esam).
	Scte35Source M2tsScte35Source

	// Inserts segmentation markers at each segmentation_time period. rai_segstart sets
	// the Random Access Indicator bit in the adaptation field. rai_adapt sets the RAI
	// bit and adds the current timecode in the private data bytes. psi_segstart
	// inserts PAT and PMT tables at the start of segments. ebp adds Encoder Boundary
	// Point information to the adaptation field as per OpenCable specification
	// OC-SP-EBP-I01-130118. ebp_legacy adds Encoder Boundary Point information to the
	// adaptation field using a legacy proprietary format.
	SegmentationMarkers M2tsSegmentationMarkers

	// The segmentation style parameter controls how segmentation markers are inserted
	// into the transport stream. With avails, it is possible that segments may be
	// truncated, which can influence where future segmentation markers are inserted.
	// When a segmentation style of "reset_cadence" is selected and a segment is
	// truncated due to an avail, we will reset the segmentation cadence. This means
	// the subsequent segment will have a duration of of $segmentation_time seconds.
	// When a segmentation style of "maintain_cadence" is selected and a segment is
	// truncated due to an avail, we will not reset the segmentation cadence. This
	// means the subsequent segment will likely be truncated as well. However, all
	// segments after that will have a duration of $segmentation_time seconds. Note
	// that EBP lookahead is a slight exception to this rule.
	SegmentationStyle M2tsSegmentationStyle

	// Specify the length, in seconds, of each segment. Required unless markers is set
	// to none.
	SegmentationTime float64

	// Specify the packet identifier (PID) for timed metadata in this output. Default
	// is 502.
	TimedMetadataPid int32

	// Specify the ID for the transport stream itself in the program map table for this
	// output. Transport stream IDs and program map tables are parts of MPEG-2
	// transport stream containers, used for organizing data.
	TransportStreamId int32

	// Specify the packet identifier (PID) of the elementary video stream in the
	// transport stream.
	VideoPid int32
}

// Settings for TS segments in HLS
type M3u8Settings struct {

	// The number of audio frames to insert for each PES packet.
	AudioFramesPerPes int32

	// Packet Identifier (PID) of the elementary audio stream(s) in the transport
	// stream. Multiple values are accepted, and can be entered in ranges and/or by
	// comma separation.
	AudioPids []int32

	// If INSERT, Nielsen inaudible tones for media tracking will be detected in the
	// input audio and an equivalent ID3 tag will be inserted in the output.
	NielsenId3 M3u8NielsenId3

	// The number of milliseconds between instances of this table in the output
	// transport stream.
	PatInterval int32

	// When set to PCR_EVERY_PES_PACKET a Program Clock Reference value is inserted for
	// every Packetized Elementary Stream (PES) header. This parameter is effective
	// only when the PCR PID is the same as the video or audio elementary stream.
	PcrControl M3u8PcrControl

	// Packet Identifier (PID) of the Program Clock Reference (PCR) in the transport
	// stream. When no value is given, the encoder will assign the same value as the
	// Video PID.
	PcrPid int32

	// The number of milliseconds between instances of this table in the output
	// transport stream.
	PmtInterval int32

	// Packet Identifier (PID) for the Program Map Table (PMT) in the transport stream.
	PmtPid int32

	// Packet Identifier (PID) of the private metadata stream in the transport stream.
	PrivateMetadataPid int32

	// The value of the program number field in the Program Map Table.
	ProgramNumber int32

	// Packet Identifier (PID) of the SCTE-35 stream in the transport stream.
	Scte35Pid int32

	// For SCTE-35 markers from your input-- Choose Passthrough (PASSTHROUGH) if you
	// want SCTE-35 markers that appear in your input to also appear in this output.
	// Choose None (NONE) if you don't want SCTE-35 markers in this output. For SCTE-35
	// markers from an ESAM XML document-- Choose None (NONE) if you don't want
	// manifest conditioning. Choose Passthrough (PASSTHROUGH) and choose Ad markers
	// (adMarkers) if you do want manifest conditioning. In both cases, also provide
	// the ESAM XML as a string in the setting Signal processing notification XML
	// (sccXml).
	Scte35Source M3u8Scte35Source

	// Applies only to HLS outputs. Use this setting to specify whether the service
	// inserts the ID3 timed metadata from the input in this output.
	TimedMetadata TimedMetadata

	// Packet Identifier (PID) of the timed metadata stream in the transport stream.
	TimedMetadataPid int32

	// The value of the transport stream ID field in the Program Map Table.
	TransportStreamId int32

	// Packet Identifier (PID) of the elementary video stream in the transport stream.
	VideoPid int32
}

// Overlay motion graphics on top of your video at the time that you specify.
type MotionImageInserter struct {

	// If your motion graphic asset is a .mov file, keep this setting unspecified. If
	// your motion graphic asset is a series of .png files, specify the frame rate of
	// the overlay in frames per second, as a fraction. For example, specify 24 fps as
	// 24/1. Make sure that the number of images in your series matches the frame rate
	// and your intended overlay duration. For example, if you want a 30-second overlay
	// at 30 fps, you should have 900 .png images. This overlay frame rate doesn't need
	// to match the frame rate of the underlying video.
	Framerate *MotionImageInsertionFramerate

	// Specify the .mov file or series of .png files that you want to overlay on your
	// video. For .png files, provide the file name of the first file in the series.
	// Make sure that the names of the .png files end with sequential numbers that
	// specify the order that they are played in. For example, overlay_000.png,
	// overlay_001.png, overlay_002.png, and so on. The sequence must start at zero,
	// and each image file name must have the same number of digits. Pad your initial
	// file names with enough zeros to complete the sequence. For example, if the first
	// image is overlay_0.png, there can be only 10 images in the sequence, with the
	// last image being overlay_9.png. But if the first image is overlay_00.png, there
	// can be 100 images in the sequence.
	Input *string

	// Choose the type of motion graphic asset that you are providing for your overlay.
	// You can choose either a .mov file or a series of .png files.
	InsertionMode MotionImageInsertionMode

	// Use Offset to specify the placement of your motion graphic overlay on the video
	// frame. Specify in pixels, from the upper-left corner of the frame. If you don't
	// specify an offset, the service scales your overlay to the full size of the
	// frame. Otherwise, the service inserts the overlay at its native resolution and
	// scales the size up or down with any video scaling.
	Offset *MotionImageInsertionOffset

	// Specify whether your motion graphic overlay repeats on a loop or plays only
	// once.
	Playback MotionImagePlayback

	// Specify when the motion overlay begins. Use timecode format (HH:MM:SS:FF or
	// HH:MM:SS;FF). Make sure that the timecode you provide here takes into account
	// how you have set up your timecode configuration under both job settings and
	// input settings. The simplest way to do that is to set both to start at 0. If you
	// need to set up your job to follow timecodes embedded in your source that don't
	// start at zero, make sure that you specify a start time that is after the first
	// embedded timecode. For more information, see
	// https://docs.aws.amazon.com/mediaconvert/latest/ug/setting-up-timecode.html Find
	// job-wide and input timecode configuration settings in your JSON job settings
	// specification at settings>timecodeConfig>source and
	// settings>inputs>timecodeSource.
	StartTime *string
}

// For motion overlays that don't have a built-in frame rate, specify the frame
// rate of the overlay in frames per second, as a fraction. For example, specify 24
// fps as 24/1. The overlay frame rate doesn't need to match the frame rate of the
// underlying video.
type MotionImageInsertionFramerate struct {

	// The bottom of the fraction that expresses your overlay frame rate. For example,
	// if your frame rate is 24 fps, set this value to 1.
	FramerateDenominator int32

	// The top of the fraction that expresses your overlay frame rate. For example, if
	// your frame rate is 24 fps, set this value to 24.
	FramerateNumerator int32
}

// Specify the offset between the upper-left corner of the video frame and the top
// left corner of the overlay.
type MotionImageInsertionOffset struct {

	// Set the distance, in pixels, between the overlay and the left edge of the video
	// frame.
	ImageX int32

	// Set the distance, in pixels, between the overlay and the top edge of the video
	// frame.
	ImageY int32
}

// Settings for MOV Container.
type MovSettings struct {

	// When enabled, include 'clap' atom if appropriate for the video output settings.
	ClapAtom MovClapAtom

	// When enabled, file composition times will start at zero, composition times in
	// the 'ctts' (composition time to sample) box for B-frames will be negative, and a
	// 'cslg' (composition shift least greatest) box will be included per 14496-1
	// amendment 1. This improves compatibility with Apple players and tools.
	CslgAtom MovCslgAtom

	// When set to XDCAM, writes MPEG2 video streams into the QuickTime file using
	// XDCAM fourcc codes. This increases compatibility with Apple editors and players,
	// but may decrease compatibility with other players. Only applicable when the
	// video codec is MPEG2.
	Mpeg2FourCCControl MovMpeg2FourCCControl

	// To make this output compatible with Omenon, keep the default value, OMNEON.
	// Unless you need Omneon compatibility, set this value to NONE. When you keep the
	// default value, OMNEON, MediaConvert increases the length of the edit list atom.
	// This might cause file rejections when a recipient of the output file doesn't
	// expct this extra padding.
	PaddingControl MovPaddingControl

	// Always keep the default value (SELF_CONTAINED) for this setting.
	Reference MovReference
}

// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
// value MP2.
type Mp2Settings struct {

	// Specify the average bitrate in bits per second.
	Bitrate int32

	// Set Channels to specify the number of channels in this output audio track.
	// Choosing Mono in the console will give you 1 output channel; choosing Stereo
	// will give you 2. In the API, valid values are 1 and 2.
	Channels int32

	// Sample rate in hz.
	SampleRate int32
}

// Required when you set Codec, under AudioDescriptions>CodecSettings, to the value
// MP3.
type Mp3Settings struct {

	// Specify the average bitrate in bits per second.
	Bitrate int32

	// Specify the number of channels in this output audio track. Choosing Mono on the
	// console gives you 1 output channel; choosing Stereo gives you 2. In the API,
	// valid values are 1 and 2.
	Channels int32

	// Specify whether the service encodes this MP3 audio output with a constant
	// bitrate (CBR) or a variable bitrate (VBR).
	RateControlMode Mp3RateControlMode

	// Sample rate in hz.
	SampleRate int32

	// Required when you set Bitrate control mode (rateControlMode) to VBR. Specify the
	// audio quality of this MP3 output from 0 (highest quality) to 9 (lowest quality).
	VbrQuality int32
}

// Settings for MP4 container. You can create audio-only AAC outputs with this
// container.
type Mp4Settings struct {

	// When enabled, file composition times will start at zero, composition times in
	// the 'ctts' (composition time to sample) box for B-frames will be negative, and a
	// 'cslg' (composition shift least greatest) box will be included per 14496-1
	// amendment 1. This improves compatibility with Apple players and tools.
	CslgAtom Mp4CslgAtom

	// Ignore this setting unless compliance to the CTTS box version specification
	// matters in your workflow. Specify a value of 1 to set your CTTS box version to 1
	// and make your output compliant with the specification. When you specify a value
	// of 1, you must also set CSLG atom (cslgAtom) to the value INCLUDE. Keep the
	// default value 0 to set your CTTS box version to 0. This can provide backward
	// compatibility for some players and packagers.
	CttsVersion int32

	// Inserts a free-space box immediately after the moov box.
	FreeSpaceBox Mp4FreeSpaceBox

	// If set to PROGRESSIVE_DOWNLOAD, the MOOV atom is relocated to the beginning of
	// the archive as required for progressive downloading. Otherwise it is placed
	// normally at the end.
	MoovPlacement Mp4MoovPlacement

	// Overrides the "Major Brand" field in the output file. Usually not necessary to
	// specify.
	Mp4MajorBrand *string
}

// Settings for MP4 segments in DASH
type MpdSettings struct {

	// Use this setting only in DASH output groups that include sidecar TTML or IMSC
	// captions. You specify sidecar captions in a separate output from your audio and
	// video. Choose Raw (RAW) for captions in a single XML file in a raw container.
	// Choose Fragmented MPEG-4 (FRAGMENTED_MP4) for captions in XML format contained
	// within fragmented MP4 files. This set of fragmented MP4 files is separate from
	// your video and audio fragmented MP4 files.
	CaptionContainerType MpdCaptionContainerType

	// Use this setting only when you specify SCTE-35 markers from ESAM. Choose INSERT
	// to put SCTE-35 markers in this output at the insertion points that you specify
	// in an ESAM XML document. Provide the document in the setting SCC XML (sccXml).
	Scte35Esam MpdScte35Esam

	// Ignore this setting unless you have SCTE-35 markers in your input video file.
	// Choose Passthrough (PASSTHROUGH) if you want SCTE-35 markers that appear in your
	// input to also appear in this output. Choose None (NONE) if you don't want those
	// SCTE-35 markers in this output.
	Scte35Source MpdScte35Source
}

// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
// value MPEG2.
type Mpeg2Settings struct {

	// Specify the strength of any adaptive quantization filters that you enable. The
	// value that you choose here applies to the following settings: Spatial adaptive
	// quantization (spatialAdaptiveQuantization), and Temporal adaptive quantization
	// (temporalAdaptiveQuantization).
	AdaptiveQuantization Mpeg2AdaptiveQuantization

	// Specify the average bitrate in bits per second. Required for VBR and CBR. For MS
	// Smooth outputs, bitrates must be unique when rounded down to the nearest
	// multiple of 1000.
	Bitrate int32

	// Use Level (Mpeg2CodecLevel) to set the MPEG-2 level for the video output.
	CodecLevel Mpeg2CodecLevel

	// Use Profile (Mpeg2CodecProfile) to set the MPEG-2 profile for the video output.
	CodecProfile Mpeg2CodecProfile

	// Choose Adaptive to improve subjective video quality for high-motion content.
	// This will cause the service to use fewer B-frames (which infer information based
	// on other frames) for high-motion portions of the video and more B-frames for
	// low-motion portions. The maximum number of B-frames is limited by the value you
	// provide for the setting B frames between reference frames
	// (numberBFramesBetweenReferenceFrames).
	DynamicSubGop Mpeg2DynamicSubGop

	// If you are using the console, use the Framerate setting to specify the frame
	// rate for this output. If you want to keep the same frame rate as the input
	// video, choose Follow source. If you want to do frame rate conversion, choose a
	// frame rate from the dropdown list or choose Custom. The framerates shown in the
	// dropdown list are decimal approximations of fractions. If you choose Custom,
	// specify your frame rate as a fraction. If you are creating your transcoding job
	// specification as a JSON file without the console, use FramerateControl to
	// specify which value the service uses for the frame rate for this output. Choose
	// INITIALIZE_FROM_SOURCE if you want the service to use the frame rate from the
	// input. Choose SPECIFIED if you want the service to use the frame rate you
	// specify in the settings FramerateNumerator and FramerateDenominator.
	FramerateControl Mpeg2FramerateControl

	// Choose the method that you want MediaConvert to use when increasing or
	// decreasing the frame rate. We recommend using drop duplicate (DUPLICATE_DROP)
	// for numerically simple conversions, such as 60 fps to 30 fps. For numerically
	// complex conversions, you can use interpolate (INTERPOLATE) to avoid stutter.
	// This results in a smooth picture, but might introduce undesirable video
	// artifacts. For complex frame rate conversions, especially if your source video
	// has already been converted from its original cadence, use FrameFormer
	// (FRAMEFORMER) to do motion-compensated interpolation. FrameFormer chooses the
	// best conversion method frame by frame. Note that using FrameFormer increases the
	// transcoding time and incurs a significant add-on cost.
	FramerateConversionAlgorithm Mpeg2FramerateConversionAlgorithm

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateDenominator to specify the denominator of this fraction. In this
	// example, use 1001 for the value of FramerateDenominator. When you use the
	// console for transcode jobs that use frame rate conversion, provide the value as
	// a decimal number for Framerate. In this example, specify 23.976.
	FramerateDenominator int32

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateNumerator to specify the numerator of this fraction. In this example,
	// use 24000 for the value of FramerateNumerator. When you use the console for
	// transcode jobs that use frame rate conversion, provide the value as a decimal
	// number for Framerate. In this example, specify 23.976.
	FramerateNumerator int32

	// Frequency of closed GOPs. In streaming applications, it is recommended that this
	// be set to 1 so a decoder joining mid-stream will receive an IDR frame as quickly
	// as possible. Setting this value to 0 will break output segmenting.
	GopClosedCadence int32

	// GOP Length (keyframe interval) in frames or seconds. Must be greater than zero.
	GopSize float64

	// Indicates if the GOP Size in MPEG2 is specified in frames or seconds. If seconds
	// the system will convert the GOP Size into a frame count at run time.
	GopSizeUnits Mpeg2GopSizeUnits

	// Percentage of the buffer that should initially be filled (HRD buffer model).
	HrdBufferInitialFillPercentage int32

	// Size of buffer (HRD buffer model) in bits. For example, enter five megabits as
	// 5000000.
	HrdBufferSize int32

	// Choose the scan line type for the output. Keep the default value, Progressive
	// (PROGRESSIVE) to create a progressive output, regardless of the scan type of
	// your input. Use Top field first (TOP_FIELD) or Bottom field first (BOTTOM_FIELD)
	// to create an output that's interlaced with the same field polarity throughout.
	// Use Follow, default top (FOLLOW_TOP_FIELD) or Follow, default bottom
	// (FOLLOW_BOTTOM_FIELD) to produce outputs with the same field polarity as the
	// source. For jobs that have multiple inputs, the output field polarity might
	// change over the course of the output. Follow behavior depends on the input scan
	// type. If the source is interlaced, the output will be interlaced with the same
	// polarity as the source. If the source is progressive, the output will be
	// interlaced with top field bottom field first, depending on which of the Follow
	// options you choose.
	InterlaceMode Mpeg2InterlaceMode

	// Use Intra DC precision (Mpeg2IntraDcPrecision) to set quantization precision for
	// intra-block DC coefficients. If you choose the value auto, the service will
	// automatically select the precision based on the per-frame compression ratio.
	IntraDcPrecision Mpeg2IntraDcPrecision

	// Maximum bitrate in bits/second. For example, enter five megabits per second as
	// 5000000.
	MaxBitrate int32

	// Enforces separation between repeated (cadence) I-frames and I-frames inserted by
	// Scene Change Detection. If a scene change I-frame is within I-interval frames of
	// a cadence I-frame, the GOP is shrunk and/or stretched to the scene change
	// I-frame. GOP stretch requires enabling lookahead as well as setting I-interval.
	// The normal cadence resumes for the next GOP. This setting is only used when
	// Scene Change Detect is enabled. Note: Maximum GOP stretch = GOP size +
	// Min-I-interval - 1
	MinIInterval int32

	// Number of B-frames between reference frames.
	NumberBFramesBetweenReferenceFrames int32

	// Optional. Specify how the service determines the pixel aspect ratio (PAR) for
	// this output. The default behavior, Follow source (INITIALIZE_FROM_SOURCE), uses
	// the PAR from your input video for your output. To specify a different PAR in the
	// console, choose any value other than Follow source. To specify a different PAR
	// by editing the JSON job specification, choose SPECIFIED. When you choose
	// SPECIFIED for this setting, you must also specify values for the parNumerator
	// and parDenominator settings.
	ParControl Mpeg2ParControl

	// Required when you set Pixel aspect ratio (parControl) to SPECIFIED. On the
	// console, this corresponds to any value other than Follow source. When you
	// specify an output pixel aspect ratio (PAR) that is different from your input
	// video PAR, provide your output PAR as a ratio. For example, for D1/DV NTSC
	// widescreen, you would specify the ratio 40:33. In this example, the value for
	// parDenominator is 33.
	ParDenominator int32

	// Required when you set Pixel aspect ratio (parControl) to SPECIFIED. On the
	// console, this corresponds to any value other than Follow source. When you
	// specify an output pixel aspect ratio (PAR) that is different from your input
	// video PAR, provide your output PAR as a ratio. For example, for D1/DV NTSC
	// widescreen, you would specify the ratio 40:33. In this example, the value for
	// parNumerator is 40.
	ParNumerator int32

	// Optional. Use Quality tuning level (qualityTuningLevel) to choose how you want
	// to trade off encoding speed for output video quality. The default behavior is
	// faster, lower quality, single-pass encoding.
	QualityTuningLevel Mpeg2QualityTuningLevel

	// Use Rate control mode (Mpeg2RateControlMode) to specifiy whether the bitrate is
	// variable (vbr) or constant (cbr).
	RateControlMode Mpeg2RateControlMode

	// Enable this setting to insert I-frames at scene changes that the service
	// automatically detects. This improves video quality and is enabled by default.
	SceneChangeDetect Mpeg2SceneChangeDetect

	// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
	// second (fps). Enable slow PAL to create a 25 fps output. When you enable slow
	// PAL, MediaConvert relabels the video frames to 25 fps and resamples your audio
	// to keep it synchronized with the video. Note that enabling this setting will
	// slightly reduce the duration of your video. Required settings: You must also set
	// Framerate to 25. In your JSON job specification, set (framerateControl) to
	// (SPECIFIED), (framerateNumerator) to 25 and (framerateDenominator) to 1.
	SlowPal Mpeg2SlowPal

	// Ignore this setting unless you need to comply with a specification that requires
	// a specific value. If you don't have a specification requirement, we recommend
	// that you adjust the softness of your output by using a lower value for the
	// setting Sharpness (sharpness) or by enabling a noise reducer filter
	// (noiseReducerFilter). The Softness (softness) setting specifies the quantization
	// matrices that the encoder uses. Keep the default value, 0, to use the AWS
	// Elemental default matrices. Choose a value from 17 to 128 to use planar
	// interpolation. Increasing values from 17 to 128 result in increasing reduction
	// of high-frequency data. The value 128 results in the softest video.
	Softness int32

	// Keep the default value, Enabled (ENABLED), to adjust quantization within each
	// frame based on spatial variation of content complexity. When you enable this
	// feature, the encoder uses fewer bits on areas that can sustain more distortion
	// with no noticeable visual degradation and uses more bits on areas where any
	// small distortion will be noticeable. For example, complex textured blocks are
	// encoded with fewer bits and smooth textured blocks are encoded with more bits.
	// Enabling this feature will almost always improve your video quality. Note,
	// though, that this feature doesn't take into account where the viewer's attention
	// is likely to be. If viewers are likely to be focusing their attention on a part
	// of the screen with a lot of complex texture, you might choose to disable this
	// feature. Related setting: When you enable spatial adaptive quantization, set the
	// value for Adaptive quantization (adaptiveQuantization) depending on your
	// content. For homogeneous content, such as cartoons and video games, set it to
	// Low. For content with a wider variety of textures, set it to High or Higher.
	SpatialAdaptiveQuantization Mpeg2SpatialAdaptiveQuantization

	// Specify whether this output's video uses the D10 syntax. Keep the default value
	// to not use the syntax. Related settings: When you choose D10 (D_10) for your MXF
	// profile (profile), you must also set this value to to D10 (D_10).
	Syntax Mpeg2Syntax

	// When you do frame rate conversion from 23.976 frames per second (fps) to 29.97
	// fps, and your output scan type is interlaced, you can optionally enable hard or
	// soft telecine to create a smoother picture. Hard telecine (HARD) produces a
	// 29.97i output. Soft telecine (SOFT) produces an output with a 23.976 output that
	// signals to the video player device to do the conversion during play back. When
	// you keep the default value, None (NONE), MediaConvert does a standard frame rate
	// conversion to 29.97 without doing anything with the field polarity to create a
	// smoother picture.
	Telecine Mpeg2Telecine

	// Keep the default value, Enabled (ENABLED), to adjust quantization within each
	// frame based on temporal variation of content complexity. When you enable this
	// feature, the encoder uses fewer bits on areas of the frame that aren't moving
	// and uses more bits on complex objects with sharp edges that move a lot. For
	// example, this feature improves the readability of text tickers on newscasts and
	// scoreboards on sports matches. Enabling this feature will almost always improve
	// your video quality. Note, though, that this feature doesn't take into account
	// where the viewer's attention is likely to be. If viewers are likely to be
	// focusing their attention on a part of the screen that doesn't have moving
	// objects with sharp edges, such as sports athletes' faces, you might choose to
	// disable this feature. Related setting: When you enable temporal quantization,
	// adjust the strength of the filter with the setting Adaptive quantization
	// (adaptiveQuantization).
	TemporalAdaptiveQuantization Mpeg2TemporalAdaptiveQuantization
}

// Specify the details for each additional Microsoft Smooth Streaming manifest that
// you want the service to generate for this output group. Each manifest can
// reference a different subset of outputs in the group.
type MsSmoothAdditionalManifest struct {

	// Specify a name modifier that the service adds to the name of this manifest to
	// make it different from the file names of the other main manifests in the output
	// group. For example, say that the default main manifest for your Microsoft Smooth
	// group is film-name.ismv. If you enter "-no-premium" for this setting, then the
	// file name the service generates for this top-level manifest is
	// film-name-no-premium.ismv.
	ManifestNameModifier *string

	// Specify the outputs that you want this additional top-level manifest to
	// reference.
	SelectedOutputs []string
}

// If you are using DRM, set DRM System (MsSmoothEncryptionSettings) to specify the
// value SpekeKeyProvider.
type MsSmoothEncryptionSettings struct {

	// If your output group type is HLS, DASH, or Microsoft Smooth, use these settings
	// when doing DRM encryption with a SPEKE-compliant key provider. If your output
	// group type is CMAF, use the SpekeKeyProviderCmaf settings instead.
	SpekeKeyProvider *SpekeKeyProvider
}

// Required when you set (Type) under (OutputGroups)>(OutputGroupSettings) to
// MS_SMOOTH_GROUP_SETTINGS.
type MsSmoothGroupSettings struct {

	// By default, the service creates one .ism Microsoft Smooth Streaming manifest for
	// each Microsoft Smooth Streaming output group in your job. This default manifest
	// references every output in the output group. To create additional manifests that
	// reference a subset of the outputs in the output group, specify a list of them
	// here.
	AdditionalManifests []MsSmoothAdditionalManifest

	// COMBINE_DUPLICATE_STREAMS combines identical audio encoding settings across a
	// Microsoft Smooth output group into a single audio stream.
	AudioDeduplication MsSmoothAudioDeduplication

	// Use Destination (Destination) to specify the S3 output location and the output
	// filename base. Destination accepts format identifiers. If you do not specify the
	// base filename in the URI, the service will use the filename of the input file.
	// If your job has multiple inputs, the service uses the filename of the first
	// input file.
	Destination *string

	// Settings associated with the destination. Will vary based on the type of
	// destination
	DestinationSettings *DestinationSettings

	// If you are using DRM, set DRM System (MsSmoothEncryptionSettings) to specify the
	// value SpekeKeyProvider.
	Encryption *MsSmoothEncryptionSettings

	// Use Fragment length (FragmentLength) to specify the mp4 fragment sizes in
	// seconds. Fragment length must be compatible with GOP size and frame rate.
	FragmentLength int32

	// Use Manifest encoding (MsSmoothManifestEncoding) to specify the encoding format
	// for the server and client manifest. Valid options are utf8 and utf16.
	ManifestEncoding MsSmoothManifestEncoding
}

// MXF settings
type MxfSettings struct {

	// Optional. When you have AFD signaling set up in your output video stream, use
	// this setting to choose whether to also include it in the MXF wrapper. Choose
	// Don't copy (NO_COPY) to exclude AFD signaling from the MXF wrapper. Choose Copy
	// from video stream (COPY_FROM_VIDEO) to copy the AFD values from the video stream
	// for this output to the MXF wrapper. Regardless of which option you choose, the
	// AFD values remain in the video stream. Related settings: To set up your output
	// to include or exclude AFD values, see AfdSignaling, under VideoDescription. On
	// the console, find AFD signaling under the output's video encoding settings.
	AfdSignaling MxfAfdSignaling

	// Specify the MXF profile, also called shim, for this output. When you choose
	// Auto, MediaConvert chooses a profile based on the video codec and resolution.
	// For a list of codecs supported with each MXF profile, see
	// https://docs.aws.amazon.com/mediaconvert/latest/ug/codecs-supported-with-each-mxf-profile.html.
	// For more information about the automatic selection behavior, see
	// https://docs.aws.amazon.com/mediaconvert/latest/ug/default-automatic-selection-of-mxf-profiles.html.
	Profile MxfProfile
}

// For forensic video watermarking, MediaConvert supports Nagra NexGuard File
// Marker watermarking. MediaConvert supports both PreRelease Content (NGPR/G2) and
// OTT Streaming workflows.
type NexGuardFileMarkerSettings struct {

	// Use the base64 license string that Nagra provides you. Enter it directly in your
	// JSON job specification or in the console. Required when you include Nagra
	// NexGuard File Marker watermarking (NexGuardWatermarkingSettings) in your job.
	License *string

	// Specify the payload ID that you want associated with this output. Valid values
	// vary depending on your Nagra NexGuard forensic watermarking workflow. Required
	// when you include Nagra NexGuard File Marker watermarking
	// (NexGuardWatermarkingSettings) in your job. For PreRelease Content (NGPR/G2),
	// specify an integer from 1 through 4,194,303. You must generate a unique ID for
	// each asset you watermark, and keep a record of which ID you have assigned to
	// each asset. Neither Nagra nor MediaConvert keep track of the relationship
	// between output files and your IDs. For OTT Streaming, create two adaptive
	// bitrate (ABR) stacks for each asset. Do this by setting up two output groups.
	// For one output group, set the value of Payload ID (payload) to 0 in every
	// output. For the other output group, set Payload ID (payload) to 1 in every
	// output.
	Payload int32

	// Enter one of the watermarking preset strings that Nagra provides you. Required
	// when you include Nagra NexGuard File Marker watermarking
	// (NexGuardWatermarkingSettings) in your job.
	Preset *string

	// Optional. Ignore this setting unless Nagra support directs you to specify a
	// value. When you don't specify a value here, the Nagra NexGuard library uses its
	// default value.
	Strength WatermarkingStrength
}

// Settings for your Nielsen configuration. If you don't do Nielsen measurement and
// analytics, ignore these settings. When you enable Nielsen configuration
// (nielsenConfiguration), MediaConvert enables PCM to ID3 tagging for all outputs
// in the job. To enable Nielsen configuration programmatically, include an
// instance of nielsenConfiguration in your JSON job specification. Even if you
// don't include any children of nielsenConfiguration, you still enable the
// setting.
type NielsenConfiguration struct {

	// Nielsen has discontinued the use of breakout code functionality. If you must
	// include this property, set the value to zero.
	BreakoutCode int32

	// Use Distributor ID (DistributorID) to specify the distributor ID that is
	// assigned to your organization by Neilsen.
	DistributorId *string
}

// Ignore these settings unless you are using Nielsen non-linear watermarking.
// Specify the values that MediaConvert uses to generate and place Nielsen
// watermarks in your output audio. In addition to specifying these values, you
// also need to set up your cloud TIC server. These settings apply to every output
// in your job. The MediaConvert implementation is currently with the following
// Nielsen versions: Nielsen Watermark SDK Version 5.2.1 Nielsen NLM Watermark
// Engine Version 1.2.7 Nielsen Watermark Authenticator [SID_TIC] Version [5.0.0]
type NielsenNonLinearWatermarkSettings struct {

	// Choose the type of Nielsen watermarks that you want in your outputs. When you
	// choose NAES 2 and NW (NAES2_AND_NW), you must provide a value for the setting
	// SID (sourceId). When you choose CBET (CBET), you must provide a value for the
	// setting CSID (cbetSourceId). When you choose NAES 2, NW, and CBET
	// (NAES2_AND_NW_AND_CBET), you must provide values for both of these settings.
	ActiveWatermarkProcess NielsenActiveWatermarkProcessType

	// Optional. Use this setting when you want the service to include an ADI file in
	// the Nielsen metadata .zip file. To provide an ADI file, store it in Amazon S3
	// and provide a URL to it here. The URL should be in the following format:
	// S3://bucket/path/ADI-file. For more information about the metadata .zip file,
	// see the setting Metadata destination (metadataDestination).
	AdiFilename *string

	// Use the asset ID that you provide to Nielsen to uniquely identify this asset.
	// Required for all Nielsen non-linear watermarking.
	AssetId *string

	// Use the asset name that you provide to Nielsen for this asset. Required for all
	// Nielsen non-linear watermarking.
	AssetName *string

	// Use the CSID that Nielsen provides to you. This CBET source ID should be unique
	// to your Nielsen account but common to all of your output assets that have CBET
	// watermarking. Required when you choose a value for the setting Watermark types
	// (ActiveWatermarkProcess) that includes CBET.
	CbetSourceId *string

	// Optional. If this asset uses an episode ID with Nielsen, provide it here.
	EpisodeId *string

	// Specify the Amazon S3 location where you want MediaConvert to save your Nielsen
	// non-linear metadata .zip file. This Amazon S3 bucket must be in the same Region
	// as the one where you do your MediaConvert transcoding. If you want to include an
	// ADI file in this .zip file, use the setting ADI file (adiFilename) to specify
	// it. MediaConvert delivers the Nielsen metadata .zip files only to your metadata
	// destination Amazon S3 bucket. It doesn't deliver the .zip files to Nielsen. You
	// are responsible for delivering the metadata .zip files to Nielsen.
	MetadataDestination *string

	// Use the SID that Nielsen provides to you. This source ID should be unique to
	// your Nielsen account but common to all of your output assets. Required for all
	// Nielsen non-linear watermarking. This ID should be unique to your Nielsen
	// account but common to all of your output assets. Required for all Nielsen
	// non-linear watermarking.
	SourceId int32

	// Required. Specify whether your source content already contains Nielsen
	// non-linear watermarks. When you set this value to Watermarked (WATERMARKED), the
	// service fails the job. Nielsen requires that you add non-linear watermarking to
	// only clean content that doesn't already have non-linear Nielsen watermarks.
	SourceWatermarkStatus NielsenSourceWatermarkStatusType

	// Specify the endpoint for the TIC server that you have deployed and configured in
	// the AWS Cloud. Required for all Nielsen non-linear watermarking. MediaConvert
	// can't connect directly to a TIC server. Instead, you must use API Gateway to
	// provide a RESTful interface between MediaConvert and a TIC server that you
	// deploy in your AWS account. For more information on deploying a TIC server in
	// your AWS account and the required API Gateway, contact Nielsen support.
	TicServerUrl *string

	// To create assets that have the same TIC values in each audio track, keep the
	// default value Share TICs (SAME_TICS_PER_TRACK). To create assets that have
	// unique TIC values for each audio track, choose Use unique TICs
	// (RESERVE_UNIQUE_TICS_PER_TRACK).
	UniqueTicPerAudioTrack NielsenUniqueTicPerAudioTrackType
}

// Enable the Noise reducer (NoiseReducer) feature to remove noise from your video
// output if necessary. Enable or disable this feature for each output
// individually. This setting is disabled by default. When you enable Noise reducer
// (NoiseReducer), you must also select a value for Noise reducer filter
// (NoiseReducerFilter).
type NoiseReducer struct {

	// Use Noise reducer filter (NoiseReducerFilter) to select one of the following
	// spatial image filtering functions. To use this setting, you must also enable
	// Noise reducer (NoiseReducer). * Bilateral preserves edges while reducing noise.
	// * Mean (softest), Gaussian, Lanczos, and Sharpen (sharpest) do convolution
	// filtering. * Conserve does min/max noise reduction. * Spatial does
	// frequency-domain filtering based on JND principles. * Temporal optimizes video
	// quality for complex motion.
	Filter NoiseReducerFilter

	// Settings for a noise reducer filter
	FilterSettings *NoiseReducerFilterSettings

	// Noise reducer filter settings for spatial filter.
	SpatialFilterSettings *NoiseReducerSpatialFilterSettings

	// Noise reducer filter settings for temporal filter.
	TemporalFilterSettings *NoiseReducerTemporalFilterSettings
}

// Settings for a noise reducer filter
type NoiseReducerFilterSettings struct {

	// Relative strength of noise reducing filter. Higher values produce stronger
	// filtering.
	Strength int32
}

// Noise reducer filter settings for spatial filter.
type NoiseReducerSpatialFilterSettings struct {

	// Specify strength of post noise reduction sharpening filter, with 0 disabling the
	// filter and 3 enabling it at maximum strength.
	PostFilterSharpenStrength int32

	// The speed of the filter, from -2 (lower speed) to 3 (higher speed), with 0 being
	// the nominal value.
	Speed int32

	// Relative strength of noise reducing filter. Higher values produce stronger
	// filtering.
	Strength int32
}

// Noise reducer filter settings for temporal filter.
type NoiseReducerTemporalFilterSettings struct {

	// Use Aggressive mode for content that has complex motion. Higher values produce
	// stronger temporal filtering. This filters highly complex scenes more
	// aggressively and creates better VQ for low bitrate outputs.
	AggressiveMode int32

	// Optional. When you set Noise reducer (noiseReducer) to Temporal (TEMPORAL), you
	// can use this setting to apply sharpening. The default behavior, Auto (AUTO),
	// allows the transcoder to determine whether to apply filtering, depending on
	// input type and quality. When you set Noise reducer to Temporal, your output
	// bandwidth is reduced. When Post temporal sharpening is also enabled, that
	// bandwidth reduction is smaller.
	PostTemporalSharpening NoiseFilterPostTemporalSharpening

	// The speed of the filter (higher number is faster). Low setting reduces bit rate
	// at the cost of transcode time, high setting improves transcode time at the cost
	// of bit rate.
	Speed int32

	// Specify the strength of the noise reducing filter on this output. Higher values
	// produce stronger filtering. We recommend the following value ranges, depending
	// on the result that you want: * 0-2 for complexity reduction with minimal
	// sharpness loss * 2-8 for complexity reduction with image preservation * 8-16 for
	// a high level of complexity reduction
	Strength int32
}

// Required when you set Codec, under AudioDescriptions>CodecSettings, to the value
// OPUS.
type OpusSettings struct {

	// Optional. Specify the average bitrate in bits per second. Valid values are
	// multiples of 8000, from 32000 through 192000. The default value is 96000, which
	// we recommend for quality and bandwidth.
	Bitrate int32

	// Specify the number of channels in this output audio track. Choosing Mono on the
	// console gives you 1 output channel; choosing Stereo gives you 2. In the API,
	// valid values are 1 and 2.
	Channels int32

	// Optional. Sample rate in hz. Valid values are 16000, 24000, and 48000. The
	// default value is 48000.
	SampleRate int32
}

// An output object describes the settings for a single output file or stream in an
// output group.
type Output struct {

	// (AudioDescriptions) contains groups of audio encoding settings organized by
	// audio codec. Include one instance of (AudioDescriptions) per output.
	// (AudioDescriptions) can contain multiple groups of encoding settings.
	AudioDescriptions []AudioDescription

	// (CaptionDescriptions) contains groups of captions settings. For each output that
	// has captions, include one instance of (CaptionDescriptions).
	// (CaptionDescriptions) can contain multiple groups of captions settings.
	CaptionDescriptions []CaptionDescription

	// Container specific settings.
	ContainerSettings *ContainerSettings

	// Use Extension (Extension) to specify the file extension for outputs in File
	// output groups. If you do not specify a value, the service will use default
	// extensions by container type as follows * MPEG-2 transport stream, m2ts *
	// Quicktime, mov * MXF container, mxf * MPEG-4 container, mp4 * WebM container,
	// webm * No Container, the service will use codec extensions (e.g. AAC, H265,
	// H265, AC3)
	Extension *string

	// Use Name modifier (NameModifier) to have the service add a string to the end of
	// each output filename. You specify the base filename as part of your destination
	// URI. When you create multiple outputs in the same output group, Name modifier
	// (NameModifier) is required. Name modifier also accepts format identifiers. For
	// DASH ISO outputs, if you use the format identifiers $Number$ or $Time$ in one
	// output, you must use them in the same way in all outputs of the output group.
	NameModifier *string

	// Specific settings for this type of output.
	OutputSettings *OutputSettings

	// Use Preset (Preset) to specifiy a preset for your transcoding settings. Provide
	// the system or custom preset name. You can specify either Preset (Preset) or
	// Container settings (ContainerSettings), but not both.
	Preset *string

	// (VideoDescription) contains a group of video encoding settings. The specific
	// video settings depend on the video codec that you choose when you specify a
	// value for Video codec (codec). Include one instance of (VideoDescription) per
	// output.
	VideoDescription *VideoDescription
}

// OutputChannel mapping settings.
type OutputChannelMapping struct {

	// List of input channels
	InputChannels []int32
}

// Details regarding output
type OutputDetail struct {

	// Duration in milliseconds
	DurationInMs int32

	// Contains details about the output's video stream
	VideoDetails *VideoDetail
}

// Group of outputs
type OutputGroup struct {

	// Use automated encoding to have MediaConvert choose your encoding settings for
	// you, based on characteristics of your input video.
	AutomatedEncodingSettings *AutomatedEncodingSettings

	// Use Custom Group Name (CustomName) to specify a name for the output group. This
	// value is displayed on the console and can make your job settings JSON more
	// human-readable. It does not affect your outputs. Use up to twelve characters
	// that are either letters, numbers, spaces, or underscores.
	CustomName *string

	// Name of the output group
	Name *string

	// Output Group settings, including type
	OutputGroupSettings *OutputGroupSettings

	// This object holds groups of encoding settings, one group of settings per output.
	Outputs []Output
}

// Contains details about the output groups specified in the job settings.
type OutputGroupDetail struct {

	// Details about the output
	OutputDetails []OutputDetail
}

// Output Group settings, including type
type OutputGroupSettings struct {

	// Required when you set (Type) under (OutputGroups)>(OutputGroupSettings) to
	// CMAF_GROUP_SETTINGS. Each output in a CMAF Output Group may only contain a
	// single video, audio, or caption output.
	CmafGroupSettings *CmafGroupSettings

	// Required when you set (Type) under (OutputGroups)>(OutputGroupSettings) to
	// DASH_ISO_GROUP_SETTINGS.
	DashIsoGroupSettings *DashIsoGroupSettings

	// Required when you set (Type) under (OutputGroups)>(OutputGroupSettings) to
	// FILE_GROUP_SETTINGS.
	FileGroupSettings *FileGroupSettings

	// Required when you set (Type) under (OutputGroups)>(OutputGroupSettings) to
	// HLS_GROUP_SETTINGS.
	HlsGroupSettings *HlsGroupSettings

	// Required when you set (Type) under (OutputGroups)>(OutputGroupSettings) to
	// MS_SMOOTH_GROUP_SETTINGS.
	MsSmoothGroupSettings *MsSmoothGroupSettings

	// Type of output group (File group, Apple HLS, DASH ISO, Microsoft Smooth
	// Streaming, CMAF)
	Type OutputGroupType
}

// Specific settings for this type of output.
type OutputSettings struct {

	// Settings for HLS output groups
	HlsSettings *HlsSettings
}

// If you work with a third party video watermarking partner, use the group of
// settings that correspond with your watermarking partner to include watermarks in
// your output.
type PartnerWatermarking struct {

	// For forensic video watermarking, MediaConvert supports Nagra NexGuard File
	// Marker watermarking. MediaConvert supports both PreRelease Content (NGPR/G2) and
	// OTT Streaming workflows.
	NexguardFileMarkerSettings *NexGuardFileMarkerSettings
}

// A preset is a collection of preconfigured media conversion settings that you
// want MediaConvert to apply to the output during the conversion process.
type Preset struct {

	// A name you create for each preset. Each name must be unique within your account.
	//
	// This member is required.
	Name *string

	// Settings for preset
	//
	// This member is required.
	Settings *PresetSettings

	// An identifier for this resource that is unique within all of AWS.
	Arn *string

	// An optional category you create to organize your presets.
	Category *string

	// The timestamp in epoch seconds for preset creation.
	CreatedAt *time.Time

	// An optional description you create for each preset.
	Description *string

	// The timestamp in epoch seconds when the preset was last updated.
	LastUpdated *time.Time

	// A preset can be of two types: system or custom. System or built-in preset can't
	// be modified or deleted by the user.
	Type Type
}

// Settings for preset
type PresetSettings struct {

	// (AudioDescriptions) contains groups of audio encoding settings organized by
	// audio codec. Include one instance of (AudioDescriptions) per output.
	// (AudioDescriptions) can contain multiple groups of encoding settings.
	AudioDescriptions []AudioDescription

	// Caption settings for this preset. There can be multiple caption settings in a
	// single output.
	CaptionDescriptions []CaptionDescriptionPreset

	// Container specific settings.
	ContainerSettings *ContainerSettings

	// (VideoDescription) contains a group of video encoding settings. The specific
	// video settings depend on the video codec that you choose when you specify a
	// value for Video codec (codec). Include one instance of (VideoDescription) per
	// output.
	VideoDescription *VideoDescription
}

// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
// value PRORES.
type ProresSettings struct {

	// Use Profile (ProResCodecProfile) to specifiy the type of Apple ProRes codec to
	// use for this output.
	CodecProfile ProresCodecProfile

	// If you are using the console, use the Framerate setting to specify the frame
	// rate for this output. If you want to keep the same frame rate as the input
	// video, choose Follow source. If you want to do frame rate conversion, choose a
	// frame rate from the dropdown list or choose Custom. The framerates shown in the
	// dropdown list are decimal approximations of fractions. If you choose Custom,
	// specify your frame rate as a fraction. If you are creating your transcoding job
	// specification as a JSON file without the console, use FramerateControl to
	// specify which value the service uses for the frame rate for this output. Choose
	// INITIALIZE_FROM_SOURCE if you want the service to use the frame rate from the
	// input. Choose SPECIFIED if you want the service to use the frame rate you
	// specify in the settings FramerateNumerator and FramerateDenominator.
	FramerateControl ProresFramerateControl

	// Choose the method that you want MediaConvert to use when increasing or
	// decreasing the frame rate. We recommend using drop duplicate (DUPLICATE_DROP)
	// for numerically simple conversions, such as 60 fps to 30 fps. For numerically
	// complex conversions, you can use interpolate (INTERPOLATE) to avoid stutter.
	// This results in a smooth picture, but might introduce undesirable video
	// artifacts. For complex frame rate conversions, especially if your source video
	// has already been converted from its original cadence, use FrameFormer
	// (FRAMEFORMER) to do motion-compensated interpolation. FrameFormer chooses the
	// best conversion method frame by frame. Note that using FrameFormer increases the
	// transcoding time and incurs a significant add-on cost.
	FramerateConversionAlgorithm ProresFramerateConversionAlgorithm

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateDenominator to specify the denominator of this fraction. In this
	// example, use 1001 for the value of FramerateDenominator. When you use the
	// console for transcode jobs that use frame rate conversion, provide the value as
	// a decimal number for Framerate. In this example, specify 23.976.
	FramerateDenominator int32

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateNumerator to specify the numerator of this fraction. In this example,
	// use 24000 for the value of FramerateNumerator. When you use the console for
	// transcode jobs that use frame rate conversion, provide the value as a decimal
	// number for Framerate. In this example, specify 23.976.
	FramerateNumerator int32

	// Choose the scan line type for the output. Keep the default value, Progressive
	// (PROGRESSIVE) to create a progressive output, regardless of the scan type of
	// your input. Use Top field first (TOP_FIELD) or Bottom field first (BOTTOM_FIELD)
	// to create an output that's interlaced with the same field polarity throughout.
	// Use Follow, default top (FOLLOW_TOP_FIELD) or Follow, default bottom
	// (FOLLOW_BOTTOM_FIELD) to produce outputs with the same field polarity as the
	// source. For jobs that have multiple inputs, the output field polarity might
	// change over the course of the output. Follow behavior depends on the input scan
	// type. If the source is interlaced, the output will be interlaced with the same
	// polarity as the source. If the source is progressive, the output will be
	// interlaced with top field bottom field first, depending on which of the Follow
	// options you choose.
	InterlaceMode ProresInterlaceMode

	// Optional. Specify how the service determines the pixel aspect ratio (PAR) for
	// this output. The default behavior, Follow source (INITIALIZE_FROM_SOURCE), uses
	// the PAR from your input video for your output. To specify a different PAR in the
	// console, choose any value other than Follow source. To specify a different PAR
	// by editing the JSON job specification, choose SPECIFIED. When you choose
	// SPECIFIED for this setting, you must also specify values for the parNumerator
	// and parDenominator settings.
	ParControl ProresParControl

	// Required when you set Pixel aspect ratio (parControl) to SPECIFIED. On the
	// console, this corresponds to any value other than Follow source. When you
	// specify an output pixel aspect ratio (PAR) that is different from your input
	// video PAR, provide your output PAR as a ratio. For example, for D1/DV NTSC
	// widescreen, you would specify the ratio 40:33. In this example, the value for
	// parDenominator is 33.
	ParDenominator int32

	// Required when you set Pixel aspect ratio (parControl) to SPECIFIED. On the
	// console, this corresponds to any value other than Follow source. When you
	// specify an output pixel aspect ratio (PAR) that is different from your input
	// video PAR, provide your output PAR as a ratio. For example, for D1/DV NTSC
	// widescreen, you would specify the ratio 40:33. In this example, the value for
	// parNumerator is 40.
	ParNumerator int32

	// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
	// second (fps). Enable slow PAL to create a 25 fps output. When you enable slow
	// PAL, MediaConvert relabels the video frames to 25 fps and resamples your audio
	// to keep it synchronized with the video. Note that enabling this setting will
	// slightly reduce the duration of your video. Required settings: You must also set
	// Framerate to 25. In your JSON job specification, set (framerateControl) to
	// (SPECIFIED), (framerateNumerator) to 25 and (framerateDenominator) to 1.
	SlowPal ProresSlowPal

	// When you do frame rate conversion from 23.976 frames per second (fps) to 29.97
	// fps, and your output scan type is interlaced, you can optionally enable hard
	// telecine (HARD) to create a smoother picture. When you keep the default value,
	// None (NONE), MediaConvert does a standard frame rate conversion to 29.97 without
	// doing anything with the field polarity to create a smoother picture.
	Telecine ProresTelecine
}

// You can use queues to manage the resources that are available to your AWS
// account for running multiple transcoding jobs at the same time. If you don't
// specify a queue, the service sends all jobs through the default queue. For more
// information, see
// https://docs.aws.amazon.com/mediaconvert/latest/ug/working-with-queues.html.
type Queue struct {

	// A name that you create for each queue. Each name must be unique within your
	// account.
	//
	// This member is required.
	Name *string

	// An identifier for this resource that is unique within all of AWS.
	Arn *string

	// The timestamp in epoch seconds for when you created the queue.
	CreatedAt *time.Time

	// An optional description that you create for each queue.
	Description *string

	// The timestamp in epoch seconds for when you most recently updated the queue.
	LastUpdated *time.Time

	// Specifies whether the pricing plan for the queue is on-demand or reserved. For
	// on-demand, you pay per minute, billed in increments of .01 minute. For reserved,
	// you pay for the transcoding capacity of the entire queue, regardless of how much
	// or how little you use it. Reserved pricing requires a 12-month commitment.
	PricingPlan PricingPlan

	// The estimated number of jobs with a PROGRESSING status.
	ProgressingJobsCount int32

	// Details about the pricing plan for your reserved queue. Required for reserved
	// queues and not applicable to on-demand queues.
	ReservationPlan *ReservationPlan

	// Queues can be ACTIVE or PAUSED. If you pause a queue, the service won't begin
	// processing jobs in that queue. Jobs that are running when you pause the queue
	// continue to run until they finish or result in an error.
	Status QueueStatus

	// The estimated number of jobs with a SUBMITTED status.
	SubmittedJobsCount int32

	// Specifies whether this on-demand queue is system or custom. System queues are
	// built in. You can't modify or delete system queues. You can create and modify
	// custom queues.
	Type Type
}

// Description of the source and destination queues between which the job has
// moved, along with the timestamp of the move
type QueueTransition struct {

	// The queue that the job was on after the transition.
	DestinationQueue *string

	// The queue that the job was on before the transition.
	SourceQueue *string

	// The time, in Unix epoch format, that the job moved from the source queue to the
	// destination queue.
	Timestamp *time.Time
}

// Use Rectangle to identify a specific area of the video frame.
type Rectangle struct {

	// Height of rectangle in pixels. Specify only even numbers.
	Height int32

	// Width of rectangle in pixels. Specify only even numbers.
	Width int32

	// The distance, in pixels, between the rectangle and the left edge of the video
	// frame. Specify only even numbers.
	X int32

	// The distance, in pixels, between the rectangle and the top edge of the video
	// frame. Specify only even numbers.
	Y int32
}

// Use Manual audio remixing (RemixSettings) to adjust audio levels for each audio
// channel in each output of your job. With audio remixing, you can output more or
// fewer audio channels than your input audio source provides.
type RemixSettings struct {

	// Channel mapping (ChannelMapping) contains the group of fields that hold the
	// remixing value for each channel. Units are in dB. Acceptable values are within
	// the range from -60 (mute) through 6. A setting of 0 passes the input channel
	// unchanged to the output channel (no attenuation or amplification).
	ChannelMapping *ChannelMapping

	// Specify the number of audio channels from your input that you want to use in
	// your output. With remixing, you might combine or split the data in these
	// channels, so the number of channels in your final output might be different.
	ChannelsIn int32

	// Specify the number of channels in this output after remixing. Valid values: 1,
	// 2, 4, 6, 8... 64. (1 and even numbers to 64.)
	ChannelsOut int32
}

// Details about the pricing plan for your reserved queue. Required for reserved
// queues and not applicable to on-demand queues.
type ReservationPlan struct {

	// The length of the term of your reserved queue pricing plan commitment.
	Commitment Commitment

	// The timestamp in epoch seconds for when the current pricing plan term for this
	// reserved queue expires.
	ExpiresAt *time.Time

	// The timestamp in epoch seconds for when you set up the current pricing plan for
	// this reserved queue.
	PurchasedAt *time.Time

	// Specifies whether the term of your reserved queue pricing plan is automatically
	// extended (AUTO_RENEW) or expires (EXPIRE) at the end of the term.
	RenewalType RenewalType

	// Specifies the number of reserved transcode slots (RTS) for this queue. The
	// number of RTS determines how many jobs the queue can process in parallel; each
	// RTS can process one job at a time. When you increase this number, you extend
	// your existing commitment with a new 12-month commitment for a larger number of
	// RTS. The new commitment begins when you purchase the additional capacity. You
	// can't decrease the number of RTS in your reserved queue.
	ReservedSlots int32

	// Specifies whether the pricing plan for your reserved queue is ACTIVE or EXPIRED.
	Status ReservationPlanStatus
}

// Details about the pricing plan for your reserved queue. Required for reserved
// queues and not applicable to on-demand queues.
type ReservationPlanSettings struct {

	// The length of the term of your reserved queue pricing plan commitment.
	//
	// This member is required.
	Commitment Commitment

	// Specifies whether the term of your reserved queue pricing plan is automatically
	// extended (AUTO_RENEW) or expires (EXPIRE) at the end of the term. When your term
	// is auto renewed, you extend your commitment by 12 months from the auto renew
	// date. You can cancel this commitment.
	//
	// This member is required.
	RenewalType RenewalType

	// Specifies the number of reserved transcode slots (RTS) for this queue. The
	// number of RTS determines how many jobs the queue can process in parallel; each
	// RTS can process one job at a time. You can't decrease the number of RTS in your
	// reserved queue. You can increase the number of RTS by extending your existing
	// commitment with a new 12-month commitment for the larger number. The new
	// commitment begins when you purchase the additional capacity. You can't cancel
	// your commitment or revert to your original commitment after you increase the
	// capacity.
	//
	// This member is required.
	ReservedSlots int32
}

// The Amazon Resource Name (ARN) and tags for an AWS Elemental MediaConvert
// resource.
type ResourceTags struct {

	// The Amazon Resource Name (ARN) of the resource.
	Arn *string

	// The tags for the resource.
	Tags map[string]string
}

// Optional. Have MediaConvert automatically apply Amazon S3 access control for the
// outputs in this output group. When you don't use this setting, S3 automatically
// applies the default access control list PRIVATE.
type S3DestinationAccessControl struct {

	// Choose an Amazon S3 canned ACL for MediaConvert to apply to this output.
	CannedAcl S3ObjectCannedAcl
}

// Settings associated with S3 destination
type S3DestinationSettings struct {

	// Optional. Have MediaConvert automatically apply Amazon S3 access control for the
	// outputs in this output group. When you don't use this setting, S3 automatically
	// applies the default access control list PRIVATE.
	AccessControl *S3DestinationAccessControl

	// Settings for how your job outputs are encrypted as they are uploaded to Amazon
	// S3.
	Encryption *S3EncryptionSettings
}

// Settings for how your job outputs are encrypted as they are uploaded to Amazon
// S3.
type S3EncryptionSettings struct {

	// Specify how you want your data keys managed. AWS uses data keys to encrypt your
	// content. AWS also encrypts the data keys themselves, using a customer master key
	// (CMK), and then stores the encrypted data keys alongside your encrypted content.
	// Use this setting to specify which AWS service manages the CMK. For simplest set
	// up, choose Amazon S3 (SERVER_SIDE_ENCRYPTION_S3). If you want your master key to
	// be managed by AWS Key Management Service (KMS), choose AWS KMS
	// (SERVER_SIDE_ENCRYPTION_KMS). By default, when you choose AWS KMS, KMS uses the
	// AWS managed customer master key (CMK) associated with Amazon S3 to encrypt your
	// data keys. You can optionally choose to specify a different, customer managed
	// CMK. Do so by specifying the Amazon Resource Name (ARN) of the key for the
	// setting KMS ARN (kmsKeyArn).
	EncryptionType S3ServerSideEncryptionType

	// Optionally, specify the customer master key (CMK) that you want to use to
	// encrypt the data key that AWS uses to encrypt your output content. Enter the
	// Amazon Resource Name (ARN) of the CMK. To use this setting, you must also set
	// Server-side encryption (S3ServerSideEncryptionType) to AWS KMS
	// (SERVER_SIDE_ENCRYPTION_KMS). If you set Server-side encryption to AWS KMS but
	// don't specify a CMK here, AWS uses the AWS managed CMK associated with Amazon
	// S3.
	KmsKeyArn *string
}

// Settings for SCC caption output.
type SccDestinationSettings struct {

	// Set Framerate (SccDestinationFramerate) to make sure that the captions and the
	// video are synchronized in the output. Specify a frame rate that matches the
	// frame rate of the associated video. If the video frame rate is 29.97, choose
	// 29.97 dropframe (FRAMERATE_29_97_DROPFRAME) only if the video has
	// video_insertion=true and drop_frame_timecode=true; otherwise, choose 29.97
	// non-dropframe (FRAMERATE_29_97_NON_DROPFRAME).
	Framerate SccDestinationFramerate
}

// If your output group type is HLS, DASH, or Microsoft Smooth, use these settings
// when doing DRM encryption with a SPEKE-compliant key provider. If your output
// group type is CMAF, use the SpekeKeyProviderCmaf settings instead.
type SpekeKeyProvider struct {

	// If you want your key provider to encrypt the content keys that it provides to
	// MediaConvert, set up a certificate with a master key using AWS Certificate
	// Manager. Specify the certificate's Amazon Resource Name (ARN) here.
	CertificateArn *string

	// Specify the resource ID that your SPEKE-compliant key provider uses to identify
	// this content.
	ResourceId *string

	// Relates to SPEKE implementation. DRM system identifiers. DASH output groups
	// support a max of two system ids. Other group types support one system id. See
	// https://dashif.org/identifiers/content_protection/ for more details.
	SystemIds []string

	// Specify the URL to the key server that your SPEKE-compliant DRM key provider
	// uses to provide keys for encrypting your content.
	Url *string
}

// If your output group type is CMAF, use these settings when doing DRM encryption
// with a SPEKE-compliant key provider. If your output group type is HLS, DASH, or
// Microsoft Smooth, use the SpekeKeyProvider settings instead.
type SpekeKeyProviderCmaf struct {

	// If you want your key provider to encrypt the content keys that it provides to
	// MediaConvert, set up a certificate with a master key using AWS Certificate
	// Manager. Specify the certificate's Amazon Resource Name (ARN) here.
	CertificateArn *string

	// Specify the DRM system IDs that you want signaled in the DASH manifest that
	// MediaConvert creates as part of this CMAF package. The DASH manifest can
	// currently signal up to three system IDs. For more information, see
	// https://dashif.org/identifiers/content_protection/.
	DashSignaledSystemIds []string

	// Specify the DRM system ID that you want signaled in the HLS manifest that
	// MediaConvert creates as part of this CMAF package. The HLS manifest can
	// currently signal only one system ID. For more information, see
	// https://dashif.org/identifiers/content_protection/.
	HlsSignaledSystemIds []string

	// Specify the resource ID that your SPEKE-compliant key provider uses to identify
	// this content.
	ResourceId *string

	// Specify the URL to the key server that your SPEKE-compliant DRM key provider
	// uses to provide keys for encrypting your content.
	Url *string
}

// Use these settings to set up encryption with a static key provider.
type StaticKeyProvider struct {

	// Relates to DRM implementation. Sets the value of the KEYFORMAT attribute. Must
	// be 'identity' or a reverse DNS string. May be omitted to indicate an implicit
	// value of 'identity'.
	KeyFormat *string

	// Relates to DRM implementation. Either a single positive integer version value or
	// a slash delimited list of version values (1/2/3).
	KeyFormatVersions *string

	// Relates to DRM implementation. Use a 32-character hexidecimal string to specify
	// Key Value (StaticKeyValue).
	StaticKeyValue *string

	// Relates to DRM implementation. The location of the license server used for
	// protecting content.
	Url *string
}

// Settings for Teletext caption output
type TeletextDestinationSettings struct {

	// Set pageNumber to the Teletext page number for the destination captions for this
	// output. This value must be a three-digit hexadecimal string; strings ending in
	// -FF are invalid. If you are passing through the entire set of Teletext data, do
	// not use this field.
	PageNumber *string

	// Specify the page types for this Teletext page. If you don't specify a value
	// here, the service sets the page type to the default value Subtitle
	// (PAGE_TYPE_SUBTITLE). If you pass through the entire set of Teletext data, don't
	// use this field. When you pass through a set of Teletext pages, your output has
	// the same page types as your input.
	PageTypes []TeletextPageType
}

// Settings specific to Teletext caption sources, including Page number.
type TeletextSourceSettings struct {

	// Use Page Number (PageNumber) to specify the three-digit hexadecimal page number
	// that will be used for Teletext captions. Do not use this setting if you are
	// passing through teletext from the input source to output.
	PageNumber *string
}

// Timecode burn-in (TimecodeBurnIn)--Burns the output timecode and specified
// prefix into the output.
type TimecodeBurnin struct {

	// Use Font Size (FontSize) to set the font size of any burned-in timecode. Valid
	// values are 10, 16, 32, 48.
	FontSize int32

	// Use Position (Position) under under Timecode burn-in (TimecodeBurnIn) to specify
	// the location the burned-in timecode on output video.
	Position TimecodeBurninPosition

	// Use Prefix (Prefix) to place ASCII characters before any burned-in timecode. For
	// example, a prefix of "EZ-" will result in the timecode "EZ-00:00:00:00". Provide
	// either the characters themselves or the ASCII code equivalents. The supported
	// range of characters is 0x20 through 0x7e. This includes letters, numbers, and
	// all special characters represented on a standard English keyboard.
	Prefix *string
}

// These settings control how the service handles timecodes throughout the job.
// These settings don't affect input clipping.
type TimecodeConfig struct {

	// If you use an editing platform that relies on an anchor timecode, use Anchor
	// Timecode (Anchor) to specify a timecode that will match the input video frame to
	// the output video frame. Use 24-hour format with frame number, (HH:MM:SS:FF) or
	// (HH:MM:SS;FF). This setting ignores frame rate conversion. System behavior for
	// Anchor Timecode varies depending on your setting for Source (TimecodeSource). *
	// If Source (TimecodeSource) is set to Specified Start (SPECIFIEDSTART), the first
	// input frame is the specified value in Start Timecode (Start). Anchor Timecode
	// (Anchor) and Start Timecode (Start) are used calculate output timecode. * If
	// Source (TimecodeSource) is set to Start at 0 (ZEROBASED) the first frame is
	// 00:00:00:00. * If Source (TimecodeSource) is set to Embedded (EMBEDDED), the
	// first frame is the timecode value on the first input frame of the input.
	Anchor *string

	// Use Source (TimecodeSource) to set how timecodes are handled within this job. To
	// make sure that your video, audio, captions, and markers are synchronized and
	// that time-based features, such as image inserter, work correctly, choose the
	// Timecode source option that matches your assets. All timecodes are in a 24-hour
	// format with frame number (HH:MM:SS:FF). * Embedded (EMBEDDED) - Use the timecode
	// that is in the input video. If no embedded timecode is in the source, the
	// service will use Start at 0 (ZEROBASED) instead. * Start at 0 (ZEROBASED) - Set
	// the timecode of the initial frame to 00:00:00:00. * Specified Start
	// (SPECIFIEDSTART) - Set the timecode of the initial frame to a value other than
	// zero. You use Start timecode (Start) to provide this value.
	Source TimecodeSource

	// Only use when you set Source (TimecodeSource) to Specified start
	// (SPECIFIEDSTART). Use Start timecode (Start) to specify the timecode for the
	// initial frame. Use 24-hour format with frame number, (HH:MM:SS:FF) or
	// (HH:MM:SS;FF).
	Start *string

	// Only applies to outputs that support program-date-time stamp. Use Timestamp
	// offset (TimestampOffset) to overwrite the timecode date without affecting the
	// time and frame number. Provide the new date as a string in the format
	// "yyyy-mm-dd". To use Time stamp offset, you must also enable Insert
	// program-date-time (InsertProgramDateTime) in the output settings. For example,
	// if the date part of your timecodes is 2002-1-25 and you want to change it to one
	// year later, set Timestamp offset (TimestampOffset) to 2003-1-25.
	TimestampOffset *string
}

// Enable Timed metadata insertion (TimedMetadataInsertion) to include ID3 tags in
// any HLS outputs. To include timed metadata, you must enable it here, enable it
// in each output container, and specify tags and timecodes in ID3 insertion
// (Id3Insertion) objects.
type TimedMetadataInsertion struct {

	// Id3Insertions contains the array of Id3Insertion instances.
	Id3Insertions []Id3Insertion
}

// Information about when jobs are submitted, started, and finished is specified in
// Unix epoch format in seconds.
type Timing struct {

	// The time, in Unix epoch format, that the transcoding job finished
	FinishTime *time.Time

	// The time, in Unix epoch format, that transcoding for the job began.
	StartTime *time.Time

	// The time, in Unix epoch format, that you submitted the job.
	SubmitTime *time.Time
}

// Settings specific to caption sources that are specified by track number.
// Currently, this is only IMSC captions in an IMF package. If your caption source
// is IMSC 1.1 in a separate xml file, use FileSourceSettings instead of
// TrackSourceSettings.
type TrackSourceSettings struct {

	// Use this setting to select a single captions track from a source. Track numbers
	// correspond to the order in the captions source file. For IMF sources, track
	// numbering is based on the order that the captions appear in the CPL. For
	// example, use 1 to select the captions asset that is listed first in the CPL. To
	// include more than one captions track in your job outputs, create multiple input
	// captions selectors. Specify one track per selector.
	TrackNumber int32
}

// Settings specific to TTML caption outputs, including Pass style information
// (TtmlStylePassthrough).
type TtmlDestinationSettings struct {

	// Pass through style and position information from a TTML-like input source (TTML,
	// SMPTE-TT) to the TTML output.
	StylePassthrough TtmlStylePassthrough
}

// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
// value VC3
type Vc3Settings struct {

	// If you are using the console, use the Framerate setting to specify the frame
	// rate for this output. If you want to keep the same frame rate as the input
	// video, choose Follow source. If you want to do frame rate conversion, choose a
	// frame rate from the dropdown list or choose Custom. The framerates shown in the
	// dropdown list are decimal approximations of fractions. If you choose Custom,
	// specify your frame rate as a fraction. If you are creating your transcoding job
	// specification as a JSON file without the console, use FramerateControl to
	// specify which value the service uses for the frame rate for this output. Choose
	// INITIALIZE_FROM_SOURCE if you want the service to use the frame rate from the
	// input. Choose SPECIFIED if you want the service to use the frame rate you
	// specify in the settings FramerateNumerator and FramerateDenominator.
	FramerateControl Vc3FramerateControl

	// Choose the method that you want MediaConvert to use when increasing or
	// decreasing the frame rate. We recommend using drop duplicate (DUPLICATE_DROP)
	// for numerically simple conversions, such as 60 fps to 30 fps. For numerically
	// complex conversions, you can use interpolate (INTERPOLATE) to avoid stutter.
	// This results in a smooth picture, but might introduce undesirable video
	// artifacts. For complex frame rate conversions, especially if your source video
	// has already been converted from its original cadence, use FrameFormer
	// (FRAMEFORMER) to do motion-compensated interpolation. FrameFormer chooses the
	// best conversion method frame by frame. Note that using FrameFormer increases the
	// transcoding time and incurs a significant add-on cost.
	FramerateConversionAlgorithm Vc3FramerateConversionAlgorithm

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateDenominator to specify the denominator of this fraction. In this
	// example, use 1001 for the value of FramerateDenominator. When you use the
	// console for transcode jobs that use frame rate conversion, provide the value as
	// a decimal number for Framerate. In this example, specify 23.976.
	FramerateDenominator int32

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateNumerator to specify the numerator of this fraction. In this example,
	// use 24000 for the value of FramerateNumerator. When you use the console for
	// transcode jobs that use frame rate conversion, provide the value as a decimal
	// number for Framerate. In this example, specify 23.976.
	FramerateNumerator int32

	// Optional. Choose the scan line type for this output. If you don't specify a
	// value, MediaConvert will create a progressive output.
	InterlaceMode Vc3InterlaceMode

	// Ignore this setting unless your input frame rate is 23.976 or 24 frames per
	// second (fps). Enable slow PAL to create a 25 fps output by relabeling the video
	// frames and resampling your audio. Note that enabling this setting will slightly
	// reduce the duration of your video. Related settings: You must also set Framerate
	// to 25. In your JSON job specification, set (framerateControl) to (SPECIFIED),
	// (framerateNumerator) to 25 and (framerateDenominator) to 1.
	SlowPal Vc3SlowPal

	// When you do frame rate conversion from 23.976 frames per second (fps) to 29.97
	// fps, and your output scan type is interlaced, you can optionally enable hard
	// telecine (HARD) to create a smoother picture. When you keep the default value,
	// None (NONE), MediaConvert does a standard frame rate conversion to 29.97 without
	// doing anything with the field polarity to create a smoother picture.
	Telecine Vc3Telecine

	// Specify the VC3 class to choose the quality characteristics for this output. VC3
	// class, together with the settings Framerate (framerateNumerator and
	// framerateDenominator) and Resolution (height and width), determine your output
	// bitrate. For example, say that your video resolution is 1920x1080 and your
	// framerate is 29.97. Then Class 145 (CLASS_145) gives you an output with a
	// bitrate of approximately 145 Mbps and Class 220 (CLASS_220) gives you and output
	// with a bitrate of approximately 220 Mbps. VC3 class also specifies the color bit
	// depth of your output.
	Vc3Class Vc3Class
}

// Video codec settings, (CodecSettings) under (VideoDescription), contains the
// group of settings related to video encoding. The settings in this group vary
// depending on the value that you choose for Video codec (Codec). For each codec
// enum that you choose, define the corresponding settings object. The following
// lists the codec enum, settings object pairs. * AV1, Av1Settings * AVC_INTRA,
// AvcIntraSettings * FRAME_CAPTURE, FrameCaptureSettings * H_264, H264Settings *
// H_265, H265Settings * MPEG2, Mpeg2Settings * PRORES, ProresSettings * VC3,
// Vc3Settings * VP8, Vp8Settings * VP9, Vp9Settings
type VideoCodecSettings struct {

	// Required when you set Codec, under VideoDescription>CodecSettings to the value
	// AV1.
	Av1Settings *Av1Settings

	// Required when you set your output video codec to AVC-Intra. For more information
	// about the AVC-I settings, see the relevant specification. For detailed
	// information about SD and HD in AVC-I, see
	// https://ieeexplore.ieee.org/document/7290936.
	AvcIntraSettings *AvcIntraSettings

	// Specifies the video codec. This must be equal to one of the enum values defined
	// by the object VideoCodec.
	Codec VideoCodec

	// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
	// value FRAME_CAPTURE.
	FrameCaptureSettings *FrameCaptureSettings

	// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
	// value H_264.
	H264Settings *H264Settings

	// Settings for H265 codec
	H265Settings *H265Settings

	// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
	// value MPEG2.
	Mpeg2Settings *Mpeg2Settings

	// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
	// value PRORES.
	ProresSettings *ProresSettings

	// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
	// value VC3
	Vc3Settings *Vc3Settings

	// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
	// value VP8.
	Vp8Settings *Vp8Settings

	// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
	// value VP9.
	Vp9Settings *Vp9Settings
}

// Settings for video outputs
type VideoDescription struct {

	// This setting only applies to H.264, H.265, and MPEG2 outputs. Use Insert AFD
	// signaling (AfdSignaling) to specify whether the service includes AFD values in
	// the output video data and what those values are. * Choose None to remove all AFD
	// values from this output. * Choose Fixed to ignore input AFD values and instead
	// encode the value specified in the job. * Choose Auto to calculate output AFD
	// values based on the input AFD scaler data.
	AfdSignaling AfdSignaling

	// The anti-alias filter is automatically applied to all outputs. The service no
	// longer accepts the value DISABLED for AntiAlias. If you specify that in your
	// job, the service will ignore the setting.
	AntiAlias AntiAlias

	// Video codec settings, (CodecSettings) under (VideoDescription), contains the
	// group of settings related to video encoding. The settings in this group vary
	// depending on the value that you choose for Video codec (Codec). For each codec
	// enum that you choose, define the corresponding settings object. The following
	// lists the codec enum, settings object pairs. * AV1, Av1Settings * AVC_INTRA,
	// AvcIntraSettings * FRAME_CAPTURE, FrameCaptureSettings * H_264, H264Settings *
	// H_265, H265Settings * MPEG2, Mpeg2Settings * PRORES, ProresSettings * VC3,
	// Vc3Settings * VP8, Vp8Settings * VP9, Vp9Settings
	CodecSettings *VideoCodecSettings

	// Choose Insert (INSERT) for this setting to include color metadata in this
	// output. Choose Ignore (IGNORE) to exclude color metadata from this output. If
	// you don't specify a value, the service sets this to Insert by default.
	ColorMetadata ColorMetadata

	// Use Cropping selection (crop) to specify the video area that the service will
	// include in the output video frame.
	Crop *Rectangle

	// Applies only to 29.97 fps outputs. When this feature is enabled, the service
	// will use drop-frame timecode on outputs. If it is not possible to use drop-frame
	// timecode, the system will fall back to non-drop-frame. This setting is enabled
	// by default when Timecode insertion (TimecodeInsertion) is enabled.
	DropFrameTimecode DropFrameTimecode

	// Applies only if you set AFD Signaling(AfdSignaling) to Fixed (FIXED). Use Fixed
	// (FixedAfd) to specify a four-bit AFD value which the service will write on all
	// frames of this video output.
	FixedAfd int32

	// Use the Height (Height) setting to define the video resolution height for this
	// output. Specify in pixels. If you don't provide a value here, the service will
	// use the input height.
	Height int32

	// Use Selection placement (position) to define the video area in your output
	// frame. The area outside of the rectangle that you specify here is black.
	Position *Rectangle

	// Use Respond to AFD (RespondToAfd) to specify how the service changes the video
	// itself in response to AFD values in the input. * Choose Respond to clip the
	// input video frame according to the AFD value, input display aspect ratio, and
	// output display aspect ratio. * Choose Passthrough to include the input AFD
	// values. Do not choose this when AfdSignaling is set to (NONE). A preferred
	// implementation of this workflow is to set RespondToAfd to (NONE) and set
	// AfdSignaling to (AUTO). * Choose None to remove all input AFD values from this
	// output.
	RespondToAfd RespondToAfd

	// Specify how the service handles outputs that have a different aspect ratio from
	// the input aspect ratio. Choose Stretch to output (STRETCH_TO_OUTPUT) to have the
	// service stretch your video image to fit. Keep the setting Default (DEFAULT) to
	// have the service letterbox your video instead. This setting overrides any value
	// that you specify for the setting Selection placement (position) in this output.
	ScalingBehavior ScalingBehavior

	// Use Sharpness (Sharpness) setting to specify the strength of anti-aliasing. This
	// setting changes the width of the anti-alias filter kernel used for scaling.
	// Sharpness only applies if your output resolution is different from your input
	// resolution. 0 is the softest setting, 100 the sharpest, and 50 recommended for
	// most content.
	Sharpness int32

	// Applies only to H.264, H.265, MPEG2, and ProRes outputs. Only enable Timecode
	// insertion when the input frame rate is identical to the output frame rate. To
	// include timecodes in this output, set Timecode insertion
	// (VideoTimecodeInsertion) to PIC_TIMING_SEI. To leave them out, set it to
	// DISABLED. Default is DISABLED. When the service inserts timecodes in an output,
	// by default, it uses any embedded timecodes from the input. If none are present,
	// the service will set the timecode for the first output frame to zero. To change
	// this default behavior, adjust the settings under Timecode configuration
	// (TimecodeConfig). In the console, these settings are located under Job > Job
	// settings > Timecode configuration. Note - Timecode source under input settings
	// (InputTimecodeSource) does not affect the timecodes that are inserted in the
	// output. Source under Job settings > Timecode configuration (TimecodeSource)
	// does.
	TimecodeInsertion VideoTimecodeInsertion

	// Find additional transcoding features under Preprocessors (VideoPreprocessors).
	// Enable the features at each output individually. These features are disabled by
	// default.
	VideoPreprocessors *VideoPreprocessor

	// Use Width (Width) to define the video resolution width, in pixels, for this
	// output. If you don't provide a value here, the service will use the input width.
	Width int32
}

// Contains details about the output's video stream
type VideoDetail struct {

	// Height in pixels for the output
	HeightInPx int32

	// Width in pixels for the output
	WidthInPx int32
}

// Find additional transcoding features under Preprocessors (VideoPreprocessors).
// Enable the features at each output individually. These features are disabled by
// default.
type VideoPreprocessor struct {

	// Enable the Color corrector (ColorCorrector) feature if necessary. Enable or
	// disable this feature for each output individually. This setting is disabled by
	// default.
	ColorCorrector *ColorCorrector

	// Use Deinterlacer (Deinterlacer) to produce smoother motion and a clearer
	// picture.
	Deinterlacer *Deinterlacer

	// Enable Dolby Vision feature to produce Dolby Vision compatible video output.
	DolbyVision *DolbyVision

	// Enable the Image inserter (ImageInserter) feature to include a graphic overlay
	// on your video. Enable or disable this feature for each output individually. This
	// setting is disabled by default.
	ImageInserter *ImageInserter

	// Enable the Noise reducer (NoiseReducer) feature to remove noise from your video
	// output if necessary. Enable or disable this feature for each output
	// individually. This setting is disabled by default.
	NoiseReducer *NoiseReducer

	// If you work with a third party video watermarking partner, use the group of
	// settings that correspond with your watermarking partner to include watermarks in
	// your output.
	PartnerWatermarking *PartnerWatermarking

	// Timecode burn-in (TimecodeBurnIn)--Burns the output timecode and specified
	// prefix into the output.
	TimecodeBurnin *TimecodeBurnin
}

// Selector for video.
type VideoSelector struct {

	// Ignore this setting unless this input is a QuickTime animation with an alpha
	// channel. Use this setting to create separate Key and Fill outputs. In each
	// output, specify which part of the input MediaConvert uses. Leave this setting at
	// the default value DISCARD to delete the alpha channel and preserve the video.
	// Set it to REMAP_TO_LUMA to delete the video and map the alpha channel to the
	// luma channel of your outputs.
	AlphaBehavior AlphaBehavior

	// If your input video has accurate color space metadata, or if you don't know
	// about color space, leave this set to the default value Follow (FOLLOW). The
	// service will automatically detect your input color space. If your input video
	// has metadata indicating the wrong color space, specify the accurate color space
	// here. If your input video is HDR 10 and the SMPTE ST 2086 Mastering Display
	// Color Volume static metadata isn't present in your video stream, or if that
	// metadata is present but not accurate, choose Force HDR 10 (FORCE_HDR10) here and
	// specify correct values in the input HDR 10 metadata (Hdr10Metadata) settings.
	// For more information about MediaConvert HDR jobs, see
	// https://docs.aws.amazon.com/console/mediaconvert/hdr.
	ColorSpace ColorSpace

	// There are two sources for color metadata, the input file and the job input
	// settings Color space (ColorSpace) and HDR master display information
	// settings(Hdr10Metadata). The Color space usage setting determines which takes
	// precedence. Choose Force (FORCE) to use color metadata from the input job
	// settings. If you don't specify values for those settings, the service defaults
	// to using metadata from your input. FALLBACK - Choose Fallback (FALLBACK) to use
	// color metadata from the source when it is present. If there's no color metadata
	// in your input file, the service defaults to using values you specify in the
	// input settings.
	ColorSpaceUsage ColorSpaceUsage

	// Use these settings to provide HDR 10 metadata that is missing or inaccurate in
	// your input video. Appropriate values vary depending on the input video and must
	// be provided by a color grader. The color grader generates these values during
	// the HDR 10 mastering process. The valid range for each of these settings is 0 to
	// 50,000. Each increment represents 0.00002 in CIE1931 color coordinate. Related
	// settings - When you specify these values, you must also set Color space
	// (ColorSpace) to HDR 10 (HDR10). To specify whether the the values you specify
	// here take precedence over the values in the metadata of your input file, set
	// Color space usage (ColorSpaceUsage). To specify whether color metadata is
	// included in an output, set Color metadata (ColorMetadata). For more information
	// about MediaConvert HDR jobs, see
	// https://docs.aws.amazon.com/console/mediaconvert/hdr.
	Hdr10Metadata *Hdr10Metadata

	// Use PID (Pid) to select specific video data from an input file. Specify this
	// value as an integer; the system automatically converts it to the hexidecimal
	// value. For example, 257 selects PID 0x101. A PID, or packet identifier, is an
	// identifier for a set of data in an MPEG-2 transport stream container.
	Pid int32

	// Selects a specific program from within a multi-program transport stream. Note
	// that Quad 4K is not currently supported.
	ProgramNumber int32

	// Use Rotate (InputRotate) to specify how the service rotates your video. You can
	// choose automatic rotation or specify a rotation. You can specify a clockwise
	// rotation of 0, 90, 180, or 270 degrees. If your input video container is .mov or
	// .mp4 and your input has rotation metadata, you can choose Automatic to have the
	// service rotate your video according to the rotation specified in the metadata.
	// The rotation must be within one degree of 90, 180, or 270 degrees. If the
	// rotation metadata specifies any other rotation, the service will default to no
	// rotation. By default, the service does no rotation, even if your input video has
	// rotation metadata. The service doesn't pass through rotation metadata.
	Rotate InputRotate
}

// Required when you set Codec, under AudioDescriptions>CodecSettings, to the value
// Vorbis.
type VorbisSettings struct {

	// Optional. Specify the number of channels in this output audio track. Choosing
	// Mono on the console gives you 1 output channel; choosing Stereo gives you 2. In
	// the API, valid values are 1 and 2. The default value is 2.
	Channels int32

	// Optional. Specify the audio sample rate in Hz. Valid values are 22050, 32000,
	// 44100, and 48000. The default value is 48000.
	SampleRate int32

	// Optional. Specify the variable audio quality of this Vorbis output from -1
	// (lowest quality, ~45 kbit/s) to 10 (highest quality, ~500 kbit/s). The default
	// value is 4 (~128 kbit/s). Values 5 and 6 are approximately 160 and 192 kbit/s,
	// respectively.
	VbrQuality int32
}

// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
// value VP8.
type Vp8Settings struct {

	// Target bitrate in bits/second. For example, enter five megabits per second as
	// 5000000.
	Bitrate int32

	// If you are using the console, use the Framerate setting to specify the frame
	// rate for this output. If you want to keep the same frame rate as the input
	// video, choose Follow source. If you want to do frame rate conversion, choose a
	// frame rate from the dropdown list or choose Custom. The framerates shown in the
	// dropdown list are decimal approximations of fractions. If you choose Custom,
	// specify your frame rate as a fraction. If you are creating your transcoding job
	// specification as a JSON file without the console, use FramerateControl to
	// specify which value the service uses for the frame rate for this output. Choose
	// INITIALIZE_FROM_SOURCE if you want the service to use the frame rate from the
	// input. Choose SPECIFIED if you want the service to use the frame rate you
	// specify in the settings FramerateNumerator and FramerateDenominator.
	FramerateControl Vp8FramerateControl

	// Choose the method that you want MediaConvert to use when increasing or
	// decreasing the frame rate. We recommend using drop duplicate (DUPLICATE_DROP)
	// for numerically simple conversions, such as 60 fps to 30 fps. For numerically
	// complex conversions, you can use interpolate (INTERPOLATE) to avoid stutter.
	// This results in a smooth picture, but might introduce undesirable video
	// artifacts. For complex frame rate conversions, especially if your source video
	// has already been converted from its original cadence, use FrameFormer
	// (FRAMEFORMER) to do motion-compensated interpolation. FrameFormer chooses the
	// best conversion method frame by frame. Note that using FrameFormer increases the
	// transcoding time and incurs a significant add-on cost.
	FramerateConversionAlgorithm Vp8FramerateConversionAlgorithm

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateDenominator to specify the denominator of this fraction. In this
	// example, use 1001 for the value of FramerateDenominator. When you use the
	// console for transcode jobs that use frame rate conversion, provide the value as
	// a decimal number for Framerate. In this example, specify 23.976.
	FramerateDenominator int32

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateNumerator to specify the numerator of this fraction. In this example,
	// use 24000 for the value of FramerateNumerator. When you use the console for
	// transcode jobs that use frame rate conversion, provide the value as a decimal
	// number for Framerate. In this example, specify 23.976.
	FramerateNumerator int32

	// GOP Length (keyframe interval) in frames. Must be greater than zero.
	GopSize float64

	// Optional. Size of buffer (HRD buffer model) in bits. For example, enter five
	// megabits as 5000000.
	HrdBufferSize int32

	// Ignore this setting unless you set qualityTuningLevel to MULTI_PASS. Optional.
	// Specify the maximum bitrate in bits/second. For example, enter five megabits per
	// second as 5000000. The default behavior uses twice the target bitrate as the
	// maximum bitrate.
	MaxBitrate int32

	// Optional. Specify how the service determines the pixel aspect ratio (PAR) for
	// this output. The default behavior, Follow source (INITIALIZE_FROM_SOURCE), uses
	// the PAR from your input video for your output. To specify a different PAR in the
	// console, choose any value other than Follow source. To specify a different PAR
	// by editing the JSON job specification, choose SPECIFIED. When you choose
	// SPECIFIED for this setting, you must also specify values for the parNumerator
	// and parDenominator settings.
	ParControl Vp8ParControl

	// Required when you set Pixel aspect ratio (parControl) to SPECIFIED. On the
	// console, this corresponds to any value other than Follow source. When you
	// specify an output pixel aspect ratio (PAR) that is different from your input
	// video PAR, provide your output PAR as a ratio. For example, for D1/DV NTSC
	// widescreen, you would specify the ratio 40:33. In this example, the value for
	// parDenominator is 33.
	ParDenominator int32

	// Required when you set Pixel aspect ratio (parControl) to SPECIFIED. On the
	// console, this corresponds to any value other than Follow source. When you
	// specify an output pixel aspect ratio (PAR) that is different from your input
	// video PAR, provide your output PAR as a ratio. For example, for D1/DV NTSC
	// widescreen, you would specify the ratio 40:33. In this example, the value for
	// parNumerator is 40.
	ParNumerator int32

	// Optional. Use Quality tuning level (qualityTuningLevel) to choose how you want
	// to trade off encoding speed for output video quality. The default behavior is
	// faster, lower quality, multi-pass encoding.
	QualityTuningLevel Vp8QualityTuningLevel

	// With the VP8 codec, you can use only the variable bitrate (VBR) rate control
	// mode.
	RateControlMode Vp8RateControlMode
}

// Required when you set (Codec) under (VideoDescription)>(CodecSettings) to the
// value VP9.
type Vp9Settings struct {

	// Target bitrate in bits/second. For example, enter five megabits per second as
	// 5000000.
	Bitrate int32

	// If you are using the console, use the Framerate setting to specify the frame
	// rate for this output. If you want to keep the same frame rate as the input
	// video, choose Follow source. If you want to do frame rate conversion, choose a
	// frame rate from the dropdown list or choose Custom. The framerates shown in the
	// dropdown list are decimal approximations of fractions. If you choose Custom,
	// specify your frame rate as a fraction. If you are creating your transcoding job
	// specification as a JSON file without the console, use FramerateControl to
	// specify which value the service uses for the frame rate for this output. Choose
	// INITIALIZE_FROM_SOURCE if you want the service to use the frame rate from the
	// input. Choose SPECIFIED if you want the service to use the frame rate you
	// specify in the settings FramerateNumerator and FramerateDenominator.
	FramerateControl Vp9FramerateControl

	// Choose the method that you want MediaConvert to use when increasing or
	// decreasing the frame rate. We recommend using drop duplicate (DUPLICATE_DROP)
	// for numerically simple conversions, such as 60 fps to 30 fps. For numerically
	// complex conversions, you can use interpolate (INTERPOLATE) to avoid stutter.
	// This results in a smooth picture, but might introduce undesirable video
	// artifacts. For complex frame rate conversions, especially if your source video
	// has already been converted from its original cadence, use FrameFormer
	// (FRAMEFORMER) to do motion-compensated interpolation. FrameFormer chooses the
	// best conversion method frame by frame. Note that using FrameFormer increases the
	// transcoding time and incurs a significant add-on cost.
	FramerateConversionAlgorithm Vp9FramerateConversionAlgorithm

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateDenominator to specify the denominator of this fraction. In this
	// example, use 1001 for the value of FramerateDenominator. When you use the
	// console for transcode jobs that use frame rate conversion, provide the value as
	// a decimal number for Framerate. In this example, specify 23.976.
	FramerateDenominator int32

	// When you use the API for transcode jobs that use frame rate conversion, specify
	// the frame rate as a fraction. For example, 24000 / 1001 = 23.976 fps. Use
	// FramerateNumerator to specify the numerator of this fraction. In this example,
	// use 24000 for the value of FramerateNumerator. When you use the console for
	// transcode jobs that use frame rate conversion, provide the value as a decimal
	// number for Framerate. In this example, specify 23.976.
	FramerateNumerator int32

	// GOP Length (keyframe interval) in frames. Must be greater than zero.
	GopSize float64

	// Size of buffer (HRD buffer model) in bits. For example, enter five megabits as
	// 5000000.
	HrdBufferSize int32

	// Ignore this setting unless you set qualityTuningLevel to MULTI_PASS. Optional.
	// Specify the maximum bitrate in bits/second. For example, enter five megabits per
	// second as 5000000. The default behavior uses twice the target bitrate as the
	// maximum bitrate.
	MaxBitrate int32

	// Optional. Specify how the service determines the pixel aspect ratio for this
	// output. The default behavior is to use the same pixel aspect ratio as your input
	// video.
	ParControl Vp9ParControl

	// Required when you set Pixel aspect ratio (parControl) to SPECIFIED. On the
	// console, this corresponds to any value other than Follow source. When you
	// specify an output pixel aspect ratio (PAR) that is different from your input
	// video PAR, provide your output PAR as a ratio. For example, for D1/DV NTSC
	// widescreen, you would specify the ratio 40:33. In this example, the value for
	// parDenominator is 33.
	ParDenominator int32

	// Required when you set Pixel aspect ratio (parControl) to SPECIFIED. On the
	// console, this corresponds to any value other than Follow source. When you
	// specify an output pixel aspect ratio (PAR) that is different from your input
	// video PAR, provide your output PAR as a ratio. For example, for D1/DV NTSC
	// widescreen, you would specify the ratio 40:33. In this example, the value for
	// parNumerator is 40.
	ParNumerator int32

	// Optional. Use Quality tuning level (qualityTuningLevel) to choose how you want
	// to trade off encoding speed for output video quality. The default behavior is
	// faster, lower quality, multi-pass encoding.
	QualityTuningLevel Vp9QualityTuningLevel

	// With the VP9 codec, you can use only the variable bitrate (VBR) rate control
	// mode.
	RateControlMode Vp9RateControlMode
}

// Required when you set (Codec) under (AudioDescriptions)>(CodecSettings) to the
// value WAV.
type WavSettings struct {

	// Specify Bit depth (BitDepth), in bits per sample, to choose the encoding quality
	// for this audio track.
	BitDepth int32

	// Specify the number of channels in this output audio track. Valid values are 1
	// and even numbers up to 64. For example, 1, 2, 4, 6, and so on, up to 64.
	Channels int32

	// The service defaults to using RIFF for WAV outputs. If your output audio is
	// likely to exceed 4 GB in file size, or if you otherwise need the extended
	// support of the RF64 format, set your output WAV file format to RF64.
	Format WavFormat

	// Sample rate in Hz.
	SampleRate int32
}

// Code generated by smithy-go-codegen DO NOT EDIT.

package rekognition

import (
	"context"
	awsmiddleware "github.com/aws/aws-sdk-go-v2/aws/middleware"
	"github.com/aws/aws-sdk-go-v2/aws/retry"
	"github.com/aws/aws-sdk-go-v2/aws/signer/v4"
	"github.com/aws/aws-sdk-go-v2/service/rekognition/types"
	smithy "github.com/awslabs/smithy-go"
	"github.com/awslabs/smithy-go/middleware"
	smithyhttp "github.com/awslabs/smithy-go/transport/http"
)

// Detects faces in the input image and adds them to the specified collection.
// Amazon Rekognition doesn't save the actual faces that are detected. Instead, the
// underlying detection algorithm first detects the faces in the input image. For
// each face, the algorithm extracts facial features into a feature vector, and
// stores it in the backend database. Amazon Rekognition uses feature vectors when
// it performs face match and search operations using the SearchFaces () and
// SearchFacesByImage () operations.  <p>For more information, see Adding Faces to
// a Collection in the Amazon Rekognition Developer Guide.</p> <p>To get the number
// of faces in a collection, call <a>DescribeCollection</a>. </p> <p>If you're
// using version 1.0 of the face detection model, <code>IndexFaces</code> indexes
// the 15 largest faces in the input image. Later versions of the face detection
// model index the 100 largest faces in the input image. </p> <p>If you're using
// version 4 or later of the face model, image orientation information is not
// returned in the <code>OrientationCorrection</code> field. </p> <p>To determine
// which version of the model you're using, call <a>DescribeCollection</a> and
// supply the collection ID. You can also get the model version from the value of
// <code>FaceModelVersion</code> in the response from <code>IndexFaces</code> </p>
// <p>For more information, see Model Versioning in the Amazon Rekognition
// Developer Guide.</p> <p>If you provide the optional <code>ExternalImageId</code>
// for the input image you provided, Amazon Rekognition associates this ID with all
// faces that it detects. When you call the <a>ListFaces</a> operation, the
// response returns the external ID. You can use this external image ID to create a
// client-side index to associate the faces with each image. You can then use the
// index to find all faces in an image.</p> <p>You can specify the maximum number
// of faces to index with the <code>MaxFaces</code> input parameter. This is useful
// when you want to index the largest faces in an image and don't want to index
// smaller faces, such as those belonging to people standing in the background.</p>
// <p>The <code>QualityFilter</code> input parameter allows you to filter out
// detected faces that don’t meet a required quality bar. The quality bar is based
// on a variety of common use cases. By default, <code>IndexFaces</code> chooses
// the quality bar that's used to filter faces. You can also explicitly choose the
// quality bar. Use <code>QualityFilter</code>, to set the quality bar by
// specifying <code>LOW</code>, <code>MEDIUM</code>, or <code>HIGH</code>. If you
// do not want to filter detected faces, specify <code>NONE</code>. </p> <note>
// <p>To use quality filtering, you need a collection associated with version 3 of
// the face model or higher. To get the version of the face model associated with a
// collection, call <a>DescribeCollection</a>. </p> </note> <p>Information about
// faces detected in an image, but not indexed, is returned in an array of
// <a>UnindexedFace</a> objects, <code>UnindexedFaces</code>. Faces aren't indexed
// for reasons such as:</p> <ul> <li> <p>The number of faces detected exceeds the
// value of the <code>MaxFaces</code> request parameter.</p> </li> <li> <p>The face
// is too small compared to the image dimensions.</p> </li> <li> <p>The face is too
// blurry.</p> </li> <li> <p>The image is too dark.</p> </li> <li> <p>The face has
// an extreme pose.</p> </li> <li> <p>The face doesn’t have enough detail to be
// suitable for face search.</p> </li> </ul> <p>In response, the
// <code>IndexFaces</code> operation returns an array of metadata for all detected
// faces, <code>FaceRecords</code>. This includes: </p> <ul> <li> <p>The bounding
// box, <code>BoundingBox</code>, of the detected face. </p> </li> <li> <p>A
// confidence value, <code>Confidence</code>, which indicates the confidence that
// the bounding box contains a face.</p> </li> <li> <p>A face ID,
// <code>FaceId</code>, assigned by the service for each face that's detected and
// stored.</p> </li> <li> <p>An image ID, <code>ImageId</code>, assigned by the
// service for the input image.</p> </li> </ul> <p>If you request all facial
// attributes (by using the <code>detectionAttributes</code> parameter), Amazon
// Rekognition returns detailed facial attributes, such as facial landmarks (for
// example, location of eye and mouth) and other facial attributes. If you provide
// the same image, specify the same collection, and use the same external ID in the
// <code>IndexFaces</code> operation, Amazon Rekognition doesn't save duplicate
// face metadata.</p> <p></p> <p>The input image is passed either as base64-encoded
// image bytes, or as a reference to an image in an Amazon S3 bucket. If you use
// the AWS CLI to call Amazon Rekognition operations, passing image bytes isn't
// supported. The image must be formatted as a PNG or JPEG file. </p> <p>This
// operation requires permissions to perform the
// <code>rekognition:IndexFaces</code> action.</p>
func (c *Client) IndexFaces(ctx context.Context, params *IndexFacesInput, optFns ...func(*Options)) (*IndexFacesOutput, error) {
	stack := middleware.NewStack("IndexFaces", smithyhttp.NewStackRequest)
	options := c.options.Copy()
	for _, fn := range optFns {
		fn(&options)
	}
	addawsAwsjson11_serdeOpIndexFacesMiddlewares(stack)
	awsmiddleware.AddRequestInvocationIDMiddleware(stack)
	smithyhttp.AddContentLengthMiddleware(stack)
	AddResolveEndpointMiddleware(stack, options)
	v4.AddComputePayloadSHA256Middleware(stack)
	retry.AddRetryMiddlewares(stack, options)
	addHTTPSignerV4Middleware(stack, options)
	awsmiddleware.AddAttemptClockSkewMiddleware(stack)
	addClientUserAgent(stack)
	smithyhttp.AddErrorCloseResponseBodyMiddleware(stack)
	smithyhttp.AddCloseResponseBodyMiddleware(stack)
	addOpIndexFacesValidationMiddleware(stack)
	stack.Initialize.Add(newServiceMetadataMiddleware_opIndexFaces(options.Region), middleware.Before)
	addResponseErrorWrapper(stack)

	for _, fn := range options.APIOptions {
		if err := fn(stack); err != nil {
			return nil, err
		}
	}
	handler := middleware.DecorateHandler(smithyhttp.NewClientHandler(options.HTTPClient), stack)
	result, metadata, err := handler.Handle(ctx, params)
	if err != nil {
		return nil, &smithy.OperationError{
			ServiceID:     ServiceID,
			OperationName: "IndexFaces",
			Err:           err,
		}
	}
	out := result.(*IndexFacesOutput)
	out.ResultMetadata = metadata
	return out, nil
}

type IndexFacesInput struct {
	// The ID of an existing collection to which you want to add the faces that are
	// detected in the input images.
	CollectionId *string
	// The ID you want to assign to all the faces detected in the image.
	ExternalImageId *string
	// An array of facial attributes that you want to be returned. This can be the
	// default list of attributes or all attributes. If you don't specify a value for
	// Attributes or if you specify ["DEFAULT"], the API returns the following subset
	// of facial attributes: BoundingBox, Confidence, Pose, Quality, and Landmarks. If
	// you provide ["ALL"], all facial attributes are returned, but the operation takes
	// longer to complete. If you provide both, ["ALL", "DEFAULT"], the service uses a
	// logical AND operator to determine which attributes to return (in this case, all
	// attributes).
	DetectionAttributes []types.Attribute
	// The input image as base64-encoded bytes or an S3 object. If you use the AWS CLI
	// to call Amazon Rekognition operations, passing base64-encoded image bytes isn't
	// supported. If you are using an AWS SDK to call Amazon Rekognition, you might not
	// need to base64-encode image bytes passed using the Bytes field. For more
	// information, see Images in the Amazon Rekognition developer guide.
	Image *types.Image
	// A filter that specifies a quality bar for how much filtering is done to identify
	// faces. Filtered faces aren't indexed. If you specify AUTO, Amazon Rekognition
	// chooses the quality bar. If you specify LOW, MEDIUM, or HIGH, filtering removes
	// all faces that don’t meet the chosen quality bar. The default value is AUTO.
	// The quality bar is based on a variety of common use cases. Low-quality
	// detections can occur for a number of reasons. Some examples are an object that's
	// misidentified as a face, a face that's too blurry, or a face with a pose that's
	// too extreme to use. If you specify <code>NONE</code>, no filtering is performed.
	// </p> <p>To use quality filtering, the collection you are using must be
	// associated with version 3 of the face model or higher.</p>
	QualityFilter types.QualityFilter
	// The maximum number of faces to index. The value of MaxFaces must be greater than
	// or equal to 1. IndexFaces returns no more than 100 detected faces in an image,
	// even if you specify a larger value for MaxFaces. If IndexFaces detects more
	// faces than the value of MaxFaces, the faces with the lowest quality are filtered
	// out first. If there are still more faces than the value of MaxFaces, the faces
	// with the smallest bounding boxes are filtered out (up to the number that's
	// needed to satisfy the value of MaxFaces). Information about the unindexed faces
	// is available in the UnindexedFaces array. The faces that are returned by
	// IndexFaces are sorted by the largest face bounding box size to the smallest
	// size, in descending order. MaxFaces can be used with a collection associated
	// with any version of the face model.
	MaxFaces *int32
}

type IndexFacesOutput struct {
	// An array of faces that were detected in the image but weren't indexed. They
	// weren't indexed because the quality filter identified them as low quality, or
	// the MaxFaces request parameter filtered them out. To use the quality filter, you
	// specify the QualityFilter request parameter.
	UnindexedFaces []*types.UnindexedFace
	// The version number of the face detection model that's associated with the input
	// collection (CollectionId).
	FaceModelVersion *string
	// If your collection is associated with a face detection model that's later than
	// version 3.0, the value of OrientationCorrection is always null and no
	// orientation information is returned.  <p>If your collection is associated with a
	// face detection model that's version 3.0 or earlier, the following applies:</p>
	// <ul> <li> <p>If the input image is in .jpeg format, it might contain
	// exchangeable image file format (Exif) metadata that includes the image's
	// orientation. Amazon Rekognition uses this orientation information to perform
	// image correction - the bounding box coordinates are translated to represent
	// object locations after the orientation information in the Exif metadata is used
	// to correct the image orientation. Images in .png format don't contain Exif
	// metadata. The value of <code>OrientationCorrection</code> is null.</p> </li>
	// <li> <p>If the image doesn't contain orientation information in its Exif
	// metadata, Amazon Rekognition returns an estimated orientation (ROTATE_0,
	// ROTATE_90, ROTATE_180, ROTATE_270). Amazon Rekognition doesn’t perform image
	// correction for images. The bounding box coordinates aren't translated and
	// represent the object locations before the image is rotated.</p> </li> </ul>
	// <p>Bounding box information is returned in the <code>FaceRecords</code> array.
	// You can get the version of the face detection model by calling
	// <a>DescribeCollection</a>. </p>
	OrientationCorrection types.OrientationCorrection
	// An array of faces detected and added to the collection. For more information,
	// see Searching Faces in a Collection in the Amazon Rekognition Developer Guide.
	FaceRecords []*types.FaceRecord

	// Metadata pertaining to the operation's result.
	ResultMetadata middleware.Metadata
}

func addawsAwsjson11_serdeOpIndexFacesMiddlewares(stack *middleware.Stack) {
	stack.Serialize.Add(&awsAwsjson11_serializeOpIndexFaces{}, middleware.After)
	stack.Deserialize.Add(&awsAwsjson11_deserializeOpIndexFaces{}, middleware.After)
}

func newServiceMetadataMiddleware_opIndexFaces(region string) awsmiddleware.RegisterServiceMetadata {
	return awsmiddleware.RegisterServiceMetadata{
		Region:        region,
		ServiceID:     ServiceID,
		SigningName:   "rekognition",
		OperationName: "IndexFaces",
	}
}

// Code generated by smithy-go-codegen DO NOT EDIT.

package types

import (
	"time"
)

// Structure containing the estimated age range, in years, for a face. Amazon
// Rekognition estimates an age range for faces detected in the input image.
// Estimated age ranges can overlap. A face of a 5-year-old might have an estimated
// range of 4-6, while the face of a 6-year-old might have an estimated range of
// 4-8.
type AgeRange struct {

	// The highest estimated age.
	High *int32

	// The lowest estimated age.
	Low *int32
}

// Assets are the images that you use to train and evaluate a model version. Assets
// can also contain validation information that you use to debug a failed model
// training.
type Asset struct {

	// The S3 bucket that contains an Amazon Sagemaker Ground Truth format manifest
	// file.
	GroundTruthManifest *GroundTruthManifest
}

// Metadata information about an audio stream. An array of AudioMetadata objects
// for the audio streams found in a stored video is returned by
// GetSegmentDetection.
type AudioMetadata struct {

	// The audio codec used to encode or decode the audio stream.
	Codec *string

	// The duration of the audio stream in milliseconds.
	DurationMillis *int64

	// The number of audio channels in the segment.
	NumberOfChannels *int64

	// The sample rate for the audio stream.
	SampleRate *int64
}

// Indicates whether or not the face has a beard, and the confidence level in the
// determination.
type Beard struct {

	// Level of confidence in the determination.
	Confidence *float32

	// Boolean value that indicates whether the face has beard or not.
	Value bool
}

// Identifies the bounding box around the label, face, text or personal protective
// equipment. The left (x-coordinate) and top (y-coordinate) are coordinates
// representing the top and left sides of the bounding box. Note that the
// upper-left corner of the image is the origin (0,0). The top and left values
// returned are ratios of the overall image size. For example, if the input image
// is 700x200 pixels, and the top-left coordinate of the bounding box is 350x50
// pixels, the API returns a left value of 0.5 (350/700) and a top value of 0.25
// (50/200). The width and height values represent the dimensions of the bounding
// box as a ratio of the overall image dimension. For example, if the input image
// is 700x200 pixels, and the bounding box width is 70 pixels, the width returned
// is 0.1. The bounding box coordinates can have negative values. For example, if
// Amazon Rekognition is able to detect a face that is at the image edge and is
// only partially visible, the service can return coordinates that are outside the
// image bounds and, depending on the image edge, you might get negative values or
// values greater than 1 for the left or top values.
type BoundingBox struct {

	// Height of the bounding box as a ratio of the overall image height.
	Height *float32

	// Left coordinate of the bounding box as a ratio of overall image width.
	Left *float32

	// Top coordinate of the bounding box as a ratio of overall image height.
	Top *float32

	// Width of the bounding box as a ratio of the overall image width.
	Width *float32
}

// Provides information about a celebrity recognized by the RecognizeCelebrities
// operation.
type Celebrity struct {

	// Provides information about the celebrity's face, such as its location on the
	// image.
	Face *ComparedFace

	// A unique identifier for the celebrity.
	Id *string

	// The confidence, in percentage, that Amazon Rekognition has that the recognized
	// face is the celebrity.
	MatchConfidence *float32

	// The name of the celebrity.
	Name *string

	// An array of URLs pointing to additional information about the celebrity. If
	// there is no additional information about the celebrity, this list is empty.
	Urls []string
}

// Information about a recognized celebrity.
type CelebrityDetail struct {

	// Bounding box around the body of a celebrity.
	BoundingBox *BoundingBox

	// The confidence, in percentage, that Amazon Rekognition has that the recognized
	// face is the celebrity.
	Confidence *float32

	// Face details for the recognized celebrity.
	Face *FaceDetail

	// The unique identifier for the celebrity.
	Id *string

	// The name of the celebrity.
	Name *string

	// An array of URLs pointing to additional celebrity information.
	Urls []string
}

// Information about a detected celebrity and the time the celebrity was detected
// in a stored video. For more information, see GetCelebrityRecognition in the
// Amazon Rekognition Developer Guide.
type CelebrityRecognition struct {

	// Information about a recognized celebrity.
	Celebrity *CelebrityDetail

	// The time, in milliseconds from the start of the video, that the celebrity was
	// recognized.
	Timestamp int64
}

// Provides face metadata for target image faces that are analyzed by CompareFaces
// and RecognizeCelebrities.
type ComparedFace struct {

	// Bounding box of the face.
	BoundingBox *BoundingBox

	// Level of confidence that what the bounding box contains is a face.
	Confidence *float32

	// An array of facial landmarks.
	Landmarks []Landmark

	// Indicates the pose of the face as determined by its pitch, roll, and yaw.
	Pose *Pose

	// Identifies face image brightness and sharpness.
	Quality *ImageQuality
}

// Type that describes the face Amazon Rekognition chose to compare with the faces
// in the target. This contains a bounding box for the selected face and confidence
// level that the bounding box contains a face. Note that Amazon Rekognition
// selects the largest face in the source image for this comparison.
type ComparedSourceImageFace struct {

	// Bounding box of the face.
	BoundingBox *BoundingBox

	// Confidence level that the selected bounding box contains a face.
	Confidence *float32
}

// Provides information about a face in a target image that matches the source
// image face analyzed by CompareFaces. The Face property contains the bounding box
// of the face in the target image. The Similarity property is the confidence that
// the source image face matches the face in the bounding box.
type CompareFacesMatch struct {

	// Provides face metadata (bounding box and confidence that the bounding box
	// actually contains a face).
	Face *ComparedFace

	// Level of confidence that the faces match.
	Similarity *float32
}

// Information about an unsafe content label detection in a stored video.
type ContentModerationDetection struct {

	// The unsafe content label detected by in the stored video.
	ModerationLabel *ModerationLabel

	// Time, in milliseconds from the beginning of the video, that the unsafe content
	// label was detected.
	Timestamp int64
}

// Information about an item of Personal Protective Equipment covering a
// corresponding body part. For more information, see DetectProtectiveEquipment.
type CoversBodyPart struct {

	// The confidence that Amazon Rekognition has in the value of Value.
	Confidence *float32

	// True if the PPE covers the corresponding body part, otherwise false.
	Value bool
}

// A custom label detected in an image by a call to DetectCustomLabels.
type CustomLabel struct {

	// The confidence that the model has in the detection of the custom label. The
	// range is 0-100. A higher value indicates a higher confidence.
	Confidence *float32

	// The location of the detected object on the image that corresponds to the custom
	// label. Includes an axis aligned coarse bounding box surrounding the object and a
	// finer grain polygon for more accurate spatial information.
	Geometry *Geometry

	// The name of the custom label.
	Name *string
}

// A set of parameters that allow you to filter out certain results from your
// returned results.
type DetectionFilter struct {

	// Sets the minimum height of the word bounding box. Words with bounding box
	// heights lesser than this value will be excluded from the result. Value is
	// relative to the video frame height.
	MinBoundingBoxHeight *float32

	// Sets the minimum width of the word bounding box. Words with bounding boxes
	// widths lesser than this value will be excluded from the result. Value is
	// relative to the video frame width.
	MinBoundingBoxWidth *float32

	// Sets confidence of word detection. Words with detection confidence below this
	// will be excluded from the result. Values should be between 0.5 and 1 as Text in
	// Video will not return any result below 0.5.
	MinConfidence *float32
}

// A set of optional parameters that you can use to set the criteria that the text
// must meet to be included in your response. WordFilter looks at a word’s height,
// width, and minimum confidence. RegionOfInterest lets you set a specific region
// of the image to look for text in.
type DetectTextFilters struct {

	// A Filter focusing on a certain area of the image. Uses a BoundingBox object to
	// set the region of the image.
	RegionsOfInterest []RegionOfInterest

	// A set of parameters that allow you to filter out certain results from your
	// returned results.
	WordFilter *DetectionFilter
}

// The emotions that appear to be expressed on the face, and the confidence level
// in the determination. The API is only making a determination of the physical
// appearance of a person's face. It is not a determination of the person’s
// internal emotional state and should not be used in such a way. For example, a
// person pretending to have a sad face might not be sad emotionally.
type Emotion struct {

	// Level of confidence in the determination.
	Confidence *float32

	// Type of emotion detected.
	Type EmotionName
}

// Information about an item of Personal Protective Equipment (PPE) detected by
// DetectProtectiveEquipment. For more information, see DetectProtectiveEquipment.
type EquipmentDetection struct {

	// A bounding box surrounding the item of detected PPE.
	BoundingBox *BoundingBox

	// The confidence that Amazon Rekognition has that the bounding box (BoundingBox)
	// contains an item of PPE.
	Confidence *float32

	// Information about the body part covered by the detected PPE.
	CoversBodyPart *CoversBodyPart

	// The type of detected PPE.
	Type ProtectiveEquipmentType
}

// The evaluation results for the training of a model.
type EvaluationResult struct {

	// The F1 score for the evaluation of all labels. The F1 score metric evaluates the
	// overall precision and recall performance of the model as a single value. A
	// higher value indicates better precision and recall performance. A lower score
	// indicates that precision, recall, or both are performing poorly.
	F1Score *float32

	// The S3 bucket that contains the training summary.
	Summary *Summary
}

// Indicates whether or not the face is wearing eye glasses, and the confidence
// level in the determination.
type Eyeglasses struct {

	// Level of confidence in the determination.
	Confidence *float32

	// Boolean value that indicates whether the face is wearing eye glasses or not.
	Value bool
}

// Indicates whether or not the eyes on the face are open, and the confidence level
// in the determination.
type EyeOpen struct {

	// Level of confidence in the determination.
	Confidence *float32

	// Boolean value that indicates whether the eyes on the face are open.
	Value bool
}

// Describes the face properties such as the bounding box, face ID, image ID of the
// input image, and external image ID that you assigned.
type Face struct {

	// Bounding box of the face.
	BoundingBox *BoundingBox

	// Confidence level that the bounding box contains a face (and not a different
	// object such as a tree).
	Confidence *float32

	// Identifier that you assign to all the faces in the input image.
	ExternalImageId *string

	// Unique identifier that Amazon Rekognition assigns to the face.
	FaceId *string

	// Unique identifier that Amazon Rekognition assigns to the input image.
	ImageId *string
}

// Structure containing attributes of the face that the algorithm detected. A
// FaceDetail object contains either the default facial attributes or all facial
// attributes. The default attributes are BoundingBox, Confidence, Landmarks, Pose,
// and Quality. GetFaceDetection is the only Amazon Rekognition Video stored video
// operation that can return a FaceDetail object with all attributes. To specify
// which attributes to return, use the FaceAttributes input parameter for
// StartFaceDetection. The following Amazon Rekognition Video operations return
// only the default attributes. The corresponding Start operations don't have a
// FaceAttributes input parameter.
//
// * GetCelebrityRecognition
//
// *
// GetPersonTracking
//
// * GetFaceSearch
//
// The Amazon Rekognition Image DetectFaces and
// IndexFaces operations can return all facial attributes. To specify which
// attributes to return, use the Attributes input parameter for DetectFaces. For
// IndexFaces, use the DetectAttributes input parameter.
type FaceDetail struct {

	// The estimated age range, in years, for the face. Low represents the lowest
	// estimated age and High represents the highest estimated age.
	AgeRange *AgeRange

	// Indicates whether or not the face has a beard, and the confidence level in the
	// determination.
	Beard *Beard

	// Bounding box of the face. Default attribute.
	BoundingBox *BoundingBox

	// Confidence level that the bounding box contains a face (and not a different
	// object such as a tree). Default attribute.
	Confidence *float32

	// The emotions that appear to be expressed on the face, and the confidence level
	// in the determination. The API is only making a determination of the physical
	// appearance of a person's face. It is not a determination of the person’s
	// internal emotional state and should not be used in such a way. For example, a
	// person pretending to have a sad face might not be sad emotionally.
	Emotions []Emotion

	// Indicates whether or not the face is wearing eye glasses, and the confidence
	// level in the determination.
	Eyeglasses *Eyeglasses

	// Indicates whether or not the eyes on the face are open, and the confidence level
	// in the determination.
	EyesOpen *EyeOpen

	// The predicted gender of a detected face.
	Gender *Gender

	// Indicates the location of landmarks on the face. Default attribute.
	Landmarks []Landmark

	// Indicates whether or not the mouth on the face is open, and the confidence level
	// in the determination.
	MouthOpen *MouthOpen

	// Indicates whether or not the face has a mustache, and the confidence level in
	// the determination.
	Mustache *Mustache

	// Indicates the pose of the face as determined by its pitch, roll, and yaw.
	// Default attribute.
	Pose *Pose

	// Identifies image brightness and sharpness. Default attribute.
	Quality *ImageQuality

	// Indicates whether or not the face is smiling, and the confidence level in the
	// determination.
	Smile *Smile

	// Indicates whether or not the face is wearing sunglasses, and the confidence
	// level in the determination.
	Sunglasses *Sunglasses
}

// Information about a face detected in a video analysis request and the time the
// face was detected in the video.
type FaceDetection struct {

	// The face properties for the detected face.
	Face *FaceDetail

	// Time, in milliseconds from the start of the video, that the face was detected.
	Timestamp int64
}

// Provides face metadata. In addition, it also provides the confidence in the
// match of this face with the input face.
type FaceMatch struct {

	// Describes the face properties such as the bounding box, face ID, image ID of the
	// source image, and external image ID that you assigned.
	Face *Face

	// Confidence in the match of this face with the input face.
	Similarity *float32
}

// Object containing both the face metadata (stored in the backend database), and
// facial attributes that are detected but aren't stored in the database.
type FaceRecord struct {

	// Describes the face properties such as the bounding box, face ID, image ID of the
	// input image, and external image ID that you assigned.
	Face *Face

	// Structure containing attributes of the face that the algorithm detected.
	FaceDetail *FaceDetail
}

// Input face recognition parameters for an Amazon Rekognition stream processor.
// FaceRecognitionSettings is a request parameter for CreateStreamProcessor.
type FaceSearchSettings struct {

	// The ID of a collection that contains faces that you want to search for.
	CollectionId *string

	// Minimum face match confidence score that must be met to return a result for a
	// recognized face. Default is 80. 0 is the lowest confidence. 100 is the highest
	// confidence.
	FaceMatchThreshold *float32
}

// The predicted gender of a detected face. Amazon Rekognition makes gender binary
// (male/female) predictions based on the physical appearance of a face in a
// particular image. This kind of prediction is not designed to categorize a
// person’s gender identity, and you shouldn't use Amazon Rekognition to make such
// a determination. For example, a male actor wearing a long-haired wig and
// earrings for a role might be predicted as female. Using Amazon Rekognition to
// make gender binary predictions is best suited for use cases where aggregate
// gender distribution statistics need to be analyzed without identifying specific
// users. For example, the percentage of female users compared to male users on a
// social media platform. We don't recommend using gender binary predictions to
// make decisions that impact  an individual's rights, privacy, or access to
// services.
type Gender struct {

	// Level of confidence in the prediction.
	Confidence *float32

	// The predicted gender of the face.
	Value GenderType
}

// Information about where an object (DetectCustomLabels) or text (DetectText) is
// located on an image.
type Geometry struct {

	// An axis-aligned coarse representation of the detected item's location on the
	// image.
	BoundingBox *BoundingBox

	// Within the bounding box, a fine-grained polygon around the detected item.
	Polygon []Point
}

// The S3 bucket that contains an Amazon Sagemaker Ground Truth format manifest
// file.
type GroundTruthManifest struct {

	// Provides the S3 bucket name and object name. The region for the S3 bucket
	// containing the S3 object must match the region you use for Amazon Rekognition
	// operations. For Amazon Rekognition to process an S3 object, the user must have
	// permission to access the S3 object. For more information, see Resource-Based
	// Policies in the Amazon Rekognition Developer Guide.
	S3Object *S3Object
}

// Shows the results of the human in the loop evaluation. If there is no
// HumanLoopArn, the input did not trigger human review.
type HumanLoopActivationOutput struct {

	// Shows the result of condition evaluations, including those conditions which
	// activated a human review.
	//
	// This value conforms to the media type: application/json
	HumanLoopActivationConditionsEvaluationResults *string

	// Shows if and why human review was needed.
	HumanLoopActivationReasons []string

	// The Amazon Resource Name (ARN) of the HumanLoop created.
	HumanLoopArn *string
}

// Sets up the flow definition the image will be sent to if one of the conditions
// is met. You can also set certain attributes of the image before review.
type HumanLoopConfig struct {

	// The Amazon Resource Name (ARN) of the flow definition. You can create a flow
	// definition by using the Amazon Sagemaker CreateFlowDefinition
	// (https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateFlowDefinition.html)
	// Operation.
	//
	// This member is required.
	FlowDefinitionArn *string

	// The name of the human review used for this image. This should be kept unique
	// within a region.
	//
	// This member is required.
	HumanLoopName *string

	// Sets attributes of the input data.
	DataAttributes *HumanLoopDataAttributes
}

// Allows you to set attributes of the image. Currently, you can declare an image
// as free of personally identifiable information.
type HumanLoopDataAttributes struct {

	// Sets whether the input image is free of personally identifiable information.
	ContentClassifiers []ContentClassifier
}

// Provides the input image either as bytes or an S3 object. You pass image bytes
// to an Amazon Rekognition API operation by using the Bytes property. For example,
// you would use the Bytes property to pass an image loaded from a local file
// system. Image bytes passed by using the Bytes property must be base64-encoded.
// Your code may not need to encode image bytes if you are using an AWS SDK to call
// Amazon Rekognition API operations. For more information, see Analyzing an Image
// Loaded from a Local File System in the Amazon Rekognition Developer Guide. You
// pass images stored in an S3 bucket to an Amazon Rekognition API operation by
// using the S3Object property. Images stored in an S3 bucket do not need to be
// base64-encoded. The region for the S3 bucket containing the S3 object must match
// the region you use for Amazon Rekognition operations. If you use the AWS CLI to
// call Amazon Rekognition operations, passing image bytes using the Bytes property
// is not supported. You must first upload the image to an Amazon S3 bucket and
// then call the operation using the S3Object property. For Amazon Rekognition to
// process an S3 object, the user must have permission to access the S3 object. For
// more information, see Resource Based Policies in the Amazon Rekognition
// Developer Guide.
type Image struct {

	// Blob of image bytes up to 5 MBs.
	Bytes []byte

	// Identifies an S3 object as the image source.
	S3Object *S3Object
}

// Identifies face image brightness and sharpness.
type ImageQuality struct {

	// Value representing brightness of the face. The service returns a value between 0
	// and 100 (inclusive). A higher value indicates a brighter face image.
	Brightness *float32

	// Value representing sharpness of the face. The service returns a value between 0
	// and 100 (inclusive). A higher value indicates a sharper face image.
	Sharpness *float32
}

// An instance of a label returned by Amazon Rekognition Image (DetectLabels) or by
// Amazon Rekognition Video (GetLabelDetection).
type Instance struct {

	// The position of the label instance on the image.
	BoundingBox *BoundingBox

	// The confidence that Amazon Rekognition has in the accuracy of the bounding box.
	Confidence *float32
}

// The Kinesis data stream Amazon Rekognition to which the analysis results of a
// Amazon Rekognition stream processor are streamed. For more information, see
// CreateStreamProcessor in the Amazon Rekognition Developer Guide.
type KinesisDataStream struct {

	// ARN of the output Amazon Kinesis Data Streams stream.
	Arn *string
}

// Kinesis video stream stream that provides the source streaming video for a
// Amazon Rekognition Video stream processor. For more information, see
// CreateStreamProcessor in the Amazon Rekognition Developer Guide.
type KinesisVideoStream struct {

	// ARN of the Kinesis video stream stream that streams the source video.
	Arn *string
}

// Structure containing details about the detected label, including the name,
// detected instances, parent labels, and level of confidence.
type Label struct {

	// Level of confidence.
	Confidence *float32

	// If Label represents an object, Instances contains the bounding boxes for each
	// instance of the detected object. Bounding boxes are returned for common object
	// labels such as people, cars, furniture, apparel or pets.
	Instances []Instance

	// The name (label) of the object or scene.
	Name *string

	// The parent labels for a label. The response includes all ancestor labels.
	Parents []Parent
}

// Information about a label detected in a video analysis request and the time the
// label was detected in the video.
type LabelDetection struct {

	// Details about the detected label.
	Label *Label

	// Time, in milliseconds from the start of the video, that the label was detected.
	Timestamp int64
}

// Indicates the location of the landmark on the face.
type Landmark struct {

	// Type of landmark.
	Type LandmarkType

	// The x-coordinate of the landmark expressed as a ratio of the width of the image.
	// The x-coordinate is measured from the left-side of the image. For example, if
	// the image is 700 pixels wide and the x-coordinate of the landmark is at 350
	// pixels, this value is 0.5.
	X *float32

	// The y-coordinate of the landmark expressed as a ratio of the height of the
	// image. The y-coordinate is measured from the top of the image. For example, if
	// the image height is 200 pixels and the y-coordinate of the landmark is at 50
	// pixels, this value is 0.25.
	Y *float32
}

// Provides information about a single type of unsafe content found in an image or
// video. Each type of moderated content has a label within a hierarchical
// taxonomy. For more information, see Detecting Unsafe Content in the Amazon
// Rekognition Developer Guide.
type ModerationLabel struct {

	// Specifies the confidence that Amazon Rekognition has that the label has been
	// correctly identified. If you don't specify the MinConfidence parameter in the
	// call to DetectModerationLabels, the operation returns labels with a confidence
	// value greater than or equal to 50 percent.
	Confidence *float32

	// The label name for the type of unsafe content detected in the image.
	Name *string

	// The name for the parent label. Labels at the top level of the hierarchy have the
	// parent label "".
	ParentName *string
}

// Indicates whether or not the mouth on the face is open, and the confidence level
// in the determination.
type MouthOpen struct {

	// Level of confidence in the determination.
	Confidence *float32

	// Boolean value that indicates whether the mouth on the face is open or not.
	Value bool
}

// Indicates whether or not the face has a mustache, and the confidence level in
// the determination.
type Mustache struct {

	// Level of confidence in the determination.
	Confidence *float32

	// Boolean value that indicates whether the face has mustache or not.
	Value bool
}

// The Amazon Simple Notification Service topic to which Amazon Rekognition
// publishes the completion status of a video analysis operation. For more
// information, see api-video.
type NotificationChannel struct {

	// The ARN of an IAM role that gives Amazon Rekognition publishing permissions to
	// the Amazon SNS topic.
	//
	// This member is required.
	RoleArn *string

	// The Amazon SNS topic to which Amazon Rekognition to posts the completion status.
	//
	// This member is required.
	SNSTopicArn *string
}

// The S3 bucket and folder location where training output is placed.
type OutputConfig struct {

	// The S3 bucket where training output is placed.
	S3Bucket *string

	// The prefix applied to the training output files.
	S3KeyPrefix *string
}

// A parent label for a label. A label can have 0, 1, or more parents.
type Parent struct {

	// The name of the parent label.
	Name *string
}

// Details about a person detected in a video analysis request.
type PersonDetail struct {

	// Bounding box around the detected person.
	BoundingBox *BoundingBox

	// Face details for the detected person.
	Face *FaceDetail

	// Identifier for the person detected person within a video. Use to keep track of
	// the person throughout the video. The identifier is not stored by Amazon
	// Rekognition.
	Index int64
}

// Details and path tracking information for a single time a person's path is
// tracked in a video. Amazon Rekognition operations that track people's paths
// return an array of PersonDetection objects with elements for each time a
// person's path is tracked in a video. For more information, see GetPersonTracking
// in the Amazon Rekognition Developer Guide.
type PersonDetection struct {

	// Details about a person whose path was tracked in a video.
	Person *PersonDetail

	// The time, in milliseconds from the start of the video, that the person's path
	// was tracked.
	Timestamp int64
}

// Information about a person whose face matches a face(s) in an Amazon Rekognition
// collection. Includes information about the faces in the Amazon Rekognition
// collection (FaceMatch), information about the person (PersonDetail), and the
// time stamp for when the person was detected in a video. An array of PersonMatch
// objects is returned by GetFaceSearch.
type PersonMatch struct {

	// Information about the faces in the input collection that match the face of a
	// person in the video.
	FaceMatches []FaceMatch

	// Information about the matched person.
	Person *PersonDetail

	// The time, in milliseconds from the beginning of the video, that the person was
	// matched in the video.
	Timestamp int64
}

// The X and Y coordinates of a point on an image. The X and Y values returned are
// ratios of the overall image size. For example, if the input image is 700x200 and
// the operation returns X=0.5 and Y=0.25, then the point is at the (350,50) pixel
// coordinate on the image. An array of Point objects, Polygon, is returned by
// DetectText and by DetectCustomLabels. Polygon represents a fine-grained polygon
// around a detected item. For more information, see Geometry in the Amazon
// Rekognition Developer Guide.
type Point struct {

	// The value of the X coordinate for a point on a Polygon.
	X *float32

	// The value of the Y coordinate for a point on a Polygon.
	Y *float32
}

// Indicates the pose of the face as determined by its pitch, roll, and yaw.
type Pose struct {

	// Value representing the face rotation on the pitch axis.
	Pitch *float32

	// Value representing the face rotation on the roll axis.
	Roll *float32

	// Value representing the face rotation on the yaw axis.
	Yaw *float32
}

// A description of a Amazon Rekognition Custom Labels project.
type ProjectDescription struct {

	// The Unix timestamp for the date and time that the project was created.
	CreationTimestamp *time.Time

	// The Amazon Resource Name (ARN) of the project.
	ProjectArn *string

	// The current status of the project.
	Status ProjectStatus
}

// The description of a version of a model.
type ProjectVersionDescription struct {

	// The duration, in seconds, that the model version has been billed for training.
	// This value is only returned if the model version has been successfully trained.
	BillableTrainingTimeInSeconds *int64

	// The Unix datetime for the date and time that training started.
	CreationTimestamp *time.Time

	// The training results. EvaluationResult is only returned if training is
	// successful.
	EvaluationResult *EvaluationResult

	// The location of the summary manifest. The summary manifest provides aggregate
	// data validation results for the training and test datasets.
	ManifestSummary *GroundTruthManifest

	// The minimum number of inference units used by the model. For more information,
	// see StartProjectVersion.
	MinInferenceUnits *int32

	// The location where training results are saved.
	OutputConfig *OutputConfig

	// The Amazon Resource Name (ARN) of the model version.
	ProjectVersionArn *string

	// The current status of the model version.
	Status ProjectVersionStatus

	// A descriptive message for an error or warning that occurred.
	StatusMessage *string

	// Contains information about the testing results.
	TestingDataResult *TestingDataResult

	// Contains information about the training results.
	TrainingDataResult *TrainingDataResult

	// The Unix date and time that training of the model ended.
	TrainingEndTimestamp *time.Time
}

// Information about a body part detected by DetectProtectiveEquipment that
// contains PPE. An array of ProtectiveEquipmentBodyPart objects is returned for
// each person detected by DetectProtectiveEquipment.
type ProtectiveEquipmentBodyPart struct {

	// The confidence that Amazon Rekognition has in the detection accuracy of the
	// detected body part.
	Confidence *float32

	// An array of Personal Protective Equipment items detected around a body part.
	EquipmentDetections []EquipmentDetection

	// The detected body part.
	Name BodyPart
}

// A person detected by a call to DetectProtectiveEquipment. The API returns all
// persons detected in the input image in an array of ProtectiveEquipmentPerson
// objects.
type ProtectiveEquipmentPerson struct {

	// An array of body parts detected on a person's body (including body parts without
	// PPE).
	BodyParts []ProtectiveEquipmentBodyPart

	// A bounding box around the detected person.
	BoundingBox *BoundingBox

	// The confidence that Amazon Rekognition has that the bounding box contains a
	// person.
	Confidence *float32

	// The identifier for the detected person. The identifier is only unique for a
	// single call to DetectProtectiveEquipment.
	Id *int32
}

// Specifies summary attributes to return from a call to DetectProtectiveEquipment.
// You can specify which types of PPE to summarize. You can also specify a minimum
// confidence value for detections. Summary information is returned in the Summary
// (ProtectiveEquipmentSummary) field of the response from
// DetectProtectiveEquipment. The summary includes which persons in an image were
// detected wearing the requested types of person protective equipment (PPE), which
// persons were detected as not wearing PPE, and the persons in which a
// determination could not be made. For more information, see
// ProtectiveEquipmentSummary.
type ProtectiveEquipmentSummarizationAttributes struct {

	// The minimum confidence level for which you want summary information. The
	// confidence level applies to person detection, body part detection, equipment
	// detection, and body part coverage. Amazon Rekognition doesn't return summary
	// information with a confidence than this specified value. There isn't a default
	// value. Specify a MinConfidence value that is between 50-100% as
	// DetectProtectiveEquipment returns predictions only where the detection
	// confidence is between 50% - 100%. If you specify a value that is less than 50%,
	// the results are the same specifying a value of 50%.
	//
	// This member is required.
	MinConfidence *float32

	// An array of personal protective equipment types for which you want summary
	// information. If a person is detected wearing a required requipment type, the
	// person's ID is added to the PersonsWithRequiredEquipment array field returned in
	// ProtectiveEquipmentSummary by DetectProtectiveEquipment.
	//
	// This member is required.
	RequiredEquipmentTypes []ProtectiveEquipmentType
}

// Summary information for required items of personal protective equipment (PPE)
// detected on persons by a call to DetectProtectiveEquipment. You specify the
// required type of PPE in the SummarizationAttributes
// (ProtectiveEquipmentSummarizationAttributes) input parameter. The summary
// includes which persons were detected wearing the required personal protective
// equipment (PersonsWithRequiredEquipment), which persons were detected as not
// wearing the required PPE (PersonsWithoutRequiredEquipment), and the persons in
// which a determination could not be made (PersonsIndeterminate). To get a total
// for each category, use the size of the field array. For example, to find out how
// many people were detected as wearing the specified PPE, use the size of the
// PersonsWithRequiredEquipment array. If you want to find out more about a person,
// such as the location (BoundingBox) of the person on the image, use the person ID
// in each array element. Each person ID matches the ID field of a
// ProtectiveEquipmentPerson object returned in the Persons array by
// DetectProtectiveEquipment.
type ProtectiveEquipmentSummary struct {

	// An array of IDs for persons where it was not possible to determine if they are
	// wearing personal protective equipment.
	PersonsIndeterminate []int32

	// An array of IDs for persons who are wearing detected personal protective
	// equipment.
	PersonsWithRequiredEquipment []int32

	// An array of IDs for persons who are not wearing all of the types of PPE
	// specified in the RequiredEquipmentTypes field of the detected personal
	// protective equipment.
	PersonsWithoutRequiredEquipment []int32
}

// Specifies a location within the frame that Rekognition checks for text. Uses a
// BoundingBox object to set a region of the screen. A word is included in the
// region if the word is more than half in that region. If there is more than one
// region, the word will be compared with all regions of the screen. Any word more
// than half in a region is kept in the results.
type RegionOfInterest struct {

	// The box representing a region of interest on screen.
	BoundingBox *BoundingBox
}

// Provides the S3 bucket name and object name. The region for the S3 bucket
// containing the S3 object must match the region you use for Amazon Rekognition
// operations. For Amazon Rekognition to process an S3 object, the user must have
// permission to access the S3 object. For more information, see Resource-Based
// Policies in the Amazon Rekognition Developer Guide.
type S3Object struct {

	// Name of the S3 bucket.
	Bucket *string

	// S3 object key name.
	Name *string

	// If the bucket is versioning enabled, you can specify the object version.
	Version *string
}

// A technical cue or shot detection segment detected in a video. An array of
// SegmentDetection objects containing all segments detected in a stored video is
// returned by GetSegmentDetection.
type SegmentDetection struct {

	// The duration of the detected segment in milliseconds.
	DurationMillis *int64

	// The duration of the timecode for the detected segment in SMPTE format.
	DurationSMPTE *string

	// The frame-accurate SMPTE timecode, from the start of a video, for the end of a
	// detected segment. EndTimecode is in HH:MM:SS:fr format (and ;fr for drop
	// frame-rates).
	EndTimecodeSMPTE *string

	// The end time of the detected segment, in milliseconds, from the start of the
	// video. This value is rounded down.
	EndTimestampMillis int64

	// If the segment is a shot detection, contains information about the shot
	// detection.
	ShotSegment *ShotSegment

	// The frame-accurate SMPTE timecode, from the start of a video, for the start of a
	// detected segment. StartTimecode is in HH:MM:SS:fr format (and ;fr for drop
	// frame-rates).
	StartTimecodeSMPTE *string

	// The start time of the detected segment in milliseconds from the start of the
	// video. This value is rounded down. For example, if the actual timestamp is
	// 100.6667 milliseconds, Amazon Rekognition Video returns a value of 100 millis.
	StartTimestampMillis int64

	// If the segment is a technical cue, contains information about the technical cue.
	TechnicalCueSegment *TechnicalCueSegment

	// The type of the segment. Valid values are TECHNICAL_CUE and SHOT.
	Type SegmentType
}

// Information about the type of a segment requested in a call to
// StartSegmentDetection. An array of SegmentTypeInfo objects is returned by the
// response from GetSegmentDetection.
type SegmentTypeInfo struct {

	// The version of the model used to detect segments.
	ModelVersion *string

	// The type of a segment (technical cue or shot detection).
	Type SegmentType
}

// Information about a shot detection segment detected in a video. For more
// information, see SegmentDetection.
type ShotSegment struct {

	// The confidence that Amazon Rekognition Video has in the accuracy of the detected
	// segment.
	Confidence *float32

	// An Identifier for a shot detection segment detected in a video.
	Index *int64
}

// Indicates whether or not the face is smiling, and the confidence level in the
// determination.
type Smile struct {

	// Level of confidence in the determination.
	Confidence *float32

	// Boolean value that indicates whether the face is smiling or not.
	Value bool
}

// Filters applied to the technical cue or shot detection segments. For more
// information, see StartSegmentDetection.
type StartSegmentDetectionFilters struct {

	// Filters that are specific to shot detections.
	ShotFilter *StartShotDetectionFilter

	// Filters that are specific to technical cues.
	TechnicalCueFilter *StartTechnicalCueDetectionFilter
}

// Filters for the shot detection segments returned by GetSegmentDetection. For
// more information, see StartSegmentDetectionFilters.
type StartShotDetectionFilter struct {

	// Specifies the minimum confidence that Amazon Rekognition Video must have in
	// order to return a detected segment. Confidence represents how certain Amazon
	// Rekognition is that a segment is correctly identified. 0 is the lowest
	// confidence. 100 is the highest confidence. Amazon Rekognition Video doesn't
	// return any segments with a confidence level lower than this specified value. If
	// you don't specify MinSegmentConfidence, the GetSegmentDetection returns segments
	// with confidence values greater than or equal to 50 percent.
	MinSegmentConfidence *float32
}

// Filters for the technical segments returned by GetSegmentDetection. For more
// information, see StartSegmentDetectionFilters.
type StartTechnicalCueDetectionFilter struct {

	// Specifies the minimum confidence that Amazon Rekognition Video must have in
	// order to return a detected segment. Confidence represents how certain Amazon
	// Rekognition is that a segment is correctly identified. 0 is the lowest
	// confidence. 100 is the highest confidence. Amazon Rekognition Video doesn't
	// return any segments with a confidence level lower than this specified value. If
	// you don't specify MinSegmentConfidence, GetSegmentDetection returns segments
	// with confidence values greater than or equal to 50 percent.
	MinSegmentConfidence *float32
}

// Set of optional parameters that let you set the criteria text must meet to be
// included in your response. WordFilter looks at a word's height, width and
// minimum confidence. RegionOfInterest lets you set a specific region of the
// screen to look for text in.
type StartTextDetectionFilters struct {

	// Filter focusing on a certain area of the frame. Uses a BoundingBox object to set
	// the region of the screen.
	RegionsOfInterest []RegionOfInterest

	// Filters focusing on qualities of the text, such as confidence or size.
	WordFilter *DetectionFilter
}

// An object that recognizes faces in a streaming video. An Amazon Rekognition
// stream processor is created by a call to CreateStreamProcessor. The request
// parameters for CreateStreamProcessor describe the Kinesis video stream source
// for the streaming video, face recognition parameters, and where to stream the
// analysis resullts.
type StreamProcessor struct {

	// Name of the Amazon Rekognition stream processor.
	Name *string

	// Current status of the Amazon Rekognition stream processor.
	Status StreamProcessorStatus
}

// Information about the source streaming video.
type StreamProcessorInput struct {

	// The Kinesis video stream input stream for the source streaming video.
	KinesisVideoStream *KinesisVideoStream
}

// Information about the Amazon Kinesis Data Streams stream to which a Amazon
// Rekognition Video stream processor streams the results of a video analysis. For
// more information, see CreateStreamProcessor in the Amazon Rekognition Developer
// Guide.
type StreamProcessorOutput struct {

	// The Amazon Kinesis Data Streams stream to which the Amazon Rekognition stream
	// processor streams the analysis results.
	KinesisDataStream *KinesisDataStream
}

// Input parameters used to recognize faces in a streaming video analyzed by a
// Amazon Rekognition stream processor.
type StreamProcessorSettings struct {

	// Face search settings to use on a streaming video.
	FaceSearch *FaceSearchSettings
}

// The S3 bucket that contains the training summary. The training summary includes
// aggregated evaluation metrics for the entire testing dataset and metrics for
// each individual label. You get the training summary S3 bucket location by
// calling DescribeProjectVersions.
type Summary struct {

	// Provides the S3 bucket name and object name. The region for the S3 bucket
	// containing the S3 object must match the region you use for Amazon Rekognition
	// operations. For Amazon Rekognition to process an S3 object, the user must have
	// permission to access the S3 object. For more information, see Resource-Based
	// Policies in the Amazon Rekognition Developer Guide.
	S3Object *S3Object
}

// Indicates whether or not the face is wearing sunglasses, and the confidence
// level in the determination.
type Sunglasses struct {

	// Level of confidence in the determination.
	Confidence *float32

	// Boolean value that indicates whether the face is wearing sunglasses or not.
	Value bool
}

// Information about a technical cue segment. For more information, see
// SegmentDetection.
type TechnicalCueSegment struct {

	// The confidence that Amazon Rekognition Video has in the accuracy of the detected
	// segment.
	Confidence *float32

	// The type of the technical cue.
	Type TechnicalCueType
}

// The dataset used for testing. Optionally, if AutoCreate is set, Amazon
// Rekognition Custom Labels creates a testing dataset using an 80/20 split of the
// training dataset.
type TestingData struct {

	// The assets used for testing.
	Assets []Asset

	// If specified, Amazon Rekognition Custom Labels creates a testing dataset with an
	// 80/20 split of the training dataset.
	AutoCreate bool
}

// Sagemaker Groundtruth format manifest files for the input, output and validation
// datasets that are used and created during testing.
type TestingDataResult struct {

	// The testing dataset that was supplied for training.
	Input *TestingData

	// The subset of the dataset that was actually tested. Some images (assets) might
	// not be tested due to file formatting and other issues.
	Output *TestingData

	// The location of the data validation manifest. The data validation manifest is
	// created for the test dataset during model training.
	Validation *ValidationData
}

// Information about a word or line of text detected by DetectText. The
// DetectedText field contains the text that Amazon Rekognition detected in the
// image. Every word and line has an identifier (Id). Each word belongs to a line
// and has a parent identifier (ParentId) that identifies the line of text in which
// the word appears. The word Id is also an index for the word within a line of
// words. For more information, see Detecting Text in the Amazon Rekognition
// Developer Guide.
type TextDetection struct {

	// The confidence that Amazon Rekognition has in the accuracy of the detected text
	// and the accuracy of the geometry points around the detected text.
	Confidence *float32

	// The word or line of text recognized by Amazon Rekognition.
	DetectedText *string

	// The location of the detected text on the image. Includes an axis aligned coarse
	// bounding box surrounding the text and a finer grain polygon for more accurate
	// spatial information.
	Geometry *Geometry

	// The identifier for the detected text. The identifier is only unique for a single
	// call to DetectText.
	Id *int32

	// The Parent identifier for the detected text identified by the value of ID. If
	// the type of detected text is LINE, the value of ParentId is Null.
	ParentId *int32

	// The type of text that was detected.
	Type TextTypes
}

// Information about text detected in a video. Incudes the detected text, the time
// in milliseconds from the start of the video that the text was detected, and
// where it was detected on the screen.
type TextDetectionResult struct {

	// Details about text detected in a video.
	TextDetection *TextDetection

	// The time, in milliseconds from the start of the video, that the text was
	// detected.
	Timestamp int64
}

// The dataset used for training.
type TrainingData struct {

	// A Sagemaker GroundTruth manifest file that contains the training images
	// (assets).
	Assets []Asset
}

// Sagemaker Groundtruth format manifest files for the input, output and validation
// datasets that are used and created during testing.
type TrainingDataResult struct {

	// The training assets that you supplied for training.
	Input *TrainingData

	// The images (assets) that were actually trained by Amazon Rekognition Custom
	// Labels.
	Output *TrainingData

	// The location of the data validation manifest. The data validation manifest is
	// created for the training dataset during model training.
	Validation *ValidationData
}

// A face that IndexFaces detected, but didn't index. Use the Reasons response
// attribute to determine why a face wasn't indexed.
type UnindexedFace struct {

	// The structure that contains attributes of a face that IndexFacesdetected, but
	// didn't index.
	FaceDetail *FaceDetail

	// An array of reasons that specify why a face wasn't indexed.
	//
	// * EXTREME_POSE -
	// The face is at a pose that can't be detected. For example, the head is turned
	// too far away from the camera.
	//
	// * EXCEEDS_MAX_FACES - The number of faces
	// detected is already higher than that specified by the MaxFaces input parameter
	// for IndexFaces.
	//
	// * LOW_BRIGHTNESS - The image is too dark.
	//
	// * LOW_SHARPNESS -
	// The image is too blurry.
	//
	// * LOW_CONFIDENCE - The face was detected with a low
	// confidence.
	//
	// * SMALL_BOUNDING_BOX - The bounding box around the face is too
	// small.
	Reasons []Reason
}

// Contains the Amazon S3 bucket location of the validation data for a model
// training job. The validation data includes error information for individual JSON
// lines in the dataset.
//
// For more information, see Debugging a Failed Model
// Training in the Amazon Rekognition Custom Labels Developer Guide. You get the
// ValidationData object for the training dataset (TrainingDataResult) and the test
// dataset (TestingDataResult) by calling DescribeProjectVersions. The assets array
// contains a single Asset object. The GroundTruthManifest field of the Asset
// object contains the S3 bucket location of the validation data.
type ValidationData struct {

	// The assets that comprise the validation data.
	Assets []Asset
}

// Video file stored in an Amazon S3 bucket. Amazon Rekognition video start
// operations such as StartLabelDetection use Video to specify a video for
// analysis. The supported file formats are .mp4, .mov and .avi.
type Video struct {

	// The Amazon S3 bucket name and file name for the video.
	S3Object *S3Object
}

// Information about a video that Amazon Rekognition analyzed. Videometadata is
// returned in every page of paginated responses from a Amazon Rekognition video
// operation.
type VideoMetadata struct {

	// Type of compression used in the analyzed video.
	Codec *string

	// Length of the video in milliseconds.
	DurationMillis *int64

	// Format of the analyzed video. Possible values are MP4, MOV and AVI.
	Format *string

	// Vertical pixel dimension of the video.
	FrameHeight *int64

	// Number of frames per second in the video.
	FrameRate *float32

	// Horizontal pixel dimension of the video.
	FrameWidth *int64
}

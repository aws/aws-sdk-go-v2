// Code generated by private/model/cli/gen-api/main.go. DO NOT EDIT.

package types

import (
	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/internal/awsutil"
	"github.com/aws/aws-sdk-go-v2/service/rekognition/enums"
)

type IndexFacesInput struct {
	_ struct{} `type:"structure"`

	// The ID of an existing collection to which you want to add the faces that
	// are detected in the input images.
	//
	// CollectionId is a required field
	CollectionId *string `min:"1" type:"string" required:"true"`

	// An array of facial attributes that you want to be returned. This can be the
	// default list of attributes or all attributes. If you don't specify a value
	// for Attributes or if you specify ["DEFAULT"], the API returns the following
	// subset of facial attributes: BoundingBox, Confidence, Pose, Quality, and
	// Landmarks. If you provide ["ALL"], all facial attributes are returned, but
	// the operation takes longer to complete.
	//
	// If you provide both, ["ALL", "DEFAULT"], the service uses a logical AND operator
	// to determine which attributes to return (in this case, all attributes).
	DetectionAttributes []enums.Attribute `type:"list"`

	// The ID you want to assign to all the faces detected in the image.
	ExternalImageId *string `min:"1" type:"string"`

	// The input image as base64-encoded bytes or an S3 object. If you use the AWS
	// CLI to call Amazon Rekognition operations, passing base64-encoded image bytes
	// isn't supported.
	//
	// If you are using an AWS SDK to call Amazon Rekognition, you might not need
	// to base64-encode image bytes passed using the Bytes field. For more information,
	// see Images in the Amazon Rekognition developer guide.
	//
	// Image is a required field
	Image *Image `type:"structure" required:"true"`

	// The maximum number of faces to index. The value of MaxFaces must be greater
	// than or equal to 1. IndexFaces returns no more than 100 detected faces in
	// an image, even if you specify a larger value for MaxFaces.
	//
	// If IndexFaces detects more faces than the value of MaxFaces, the faces with
	// the lowest quality are filtered out first. If there are still more faces
	// than the value of MaxFaces, the faces with the smallest bounding boxes are
	// filtered out (up to the number that's needed to satisfy the value of MaxFaces).
	// Information about the unindexed faces is available in the UnindexedFaces
	// array.
	//
	// The faces that are returned by IndexFaces are sorted by the largest face
	// bounding box size to the smallest size, in descending order.
	//
	// MaxFaces can be used with a collection associated with any version of the
	// face model.
	MaxFaces *int64 `min:"1" type:"integer"`

	// A filter that specifies how much filtering is done to identify faces that
	// are detected with low quality. Filtered faces aren't indexed. If you specify
	// AUTO, filtering prioritizes the identification of faces that don’t meet
	// the required quality bar chosen by Amazon Rekognition. The quality bar is
	// based on a variety of common use cases. Low-quality detections can occur
	// for a number of reasons. Some examples are an object that's misidentified
	// as a face, a face that's too blurry, or a face with a pose that's too extreme
	// to use. If you specify NONE, no filtering is performed. The default value
	// is AUTO.
	//
	// To use quality filtering, the collection you are using must be associated
	// with version 3 of the face model.
	QualityFilter enums.QualityFilter `type:"string" enum:"true"`
}

// String returns the string representation
func (s IndexFacesInput) String() string {
	return awsutil.Prettify(s)
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *IndexFacesInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "IndexFacesInput"}

	if s.CollectionId == nil {
		invalidParams.Add(aws.NewErrParamRequired("CollectionId"))
	}
	if s.CollectionId != nil && len(*s.CollectionId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("CollectionId", 1))
	}
	if s.ExternalImageId != nil && len(*s.ExternalImageId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("ExternalImageId", 1))
	}

	if s.Image == nil {
		invalidParams.Add(aws.NewErrParamRequired("Image"))
	}
	if s.MaxFaces != nil && *s.MaxFaces < 1 {
		invalidParams.Add(aws.NewErrParamMinValue("MaxFaces", 1))
	}
	if s.Image != nil {
		if err := s.Image.Validate(); err != nil {
			invalidParams.AddNested("Image", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type IndexFacesOutput struct {
	_ struct{} `type:"structure"`

	// The version number of the face detection model that's associated with the
	// input collection (CollectionId).
	FaceModelVersion *string `type:"string"`

	// An array of faces detected and added to the collection. For more information,
	// see Searching Faces in a Collection in the Amazon Rekognition Developer Guide.
	FaceRecords []FaceRecord `type:"list"`

	// If your collection is associated with a face detection model that's later
	// than version 3.0, the value of OrientationCorrection is always null and no
	// orientation information is returned.
	//
	// If your collection is associated with a face detection model that's version
	// 3.0 or earlier, the following applies:
	//
	//    * If the input image is in .jpeg format, it might contain exchangeable
	//    image file format (Exif) metadata that includes the image's orientation.
	//    Amazon Rekognition uses this orientation information to perform image
	//    correction - the bounding box coordinates are translated to represent
	//    object locations after the orientation information in the Exif metadata
	//    is used to correct the image orientation. Images in .png format don't
	//    contain Exif metadata. The value of OrientationCorrection is null.
	//
	//    * If the image doesn't contain orientation information in its Exif metadata,
	//    Amazon Rekognition returns an estimated orientation (ROTATE_0, ROTATE_90,
	//    ROTATE_180, ROTATE_270). Amazon Rekognition doesn’t perform image correction
	//    for images. The bounding box coordinates aren't translated and represent
	//    the object locations before the image is rotated.
	//
	// Bounding box information is returned in the FaceRecords array. You can get
	// the version of the face detection model by calling DescribeCollection.
	OrientationCorrection enums.OrientationCorrection `type:"string" enum:"true"`

	// An array of faces that were detected in the image but weren't indexed. They
	// weren't indexed because the quality filter identified them as low quality,
	// or the MaxFaces request parameter filtered them out. To use the quality filter,
	// you specify the QualityFilter request parameter.
	UnindexedFaces []UnindexedFace `type:"list"`
}

// String returns the string representation
func (s IndexFacesOutput) String() string {
	return awsutil.Prettify(s)
}

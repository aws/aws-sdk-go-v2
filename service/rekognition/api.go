// Code generated by private/model/cli/gen-api/main.go. DO NOT EDIT.

package rekognition

import (
	"time"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/internal/awsutil"
)

const opCompareFaces = "CompareFaces"

// CompareFacesRequest is a API request type for the CompareFaces API operation.
type CompareFacesRequest struct {
	*aws.Request
	Input *CompareFacesInput
	Copy  func(*CompareFacesInput) CompareFacesRequest
}

// Send marshals and sends the CompareFaces API request.
func (r CompareFacesRequest) Send() (*CompareFacesOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*CompareFacesOutput), nil
}

// CompareFacesRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Compares a face in the source input image with each of the 100 largest faces
// detected in the target input image.
//
// If the source image contains multiple faces, the service detects the largest
// face and compares it with each face detected in the target image.
//
// You pass the input and target images either as base64-encoded image bytes
// or as references to images in an Amazon S3 bucket. If you use the AWS CLI
// to call Amazon Rekognition operations, passing image bytes isn't supported.
// The image must be formatted as a PNG or JPEG file.
//
// In response, the operation returns an array of face matches ordered by similarity
// score in descending order. For each face match, the response provides a bounding
// box of the face, facial landmarks, pose details (pitch, role, and yaw), quality
// (brightness and sharpness), and confidence value (indicating the level of
// confidence that the bounding box contains a face). The response also provides
// a similarity score, which indicates how closely the faces match.
//
// By default, only faces with a similarity score of greater than or equal to
// 80% are returned in the response. You can change this value by specifying
// the SimilarityThreshold parameter.
//
// CompareFaces also returns an array of faces that don't match the source image.
// For each face, it returns a bounding box, confidence value, landmarks, pose
// details, and quality. The response also returns information about the face
// in the source image, including the bounding box of the face and confidence
// value.
//
// If the image doesn't contain Exif metadata, CompareFaces returns orientation
// information for the source and target images. Use these values to display
// the images with the correct image orientation.
//
// If no faces are detected in the source or target images, CompareFaces returns
// an InvalidParameterException error.
//
// This is a stateless API operation. That is, data returned by this operation
// doesn't persist.
//
// For an example, see Comparing Faces in Images in the Amazon Rekognition Developer
// Guide.
//
// This operation requires permissions to perform the rekognition:CompareFaces
// action.
//
//    // Example sending a request using the CompareFacesRequest method.
//    req := client.CompareFacesRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) CompareFacesRequest(input *CompareFacesInput) CompareFacesRequest {
	op := &aws.Operation{
		Name:       opCompareFaces,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &CompareFacesInput{}
	}

	output := &CompareFacesOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return CompareFacesRequest{Request: req, Input: input, Copy: c.CompareFacesRequest}
}

const opCreateCollection = "CreateCollection"

// CreateCollectionRequest is a API request type for the CreateCollection API operation.
type CreateCollectionRequest struct {
	*aws.Request
	Input *CreateCollectionInput
	Copy  func(*CreateCollectionInput) CreateCollectionRequest
}

// Send marshals and sends the CreateCollection API request.
func (r CreateCollectionRequest) Send() (*CreateCollectionOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*CreateCollectionOutput), nil
}

// CreateCollectionRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Creates a collection in an AWS Region. You can add faces to the collection
// using the operation.
//
// For example, you might create collections, one for each of your application
// users. A user can then index faces using the IndexFaces operation and persist
// results in a specific collection. Then, a user can search the collection
// for faces in the user-specific container.
//
// When you create a collection, it is associated with the latest version of
// the face model version.
//
// Collection names are case-sensitive.
//
// This operation requires permissions to perform the rekognition:CreateCollection
// action.
//
//    // Example sending a request using the CreateCollectionRequest method.
//    req := client.CreateCollectionRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) CreateCollectionRequest(input *CreateCollectionInput) CreateCollectionRequest {
	op := &aws.Operation{
		Name:       opCreateCollection,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &CreateCollectionInput{}
	}

	output := &CreateCollectionOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return CreateCollectionRequest{Request: req, Input: input, Copy: c.CreateCollectionRequest}
}

const opCreateStreamProcessor = "CreateStreamProcessor"

// CreateStreamProcessorRequest is a API request type for the CreateStreamProcessor API operation.
type CreateStreamProcessorRequest struct {
	*aws.Request
	Input *CreateStreamProcessorInput
	Copy  func(*CreateStreamProcessorInput) CreateStreamProcessorRequest
}

// Send marshals and sends the CreateStreamProcessor API request.
func (r CreateStreamProcessorRequest) Send() (*CreateStreamProcessorOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*CreateStreamProcessorOutput), nil
}

// CreateStreamProcessorRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Creates an Amazon Rekognition stream processor that you can use to detect
// and recognize faces in a streaming video.
//
// Amazon Rekognition Video is a consumer of live video from Amazon Kinesis
// Video Streams. Amazon Rekognition Video sends analysis results to Amazon
// Kinesis Data Streams.
//
// You provide as input a Kinesis video stream (Input) and a Kinesis data stream
// (Output) stream. You also specify the face recognition criteria in Settings.
// For example, the collection containing faces that you want to recognize.
// Use Name to assign an identifier for the stream processor. You use Name to
// manage the stream processor. For example, you can start processing the source
// video by calling with the Name field.
//
// After you have finished analyzing a streaming video, use to stop processing.
// You can delete the stream processor by calling .
//
//    // Example sending a request using the CreateStreamProcessorRequest method.
//    req := client.CreateStreamProcessorRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) CreateStreamProcessorRequest(input *CreateStreamProcessorInput) CreateStreamProcessorRequest {
	op := &aws.Operation{
		Name:       opCreateStreamProcessor,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &CreateStreamProcessorInput{}
	}

	output := &CreateStreamProcessorOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return CreateStreamProcessorRequest{Request: req, Input: input, Copy: c.CreateStreamProcessorRequest}
}

const opDeleteCollection = "DeleteCollection"

// DeleteCollectionRequest is a API request type for the DeleteCollection API operation.
type DeleteCollectionRequest struct {
	*aws.Request
	Input *DeleteCollectionInput
	Copy  func(*DeleteCollectionInput) DeleteCollectionRequest
}

// Send marshals and sends the DeleteCollection API request.
func (r DeleteCollectionRequest) Send() (*DeleteCollectionOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*DeleteCollectionOutput), nil
}

// DeleteCollectionRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Deletes the specified collection. Note that this operation removes all faces
// in the collection. For an example, see delete-collection-procedure.
//
// This operation requires permissions to perform the rekognition:DeleteCollection
// action.
//
//    // Example sending a request using the DeleteCollectionRequest method.
//    req := client.DeleteCollectionRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) DeleteCollectionRequest(input *DeleteCollectionInput) DeleteCollectionRequest {
	op := &aws.Operation{
		Name:       opDeleteCollection,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &DeleteCollectionInput{}
	}

	output := &DeleteCollectionOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return DeleteCollectionRequest{Request: req, Input: input, Copy: c.DeleteCollectionRequest}
}

const opDeleteFaces = "DeleteFaces"

// DeleteFacesRequest is a API request type for the DeleteFaces API operation.
type DeleteFacesRequest struct {
	*aws.Request
	Input *DeleteFacesInput
	Copy  func(*DeleteFacesInput) DeleteFacesRequest
}

// Send marshals and sends the DeleteFaces API request.
func (r DeleteFacesRequest) Send() (*DeleteFacesOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*DeleteFacesOutput), nil
}

// DeleteFacesRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Deletes faces from a collection. You specify a collection ID and an array
// of face IDs to remove from the collection.
//
// This operation requires permissions to perform the rekognition:DeleteFaces
// action.
//
//    // Example sending a request using the DeleteFacesRequest method.
//    req := client.DeleteFacesRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) DeleteFacesRequest(input *DeleteFacesInput) DeleteFacesRequest {
	op := &aws.Operation{
		Name:       opDeleteFaces,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &DeleteFacesInput{}
	}

	output := &DeleteFacesOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return DeleteFacesRequest{Request: req, Input: input, Copy: c.DeleteFacesRequest}
}

const opDeleteStreamProcessor = "DeleteStreamProcessor"

// DeleteStreamProcessorRequest is a API request type for the DeleteStreamProcessor API operation.
type DeleteStreamProcessorRequest struct {
	*aws.Request
	Input *DeleteStreamProcessorInput
	Copy  func(*DeleteStreamProcessorInput) DeleteStreamProcessorRequest
}

// Send marshals and sends the DeleteStreamProcessor API request.
func (r DeleteStreamProcessorRequest) Send() (*DeleteStreamProcessorOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*DeleteStreamProcessorOutput), nil
}

// DeleteStreamProcessorRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Deletes the stream processor identified by Name. You assign the value for
// Name when you create the stream processor with . You might not be able to
// use the same name for a stream processor for a few seconds after calling
// DeleteStreamProcessor.
//
//    // Example sending a request using the DeleteStreamProcessorRequest method.
//    req := client.DeleteStreamProcessorRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) DeleteStreamProcessorRequest(input *DeleteStreamProcessorInput) DeleteStreamProcessorRequest {
	op := &aws.Operation{
		Name:       opDeleteStreamProcessor,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &DeleteStreamProcessorInput{}
	}

	output := &DeleteStreamProcessorOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return DeleteStreamProcessorRequest{Request: req, Input: input, Copy: c.DeleteStreamProcessorRequest}
}

const opDescribeCollection = "DescribeCollection"

// DescribeCollectionRequest is a API request type for the DescribeCollection API operation.
type DescribeCollectionRequest struct {
	*aws.Request
	Input *DescribeCollectionInput
	Copy  func(*DescribeCollectionInput) DescribeCollectionRequest
}

// Send marshals and sends the DescribeCollection API request.
func (r DescribeCollectionRequest) Send() (*DescribeCollectionOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*DescribeCollectionOutput), nil
}

// DescribeCollectionRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Describes the specified collection. You can use DescribeCollection to get
// information, such as the number of faces indexed into a collection and the
// version of the model used by the collection for face detection.
//
// For more information, see Describing a Collection in the Amazon Rekognition
// Developer Guide.
//
//    // Example sending a request using the DescribeCollectionRequest method.
//    req := client.DescribeCollectionRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) DescribeCollectionRequest(input *DescribeCollectionInput) DescribeCollectionRequest {
	op := &aws.Operation{
		Name:       opDescribeCollection,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &DescribeCollectionInput{}
	}

	output := &DescribeCollectionOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return DescribeCollectionRequest{Request: req, Input: input, Copy: c.DescribeCollectionRequest}
}

const opDescribeStreamProcessor = "DescribeStreamProcessor"

// DescribeStreamProcessorRequest is a API request type for the DescribeStreamProcessor API operation.
type DescribeStreamProcessorRequest struct {
	*aws.Request
	Input *DescribeStreamProcessorInput
	Copy  func(*DescribeStreamProcessorInput) DescribeStreamProcessorRequest
}

// Send marshals and sends the DescribeStreamProcessor API request.
func (r DescribeStreamProcessorRequest) Send() (*DescribeStreamProcessorOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*DescribeStreamProcessorOutput), nil
}

// DescribeStreamProcessorRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Provides information about a stream processor created by . You can get information
// about the input and output streams, the input parameters for the face recognition
// being performed, and the current status of the stream processor.
//
//    // Example sending a request using the DescribeStreamProcessorRequest method.
//    req := client.DescribeStreamProcessorRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) DescribeStreamProcessorRequest(input *DescribeStreamProcessorInput) DescribeStreamProcessorRequest {
	op := &aws.Operation{
		Name:       opDescribeStreamProcessor,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &DescribeStreamProcessorInput{}
	}

	output := &DescribeStreamProcessorOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return DescribeStreamProcessorRequest{Request: req, Input: input, Copy: c.DescribeStreamProcessorRequest}
}

const opDetectFaces = "DetectFaces"

// DetectFacesRequest is a API request type for the DetectFaces API operation.
type DetectFacesRequest struct {
	*aws.Request
	Input *DetectFacesInput
	Copy  func(*DetectFacesInput) DetectFacesRequest
}

// Send marshals and sends the DetectFaces API request.
func (r DetectFacesRequest) Send() (*DetectFacesOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*DetectFacesOutput), nil
}

// DetectFacesRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Detects faces within an image that is provided as input.
//
// DetectFaces detects the 100 largest faces in the image. For each face detected,
// the operation returns face details. These details include a bounding box
// of the face, a confidence value (that the bounding box contains a face),
// and a fixed set of attributes such as facial landmarks (for example, coordinates
// of eye and mouth), gender, presence of beard, sunglasses, and so on.
//
// The face-detection algorithm is most effective on frontal faces. For non-frontal
// or obscured faces, the algorithm might not detect the faces or might detect
// faces with lower confidence.
//
// You pass the input image either as base64-encoded image bytes or as a reference
// to an image in an Amazon S3 bucket. If you use the to call Amazon Rekognition
// operations, passing image bytes is not supported. The image must be either
// a PNG or JPEG formatted file.
//
// This is a stateless API operation. That is, the operation does not persist
// any data.
//
// This operation requires permissions to perform the rekognition:DetectFaces
// action.
//
//    // Example sending a request using the DetectFacesRequest method.
//    req := client.DetectFacesRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) DetectFacesRequest(input *DetectFacesInput) DetectFacesRequest {
	op := &aws.Operation{
		Name:       opDetectFaces,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &DetectFacesInput{}
	}

	output := &DetectFacesOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return DetectFacesRequest{Request: req, Input: input, Copy: c.DetectFacesRequest}
}

const opDetectLabels = "DetectLabels"

// DetectLabelsRequest is a API request type for the DetectLabels API operation.
type DetectLabelsRequest struct {
	*aws.Request
	Input *DetectLabelsInput
	Copy  func(*DetectLabelsInput) DetectLabelsRequest
}

// Send marshals and sends the DetectLabels API request.
func (r DetectLabelsRequest) Send() (*DetectLabelsOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*DetectLabelsOutput), nil
}

// DetectLabelsRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Detects instances of real-world entities within an image (JPEG or PNG) provided
// as input. This includes objects like flower, tree, and table; events like
// wedding, graduation, and birthday party; and concepts like landscape, evening,
// and nature.
//
// For an example, see Analyzing Images Stored in an Amazon S3 Bucket in the
// Amazon Rekognition Developer Guide.
//
// DetectLabels does not support the detection of activities. However, activity
// detection is supported for label detection in videos. For more information,
// see StartLabelDetection in the Amazon Rekognition Developer Guide.
//
// You pass the input image as base64-encoded image bytes or as a reference
// to an image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon
// Rekognition operations, passing image bytes is not supported. The image must
// be either a PNG or JPEG formatted file.
//
// For each object, scene, and concept the API returns one or more labels. Each
// label provides the object name, and the level of confidence that the image
// contains the object. For example, suppose the input image has a lighthouse,
// the sea, and a rock. The response includes all three labels, one for each
// object.
//
// {Name: lighthouse, Confidence: 98.4629}
//
// {Name: rock,Confidence: 79.2097}
//
// {Name: sea,Confidence: 75.061}
//
// In the preceding example, the operation returns one label for each of the
// three objects. The operation can also return multiple labels for the same
// object in the image. For example, if the input image shows a flower (for
// example, a tulip), the operation might return the following three labels.
//
// {Name: flower,Confidence: 99.0562}
//
// {Name: plant,Confidence: 99.0562}
//
// {Name: tulip,Confidence: 99.0562}
//
// In this example, the detection algorithm more precisely identifies the flower
// as a tulip.
//
// In response, the API returns an array of labels. In addition, the response
// also includes the orientation correction. Optionally, you can specify MinConfidence
// to control the confidence threshold for the labels returned. The default
// is 50%. You can also add the MaxLabels parameter to limit the number of labels
// returned.
//
// If the object detected is a person, the operation doesn't provide the same
// facial details that the DetectFaces operation provides.
//
// DetectLabels returns bounding boxes for instances of common object labels
// in an array of objects. An Instance object contains a object, for the location
// of the label on the image. It also includes the confidence by which the bounding
// box was detected.
//
// DetectLabels also returns a hierarchical taxonomy of detected labels. For
// example, a detected car might be assigned the label car. The label car has
// two parent labels: Vehicle (its parent) and Transportation (its grandparent).
// The response returns the entire list of ancestors for a label. Each ancestor
// is a unique label in the response. In the previous example, Car, Vehicle,
// and Transportation are returned as unique labels in the response.
//
// This is a stateless API operation. That is, the operation does not persist
// any data.
//
// This operation requires permissions to perform the rekognition:DetectLabels
// action.
//
//    // Example sending a request using the DetectLabelsRequest method.
//    req := client.DetectLabelsRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) DetectLabelsRequest(input *DetectLabelsInput) DetectLabelsRequest {
	op := &aws.Operation{
		Name:       opDetectLabels,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &DetectLabelsInput{}
	}

	output := &DetectLabelsOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return DetectLabelsRequest{Request: req, Input: input, Copy: c.DetectLabelsRequest}
}

const opDetectModerationLabels = "DetectModerationLabels"

// DetectModerationLabelsRequest is a API request type for the DetectModerationLabels API operation.
type DetectModerationLabelsRequest struct {
	*aws.Request
	Input *DetectModerationLabelsInput
	Copy  func(*DetectModerationLabelsInput) DetectModerationLabelsRequest
}

// Send marshals and sends the DetectModerationLabels API request.
func (r DetectModerationLabelsRequest) Send() (*DetectModerationLabelsOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*DetectModerationLabelsOutput), nil
}

// DetectModerationLabelsRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Detects explicit or suggestive adult content in a specified JPEG or PNG format
// image. Use DetectModerationLabels to moderate images depending on your requirements.
// For example, you might want to filter images that contain nudity, but not
// images containing suggestive content.
//
// To filter images, use the labels returned by DetectModerationLabels to determine
// which types of content are appropriate.
//
// For information about moderation labels, see Detecting Unsafe Content in
// the Amazon Rekognition Developer Guide.
//
// You pass the input image either as base64-encoded image bytes or as a reference
// to an image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon
// Rekognition operations, passing image bytes is not supported. The image must
// be either a PNG or JPEG formatted file.
//
//    // Example sending a request using the DetectModerationLabelsRequest method.
//    req := client.DetectModerationLabelsRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) DetectModerationLabelsRequest(input *DetectModerationLabelsInput) DetectModerationLabelsRequest {
	op := &aws.Operation{
		Name:       opDetectModerationLabels,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &DetectModerationLabelsInput{}
	}

	output := &DetectModerationLabelsOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return DetectModerationLabelsRequest{Request: req, Input: input, Copy: c.DetectModerationLabelsRequest}
}

const opDetectText = "DetectText"

// DetectTextRequest is a API request type for the DetectText API operation.
type DetectTextRequest struct {
	*aws.Request
	Input *DetectTextInput
	Copy  func(*DetectTextInput) DetectTextRequest
}

// Send marshals and sends the DetectText API request.
func (r DetectTextRequest) Send() (*DetectTextOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*DetectTextOutput), nil
}

// DetectTextRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Detects text in the input image and converts it into machine-readable text.
//
// Pass the input image as base64-encoded image bytes or as a reference to an
// image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon Rekognition
// operations, you must pass it as a reference to an image in an Amazon S3 bucket.
// For the AWS CLI, passing image bytes is not supported. The image must be
// either a .png or .jpeg formatted file.
//
// The DetectText operation returns text in an array of elements, TextDetections.
// Each TextDetection element provides information about a single word or line
// of text that was detected in the image.
//
// A word is one or more ISO basic latin script characters that are not separated
// by spaces. DetectText can detect up to 50 words in an image.
//
// A line is a string of equally spaced words. A line isn't necessarily a complete
// sentence. For example, a driver's license number is detected as a line. A
// line ends when there is no aligned text after it. Also, a line ends when
// there is a large gap between words, relative to the length of the words.
// This means, depending on the gap between words, Amazon Rekognition may detect
// multiple lines in text aligned in the same direction. Periods don't represent
// the end of a line. If a sentence spans multiple lines, the DetectText operation
// returns multiple lines.
//
// To determine whether a TextDetection element is a line of text or a word,
// use the TextDetection object Type field.
//
// To be detected, text must be within +/- 90 degrees orientation of the horizontal
// axis.
//
// For more information, see DetectText in the Amazon Rekognition Developer
// Guide.
//
//    // Example sending a request using the DetectTextRequest method.
//    req := client.DetectTextRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) DetectTextRequest(input *DetectTextInput) DetectTextRequest {
	op := &aws.Operation{
		Name:       opDetectText,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &DetectTextInput{}
	}

	output := &DetectTextOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return DetectTextRequest{Request: req, Input: input, Copy: c.DetectTextRequest}
}

const opGetCelebrityInfo = "GetCelebrityInfo"

// GetCelebrityInfoRequest is a API request type for the GetCelebrityInfo API operation.
type GetCelebrityInfoRequest struct {
	*aws.Request
	Input *GetCelebrityInfoInput
	Copy  func(*GetCelebrityInfoInput) GetCelebrityInfoRequest
}

// Send marshals and sends the GetCelebrityInfo API request.
func (r GetCelebrityInfoRequest) Send() (*GetCelebrityInfoOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*GetCelebrityInfoOutput), nil
}

// GetCelebrityInfoRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Gets the name and additional information about a celebrity based on his or
// her Amazon Rekognition ID. The additional information is returned as an array
// of URLs. If there is no additional information about the celebrity, this
// list is empty.
//
// For more information, see Recognizing Celebrities in an Image in the Amazon
// Rekognition Developer Guide.
//
// This operation requires permissions to perform the rekognition:GetCelebrityInfo
// action.
//
//    // Example sending a request using the GetCelebrityInfoRequest method.
//    req := client.GetCelebrityInfoRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) GetCelebrityInfoRequest(input *GetCelebrityInfoInput) GetCelebrityInfoRequest {
	op := &aws.Operation{
		Name:       opGetCelebrityInfo,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &GetCelebrityInfoInput{}
	}

	output := &GetCelebrityInfoOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return GetCelebrityInfoRequest{Request: req, Input: input, Copy: c.GetCelebrityInfoRequest}
}

const opGetCelebrityRecognition = "GetCelebrityRecognition"

// GetCelebrityRecognitionRequest is a API request type for the GetCelebrityRecognition API operation.
type GetCelebrityRecognitionRequest struct {
	*aws.Request
	Input *GetCelebrityRecognitionInput
	Copy  func(*GetCelebrityRecognitionInput) GetCelebrityRecognitionRequest
}

// Send marshals and sends the GetCelebrityRecognition API request.
func (r GetCelebrityRecognitionRequest) Send() (*GetCelebrityRecognitionOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*GetCelebrityRecognitionOutput), nil
}

// GetCelebrityRecognitionRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Gets the celebrity recognition results for a Amazon Rekognition Video analysis
// started by .
//
// Celebrity recognition in a video is an asynchronous operation. Analysis is
// started by a call to which returns a job identifier (JobId). When the celebrity
// recognition operation finishes, Amazon Rekognition Video publishes a completion
// status to the Amazon Simple Notification Service topic registered in the
// initial call to StartCelebrityRecognition. To get the results of the celebrity
// recognition analysis, first check that the status value published to the
// Amazon SNS topic is SUCCEEDED. If so, call GetCelebrityDetection and pass
// the job identifier (JobId) from the initial call to StartCelebrityDetection.
//
// For more information, see Working With Stored Videos in the Amazon Rekognition
// Developer Guide.
//
// GetCelebrityRecognition returns detected celebrities and the time(s) they
// are detected in an array (Celebrities) of objects. Each CelebrityRecognition
// contains information about the celebrity in a object and the time, Timestamp,
// the celebrity was detected.
//
// GetCelebrityRecognition only returns the default facial attributes (BoundingBox,
// Confidence, Landmarks, Pose, and Quality). The other facial attributes listed
// in the Face object of the following response syntax are not returned. For
// more information, see FaceDetail in the Amazon Rekognition Developer Guide.
//
// By default, the Celebrities array is sorted by time (milliseconds from the
// start of the video). You can also sort the array by celebrity by specifying
// the value ID in the SortBy input parameter.
//
// The CelebrityDetail object includes the celebrity identifer and additional
// information urls. If you don't store the additional information urls, you
// can get them later by calling with the celebrity identifer.
//
// No information is returned for faces not recognized as celebrities.
//
// Use MaxResults parameter to limit the number of labels returned. If there
// are more results than specified in MaxResults, the value of NextToken in
// the operation response contains a pagination token for getting the next set
// of results. To get the next page of results, call GetCelebrityDetection and
// populate the NextToken request parameter with the token value returned from
// the previous call to GetCelebrityRecognition.
//
//    // Example sending a request using the GetCelebrityRecognitionRequest method.
//    req := client.GetCelebrityRecognitionRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) GetCelebrityRecognitionRequest(input *GetCelebrityRecognitionInput) GetCelebrityRecognitionRequest {
	op := &aws.Operation{
		Name:       opGetCelebrityRecognition,
		HTTPMethod: "POST",
		HTTPPath:   "/",
		Paginator: &aws.Paginator{
			InputTokens:     []string{"NextToken"},
			OutputTokens:    []string{"NextToken"},
			LimitToken:      "MaxResults",
			TruncationToken: "",
		},
	}

	if input == nil {
		input = &GetCelebrityRecognitionInput{}
	}

	output := &GetCelebrityRecognitionOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return GetCelebrityRecognitionRequest{Request: req, Input: input, Copy: c.GetCelebrityRecognitionRequest}
}

// Paginate pages iterates over the pages of a GetCelebrityRecognitionRequest operation,
// calling the Next method for each page. Using the paginators Next
// method will depict whether or not there are more pages.
//
// Note: This operation can generate multiple requests to a service.
//
//    // Example iterating over at most 3 pages of a GetCelebrityRecognition operation.
//		req := client.GetCelebrityRecognitionRequest(input)
//		p := req.Paginate()
//		for p.Next() {
//			page := p.CurrentPage()
//		}
//
//		if err := p.Err(); err != nil {
//			return err
//		}
//
func (p *GetCelebrityRecognitionRequest) Paginate(opts ...aws.Option) GetCelebrityRecognitionPager {
	return GetCelebrityRecognitionPager{
		Pager: aws.Pager{
			NewRequest: func() (*aws.Request, error) {
				var inCpy *GetCelebrityRecognitionInput
				if p.Input != nil {
					tmp := *p.Input
					inCpy = &tmp
				}

				req := p.Copy(inCpy)
				req.ApplyOptions(opts...)

				return req.Request, nil
			},
		},
	}
}

// GetCelebrityRecognitionPager is used to paginate the request. This can be done by
// calling Next and CurrentPage.
type GetCelebrityRecognitionPager struct {
	aws.Pager
}

func (p *GetCelebrityRecognitionPager) CurrentPage() *GetCelebrityRecognitionOutput {
	return p.Pager.CurrentPage().(*GetCelebrityRecognitionOutput)
}

const opGetContentModeration = "GetContentModeration"

// GetContentModerationRequest is a API request type for the GetContentModeration API operation.
type GetContentModerationRequest struct {
	*aws.Request
	Input *GetContentModerationInput
	Copy  func(*GetContentModerationInput) GetContentModerationRequest
}

// Send marshals and sends the GetContentModeration API request.
func (r GetContentModerationRequest) Send() (*GetContentModerationOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*GetContentModerationOutput), nil
}

// GetContentModerationRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Gets the content moderation analysis results for a Amazon Rekognition Video
// analysis started by .
//
// Content moderation analysis of a video is an asynchronous operation. You
// start analysis by calling . which returns a job identifier (JobId). When
// analysis finishes, Amazon Rekognition Video publishes a completion status
// to the Amazon Simple Notification Service topic registered in the initial
// call to StartContentModeration. To get the results of the content moderation
// analysis, first check that the status value published to the Amazon SNS topic
// is SUCCEEDED. If so, call GetCelebrityDetection and pass the job identifier
// (JobId) from the initial call to StartCelebrityDetection.
//
// For more information, see Working with Stored Videos in the Amazon Rekognition
// Devlopers Guide.
//
// GetContentModeration returns detected content moderation labels, and the
// time they are detected, in an array, ModerationLabels, of objects.
//
// By default, the moderated labels are returned sorted by time, in milliseconds
// from the start of the video. You can also sort them by moderated label by
// specifying NAME for the SortBy input parameter.
//
// Since video analysis can return a large number of results, use the MaxResults
// parameter to limit the number of labels returned in a single call to GetContentModeration.
// If there are more results than specified in MaxResults, the value of NextToken
// in the operation response contains a pagination token for getting the next
// set of results. To get the next page of results, call GetContentModeration
// and populate the NextToken request parameter with the value of NextToken
// returned from the previous call to GetContentModeration.
//
// For more information, see Detecting Unsafe Content in the Amazon Rekognition
// Developer Guide.
//
//    // Example sending a request using the GetContentModerationRequest method.
//    req := client.GetContentModerationRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) GetContentModerationRequest(input *GetContentModerationInput) GetContentModerationRequest {
	op := &aws.Operation{
		Name:       opGetContentModeration,
		HTTPMethod: "POST",
		HTTPPath:   "/",
		Paginator: &aws.Paginator{
			InputTokens:     []string{"NextToken"},
			OutputTokens:    []string{"NextToken"},
			LimitToken:      "MaxResults",
			TruncationToken: "",
		},
	}

	if input == nil {
		input = &GetContentModerationInput{}
	}

	output := &GetContentModerationOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return GetContentModerationRequest{Request: req, Input: input, Copy: c.GetContentModerationRequest}
}

// Paginate pages iterates over the pages of a GetContentModerationRequest operation,
// calling the Next method for each page. Using the paginators Next
// method will depict whether or not there are more pages.
//
// Note: This operation can generate multiple requests to a service.
//
//    // Example iterating over at most 3 pages of a GetContentModeration operation.
//		req := client.GetContentModerationRequest(input)
//		p := req.Paginate()
//		for p.Next() {
//			page := p.CurrentPage()
//		}
//
//		if err := p.Err(); err != nil {
//			return err
//		}
//
func (p *GetContentModerationRequest) Paginate(opts ...aws.Option) GetContentModerationPager {
	return GetContentModerationPager{
		Pager: aws.Pager{
			NewRequest: func() (*aws.Request, error) {
				var inCpy *GetContentModerationInput
				if p.Input != nil {
					tmp := *p.Input
					inCpy = &tmp
				}

				req := p.Copy(inCpy)
				req.ApplyOptions(opts...)

				return req.Request, nil
			},
		},
	}
}

// GetContentModerationPager is used to paginate the request. This can be done by
// calling Next and CurrentPage.
type GetContentModerationPager struct {
	aws.Pager
}

func (p *GetContentModerationPager) CurrentPage() *GetContentModerationOutput {
	return p.Pager.CurrentPage().(*GetContentModerationOutput)
}

const opGetFaceDetection = "GetFaceDetection"

// GetFaceDetectionRequest is a API request type for the GetFaceDetection API operation.
type GetFaceDetectionRequest struct {
	*aws.Request
	Input *GetFaceDetectionInput
	Copy  func(*GetFaceDetectionInput) GetFaceDetectionRequest
}

// Send marshals and sends the GetFaceDetection API request.
func (r GetFaceDetectionRequest) Send() (*GetFaceDetectionOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*GetFaceDetectionOutput), nil
}

// GetFaceDetectionRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Gets face detection results for a Amazon Rekognition Video analysis started
// by .
//
// Face detection with Amazon Rekognition Video is an asynchronous operation.
// You start face detection by calling which returns a job identifier (JobId).
// When the face detection operation finishes, Amazon Rekognition Video publishes
// a completion status to the Amazon Simple Notification Service topic registered
// in the initial call to StartFaceDetection. To get the results of the face
// detection operation, first check that the status value published to the Amazon
// SNS topic is SUCCEEDED. If so, call and pass the job identifier (JobId) from
// the initial call to StartFaceDetection.
//
// GetFaceDetection returns an array of detected faces (Faces) sorted by the
// time the faces were detected.
//
// Use MaxResults parameter to limit the number of labels returned. If there
// are more results than specified in MaxResults, the value of NextToken in
// the operation response contains a pagination token for getting the next set
// of results. To get the next page of results, call GetFaceDetection and populate
// the NextToken request parameter with the token value returned from the previous
// call to GetFaceDetection.
//
//    // Example sending a request using the GetFaceDetectionRequest method.
//    req := client.GetFaceDetectionRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) GetFaceDetectionRequest(input *GetFaceDetectionInput) GetFaceDetectionRequest {
	op := &aws.Operation{
		Name:       opGetFaceDetection,
		HTTPMethod: "POST",
		HTTPPath:   "/",
		Paginator: &aws.Paginator{
			InputTokens:     []string{"NextToken"},
			OutputTokens:    []string{"NextToken"},
			LimitToken:      "MaxResults",
			TruncationToken: "",
		},
	}

	if input == nil {
		input = &GetFaceDetectionInput{}
	}

	output := &GetFaceDetectionOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return GetFaceDetectionRequest{Request: req, Input: input, Copy: c.GetFaceDetectionRequest}
}

// Paginate pages iterates over the pages of a GetFaceDetectionRequest operation,
// calling the Next method for each page. Using the paginators Next
// method will depict whether or not there are more pages.
//
// Note: This operation can generate multiple requests to a service.
//
//    // Example iterating over at most 3 pages of a GetFaceDetection operation.
//		req := client.GetFaceDetectionRequest(input)
//		p := req.Paginate()
//		for p.Next() {
//			page := p.CurrentPage()
//		}
//
//		if err := p.Err(); err != nil {
//			return err
//		}
//
func (p *GetFaceDetectionRequest) Paginate(opts ...aws.Option) GetFaceDetectionPager {
	return GetFaceDetectionPager{
		Pager: aws.Pager{
			NewRequest: func() (*aws.Request, error) {
				var inCpy *GetFaceDetectionInput
				if p.Input != nil {
					tmp := *p.Input
					inCpy = &tmp
				}

				req := p.Copy(inCpy)
				req.ApplyOptions(opts...)

				return req.Request, nil
			},
		},
	}
}

// GetFaceDetectionPager is used to paginate the request. This can be done by
// calling Next and CurrentPage.
type GetFaceDetectionPager struct {
	aws.Pager
}

func (p *GetFaceDetectionPager) CurrentPage() *GetFaceDetectionOutput {
	return p.Pager.CurrentPage().(*GetFaceDetectionOutput)
}

const opGetFaceSearch = "GetFaceSearch"

// GetFaceSearchRequest is a API request type for the GetFaceSearch API operation.
type GetFaceSearchRequest struct {
	*aws.Request
	Input *GetFaceSearchInput
	Copy  func(*GetFaceSearchInput) GetFaceSearchRequest
}

// Send marshals and sends the GetFaceSearch API request.
func (r GetFaceSearchRequest) Send() (*GetFaceSearchOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*GetFaceSearchOutput), nil
}

// GetFaceSearchRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Gets the face search results for Amazon Rekognition Video face search started
// by . The search returns faces in a collection that match the faces of persons
// detected in a video. It also includes the time(s) that faces are matched
// in the video.
//
// Face search in a video is an asynchronous operation. You start face search
// by calling to which returns a job identifier (JobId). When the search operation
// finishes, Amazon Rekognition Video publishes a completion status to the Amazon
// Simple Notification Service topic registered in the initial call to StartFaceSearch.
// To get the search results, first check that the status value published to
// the Amazon SNS topic is SUCCEEDED. If so, call GetFaceSearch and pass the
// job identifier (JobId) from the initial call to StartFaceSearch.
//
// For more information, see Searching Faces in a Collection in the Amazon Rekognition
// Developer Guide.
//
// The search results are retured in an array, Persons, of objects. EachPersonMatch
// element contains details about the matching faces in the input collection,
// person information (facial attributes, bounding boxes, and person identifer)
// for the matched person, and the time the person was matched in the video.
//
// GetFaceSearch only returns the default facial attributes (BoundingBox, Confidence,
// Landmarks, Pose, and Quality). The other facial attributes listed in the
// Face object of the following response syntax are not returned. For more information,
// see FaceDetail in the Amazon Rekognition Developer Guide.
//
// By default, the Persons array is sorted by the time, in milliseconds from
// the start of the video, persons are matched. You can also sort by persons
// by specifying INDEX for the SORTBY input parameter.
//
//    // Example sending a request using the GetFaceSearchRequest method.
//    req := client.GetFaceSearchRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) GetFaceSearchRequest(input *GetFaceSearchInput) GetFaceSearchRequest {
	op := &aws.Operation{
		Name:       opGetFaceSearch,
		HTTPMethod: "POST",
		HTTPPath:   "/",
		Paginator: &aws.Paginator{
			InputTokens:     []string{"NextToken"},
			OutputTokens:    []string{"NextToken"},
			LimitToken:      "MaxResults",
			TruncationToken: "",
		},
	}

	if input == nil {
		input = &GetFaceSearchInput{}
	}

	output := &GetFaceSearchOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return GetFaceSearchRequest{Request: req, Input: input, Copy: c.GetFaceSearchRequest}
}

// Paginate pages iterates over the pages of a GetFaceSearchRequest operation,
// calling the Next method for each page. Using the paginators Next
// method will depict whether or not there are more pages.
//
// Note: This operation can generate multiple requests to a service.
//
//    // Example iterating over at most 3 pages of a GetFaceSearch operation.
//		req := client.GetFaceSearchRequest(input)
//		p := req.Paginate()
//		for p.Next() {
//			page := p.CurrentPage()
//		}
//
//		if err := p.Err(); err != nil {
//			return err
//		}
//
func (p *GetFaceSearchRequest) Paginate(opts ...aws.Option) GetFaceSearchPager {
	return GetFaceSearchPager{
		Pager: aws.Pager{
			NewRequest: func() (*aws.Request, error) {
				var inCpy *GetFaceSearchInput
				if p.Input != nil {
					tmp := *p.Input
					inCpy = &tmp
				}

				req := p.Copy(inCpy)
				req.ApplyOptions(opts...)

				return req.Request, nil
			},
		},
	}
}

// GetFaceSearchPager is used to paginate the request. This can be done by
// calling Next and CurrentPage.
type GetFaceSearchPager struct {
	aws.Pager
}

func (p *GetFaceSearchPager) CurrentPage() *GetFaceSearchOutput {
	return p.Pager.CurrentPage().(*GetFaceSearchOutput)
}

const opGetLabelDetection = "GetLabelDetection"

// GetLabelDetectionRequest is a API request type for the GetLabelDetection API operation.
type GetLabelDetectionRequest struct {
	*aws.Request
	Input *GetLabelDetectionInput
	Copy  func(*GetLabelDetectionInput) GetLabelDetectionRequest
}

// Send marshals and sends the GetLabelDetection API request.
func (r GetLabelDetectionRequest) Send() (*GetLabelDetectionOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*GetLabelDetectionOutput), nil
}

// GetLabelDetectionRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Gets the label detection results of a Amazon Rekognition Video analysis started
// by .
//
// The label detection operation is started by a call to which returns a job
// identifier (JobId). When the label detection operation finishes, Amazon Rekognition
// publishes a completion status to the Amazon Simple Notification Service topic
// registered in the initial call to StartlabelDetection. To get the results
// of the label detection operation, first check that the status value published
// to the Amazon SNS topic is SUCCEEDED. If so, call and pass the job identifier
// (JobId) from the initial call to StartLabelDetection.
//
// GetLabelDetection returns an array of detected labels (Labels) sorted by
// the time the labels were detected. You can also sort by the label name by
// specifying NAME for the SortBy input parameter.
//
// The labels returned include the label name, the percentage confidence in
// the accuracy of the detected label, and the time the label was detected in
// the video.
//
// Use MaxResults parameter to limit the number of labels returned. If there
// are more results than specified in MaxResults, the value of NextToken in
// the operation response contains a pagination token for getting the next set
// of results. To get the next page of results, call GetlabelDetection and populate
// the NextToken request parameter with the token value returned from the previous
// call to GetLabelDetection.
//
// GetLabelDetection doesn't return a hierarchical taxonomy, or bounding box
// information, for detected labels. GetLabelDetection returns null for the
// Parents and Instances attributes of the object which is returned in the Labels
// array.
//
//    // Example sending a request using the GetLabelDetectionRequest method.
//    req := client.GetLabelDetectionRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) GetLabelDetectionRequest(input *GetLabelDetectionInput) GetLabelDetectionRequest {
	op := &aws.Operation{
		Name:       opGetLabelDetection,
		HTTPMethod: "POST",
		HTTPPath:   "/",
		Paginator: &aws.Paginator{
			InputTokens:     []string{"NextToken"},
			OutputTokens:    []string{"NextToken"},
			LimitToken:      "MaxResults",
			TruncationToken: "",
		},
	}

	if input == nil {
		input = &GetLabelDetectionInput{}
	}

	output := &GetLabelDetectionOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return GetLabelDetectionRequest{Request: req, Input: input, Copy: c.GetLabelDetectionRequest}
}

// Paginate pages iterates over the pages of a GetLabelDetectionRequest operation,
// calling the Next method for each page. Using the paginators Next
// method will depict whether or not there are more pages.
//
// Note: This operation can generate multiple requests to a service.
//
//    // Example iterating over at most 3 pages of a GetLabelDetection operation.
//		req := client.GetLabelDetectionRequest(input)
//		p := req.Paginate()
//		for p.Next() {
//			page := p.CurrentPage()
//		}
//
//		if err := p.Err(); err != nil {
//			return err
//		}
//
func (p *GetLabelDetectionRequest) Paginate(opts ...aws.Option) GetLabelDetectionPager {
	return GetLabelDetectionPager{
		Pager: aws.Pager{
			NewRequest: func() (*aws.Request, error) {
				var inCpy *GetLabelDetectionInput
				if p.Input != nil {
					tmp := *p.Input
					inCpy = &tmp
				}

				req := p.Copy(inCpy)
				req.ApplyOptions(opts...)

				return req.Request, nil
			},
		},
	}
}

// GetLabelDetectionPager is used to paginate the request. This can be done by
// calling Next and CurrentPage.
type GetLabelDetectionPager struct {
	aws.Pager
}

func (p *GetLabelDetectionPager) CurrentPage() *GetLabelDetectionOutput {
	return p.Pager.CurrentPage().(*GetLabelDetectionOutput)
}

const opGetPersonTracking = "GetPersonTracking"

// GetPersonTrackingRequest is a API request type for the GetPersonTracking API operation.
type GetPersonTrackingRequest struct {
	*aws.Request
	Input *GetPersonTrackingInput
	Copy  func(*GetPersonTrackingInput) GetPersonTrackingRequest
}

// Send marshals and sends the GetPersonTracking API request.
func (r GetPersonTrackingRequest) Send() (*GetPersonTrackingOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*GetPersonTrackingOutput), nil
}

// GetPersonTrackingRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Gets the path tracking results of a Amazon Rekognition Video analysis started
// by .
//
// The person path tracking operation is started by a call to StartPersonTracking
// which returns a job identifier (JobId). When the operation finishes, Amazon
// Rekognition Video publishes a completion status to the Amazon Simple Notification
// Service topic registered in the initial call to StartPersonTracking.
//
// To get the results of the person path tracking operation, first check that
// the status value published to the Amazon SNS topic is SUCCEEDED. If so, call
// and pass the job identifier (JobId) from the initial call to StartPersonTracking.
//
// GetPersonTracking returns an array, Persons, of tracked persons and the time(s)
// their paths were tracked in the video.
//
// GetPersonTracking only returns the default facial attributes (BoundingBox,
// Confidence, Landmarks, Pose, and Quality). The other facial attributes listed
// in the Face object of the following response syntax are not returned.
//
// For more information, see FaceDetail in the Amazon Rekognition Developer
// Guide.
//
// By default, the array is sorted by the time(s) a person's path is tracked
// in the video. You can sort by tracked persons by specifying INDEX for the
// SortBy input parameter.
//
// Use the MaxResults parameter to limit the number of items returned. If there
// are more results than specified in MaxResults, the value of NextToken in
// the operation response contains a pagination token for getting the next set
// of results. To get the next page of results, call GetPersonTracking and populate
// the NextToken request parameter with the token value returned from the previous
// call to GetPersonTracking.
//
//    // Example sending a request using the GetPersonTrackingRequest method.
//    req := client.GetPersonTrackingRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) GetPersonTrackingRequest(input *GetPersonTrackingInput) GetPersonTrackingRequest {
	op := &aws.Operation{
		Name:       opGetPersonTracking,
		HTTPMethod: "POST",
		HTTPPath:   "/",
		Paginator: &aws.Paginator{
			InputTokens:     []string{"NextToken"},
			OutputTokens:    []string{"NextToken"},
			LimitToken:      "MaxResults",
			TruncationToken: "",
		},
	}

	if input == nil {
		input = &GetPersonTrackingInput{}
	}

	output := &GetPersonTrackingOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return GetPersonTrackingRequest{Request: req, Input: input, Copy: c.GetPersonTrackingRequest}
}

// Paginate pages iterates over the pages of a GetPersonTrackingRequest operation,
// calling the Next method for each page. Using the paginators Next
// method will depict whether or not there are more pages.
//
// Note: This operation can generate multiple requests to a service.
//
//    // Example iterating over at most 3 pages of a GetPersonTracking operation.
//		req := client.GetPersonTrackingRequest(input)
//		p := req.Paginate()
//		for p.Next() {
//			page := p.CurrentPage()
//		}
//
//		if err := p.Err(); err != nil {
//			return err
//		}
//
func (p *GetPersonTrackingRequest) Paginate(opts ...aws.Option) GetPersonTrackingPager {
	return GetPersonTrackingPager{
		Pager: aws.Pager{
			NewRequest: func() (*aws.Request, error) {
				var inCpy *GetPersonTrackingInput
				if p.Input != nil {
					tmp := *p.Input
					inCpy = &tmp
				}

				req := p.Copy(inCpy)
				req.ApplyOptions(opts...)

				return req.Request, nil
			},
		},
	}
}

// GetPersonTrackingPager is used to paginate the request. This can be done by
// calling Next and CurrentPage.
type GetPersonTrackingPager struct {
	aws.Pager
}

func (p *GetPersonTrackingPager) CurrentPage() *GetPersonTrackingOutput {
	return p.Pager.CurrentPage().(*GetPersonTrackingOutput)
}

const opIndexFaces = "IndexFaces"

// IndexFacesRequest is a API request type for the IndexFaces API operation.
type IndexFacesRequest struct {
	*aws.Request
	Input *IndexFacesInput
	Copy  func(*IndexFacesInput) IndexFacesRequest
}

// Send marshals and sends the IndexFaces API request.
func (r IndexFacesRequest) Send() (*IndexFacesOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*IndexFacesOutput), nil
}

// IndexFacesRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Detects faces in the input image and adds them to the specified collection.
//
// Amazon Rekognition doesn't save the actual faces that are detected. Instead,
// the underlying detection algorithm first detects the faces in the input image.
// For each face, the algorithm extracts facial features into a feature vector,
// and stores it in the backend database. Amazon Rekognition uses feature vectors
// when it performs face match and search operations using the and operations.
//
// For more information, see Adding Faces to a Collection in the Amazon Rekognition
// Developer Guide.
//
// To get the number of faces in a collection, call .
//
// If you're using version 1.0 of the face detection model, IndexFaces indexes
// the 15 largest faces in the input image. Later versions of the face detection
// model index the 100 largest faces in the input image.
//
// If you're using version 4 or later of the face model, image orientation information
// is not returned in the OrientationCorrection field.
//
// To determine which version of the model you're using, call and supply the
// collection ID. You can also get the model version from the value of FaceModelVersion
// in the response from IndexFaces
//
// For more information, see Model Versioning in the Amazon Rekognition Developer
// Guide.
//
// If you provide the optional ExternalImageID for the input image you provided,
// Amazon Rekognition associates this ID with all faces that it detects. When
// you call the operation, the response returns the external ID. You can use
// this external image ID to create a client-side index to associate the faces
// with each image. You can then use the index to find all faces in an image.
//
// You can specify the maximum number of faces to index with the MaxFaces input
// parameter. This is useful when you want to index the largest faces in an
// image and don't want to index smaller faces, such as those belonging to people
// standing in the background.
//
// The QualityFilter input parameter allows you to filter out detected faces
// that dont meet the required quality bar chosen by Amazon Rekognition. The
// quality bar is based on a variety of common use cases. By default, IndexFaces
// filters detected faces. You can also explicitly filter detected faces by
// specifying AUTO for the value of QualityFilter. If you do not want to filter
// detected faces, specify NONE.
//
// To use quality filtering, you need a collection associated with version 3
// of the face model. To get the version of the face model associated with a
// collection, call .
//
// Information about faces detected in an image, but not indexed, is returned
// in an array of objects, UnindexedFaces. Faces aren't indexed for reasons
// such as:
//
//    * The number of faces detected exceeds the value of the MaxFaces request
//    parameter.
//
//    * The face is too small compared to the image dimensions.
//
//    * The face is too blurry.
//
//    * The image is too dark.
//
//    * The face has an extreme pose.
//
// In response, the IndexFaces operation returns an array of metadata for all
// detected faces, FaceRecords. This includes:
//
//    * The bounding box, BoundingBox, of the detected face.
//
//    * A confidence value, Confidence, which indicates the confidence that
//    the bounding box contains a face.
//
//    * A face ID, faceId, assigned by the service for each face that's detected
//    and stored.
//
//    * An image ID, ImageId, assigned by the service for the input image.
//
// If you request all facial attributes (by using the detectionAttributes parameter),
// Amazon Rekognition returns detailed facial attributes, such as facial landmarks
// (for example, location of eye and mouth) and other facial attributes like
// gender. If you provide the same image, specify the same collection, and use
// the same external ID in the IndexFaces operation, Amazon Rekognition doesn't
// save duplicate face metadata.
//
// The input image is passed either as base64-encoded image bytes, or as a reference
// to an image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon
// Rekognition operations, passing image bytes isn't supported. The image must
// be formatted as a PNG or JPEG file.
//
// This operation requires permissions to perform the rekognition:IndexFaces
//
//    // Example sending a request using the IndexFacesRequest method.
//    req := client.IndexFacesRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) IndexFacesRequest(input *IndexFacesInput) IndexFacesRequest {
	op := &aws.Operation{
		Name:       opIndexFaces,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &IndexFacesInput{}
	}

	output := &IndexFacesOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return IndexFacesRequest{Request: req, Input: input, Copy: c.IndexFacesRequest}
}

const opListCollections = "ListCollections"

// ListCollectionsRequest is a API request type for the ListCollections API operation.
type ListCollectionsRequest struct {
	*aws.Request
	Input *ListCollectionsInput
	Copy  func(*ListCollectionsInput) ListCollectionsRequest
}

// Send marshals and sends the ListCollections API request.
func (r ListCollectionsRequest) Send() (*ListCollectionsOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*ListCollectionsOutput), nil
}

// ListCollectionsRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Returns list of collection IDs in your account. If the result is truncated,
// the response also provides a NextToken that you can use in the subsequent
// request to fetch the next set of collection IDs.
//
// For an example, see Listing Collections in the Amazon Rekognition Developer
// Guide.
//
// This operation requires permissions to perform the rekognition:ListCollections
// action.
//
//    // Example sending a request using the ListCollectionsRequest method.
//    req := client.ListCollectionsRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) ListCollectionsRequest(input *ListCollectionsInput) ListCollectionsRequest {
	op := &aws.Operation{
		Name:       opListCollections,
		HTTPMethod: "POST",
		HTTPPath:   "/",
		Paginator: &aws.Paginator{
			InputTokens:     []string{"NextToken"},
			OutputTokens:    []string{"NextToken"},
			LimitToken:      "MaxResults",
			TruncationToken: "",
		},
	}

	if input == nil {
		input = &ListCollectionsInput{}
	}

	output := &ListCollectionsOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return ListCollectionsRequest{Request: req, Input: input, Copy: c.ListCollectionsRequest}
}

// Paginate pages iterates over the pages of a ListCollectionsRequest operation,
// calling the Next method for each page. Using the paginators Next
// method will depict whether or not there are more pages.
//
// Note: This operation can generate multiple requests to a service.
//
//    // Example iterating over at most 3 pages of a ListCollections operation.
//		req := client.ListCollectionsRequest(input)
//		p := req.Paginate()
//		for p.Next() {
//			page := p.CurrentPage()
//		}
//
//		if err := p.Err(); err != nil {
//			return err
//		}
//
func (p *ListCollectionsRequest) Paginate(opts ...aws.Option) ListCollectionsPager {
	return ListCollectionsPager{
		Pager: aws.Pager{
			NewRequest: func() (*aws.Request, error) {
				var inCpy *ListCollectionsInput
				if p.Input != nil {
					tmp := *p.Input
					inCpy = &tmp
				}

				req := p.Copy(inCpy)
				req.ApplyOptions(opts...)

				return req.Request, nil
			},
		},
	}
}

// ListCollectionsPager is used to paginate the request. This can be done by
// calling Next and CurrentPage.
type ListCollectionsPager struct {
	aws.Pager
}

func (p *ListCollectionsPager) CurrentPage() *ListCollectionsOutput {
	return p.Pager.CurrentPage().(*ListCollectionsOutput)
}

const opListFaces = "ListFaces"

// ListFacesRequest is a API request type for the ListFaces API operation.
type ListFacesRequest struct {
	*aws.Request
	Input *ListFacesInput
	Copy  func(*ListFacesInput) ListFacesRequest
}

// Send marshals and sends the ListFaces API request.
func (r ListFacesRequest) Send() (*ListFacesOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*ListFacesOutput), nil
}

// ListFacesRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Returns metadata for faces in the specified collection. This metadata includes
// information such as the bounding box coordinates, the confidence (that the
// bounding box contains a face), and face ID. For an example, see Listing Faces
// in a Collection in the Amazon Rekognition Developer Guide.
//
// This operation requires permissions to perform the rekognition:ListFaces
// action.
//
//    // Example sending a request using the ListFacesRequest method.
//    req := client.ListFacesRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) ListFacesRequest(input *ListFacesInput) ListFacesRequest {
	op := &aws.Operation{
		Name:       opListFaces,
		HTTPMethod: "POST",
		HTTPPath:   "/",
		Paginator: &aws.Paginator{
			InputTokens:     []string{"NextToken"},
			OutputTokens:    []string{"NextToken"},
			LimitToken:      "MaxResults",
			TruncationToken: "",
		},
	}

	if input == nil {
		input = &ListFacesInput{}
	}

	output := &ListFacesOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return ListFacesRequest{Request: req, Input: input, Copy: c.ListFacesRequest}
}

// Paginate pages iterates over the pages of a ListFacesRequest operation,
// calling the Next method for each page. Using the paginators Next
// method will depict whether or not there are more pages.
//
// Note: This operation can generate multiple requests to a service.
//
//    // Example iterating over at most 3 pages of a ListFaces operation.
//		req := client.ListFacesRequest(input)
//		p := req.Paginate()
//		for p.Next() {
//			page := p.CurrentPage()
//		}
//
//		if err := p.Err(); err != nil {
//			return err
//		}
//
func (p *ListFacesRequest) Paginate(opts ...aws.Option) ListFacesPager {
	return ListFacesPager{
		Pager: aws.Pager{
			NewRequest: func() (*aws.Request, error) {
				var inCpy *ListFacesInput
				if p.Input != nil {
					tmp := *p.Input
					inCpy = &tmp
				}

				req := p.Copy(inCpy)
				req.ApplyOptions(opts...)

				return req.Request, nil
			},
		},
	}
}

// ListFacesPager is used to paginate the request. This can be done by
// calling Next and CurrentPage.
type ListFacesPager struct {
	aws.Pager
}

func (p *ListFacesPager) CurrentPage() *ListFacesOutput {
	return p.Pager.CurrentPage().(*ListFacesOutput)
}

const opListStreamProcessors = "ListStreamProcessors"

// ListStreamProcessorsRequest is a API request type for the ListStreamProcessors API operation.
type ListStreamProcessorsRequest struct {
	*aws.Request
	Input *ListStreamProcessorsInput
	Copy  func(*ListStreamProcessorsInput) ListStreamProcessorsRequest
}

// Send marshals and sends the ListStreamProcessors API request.
func (r ListStreamProcessorsRequest) Send() (*ListStreamProcessorsOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*ListStreamProcessorsOutput), nil
}

// ListStreamProcessorsRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Gets a list of stream processors that you have created with .
//
//    // Example sending a request using the ListStreamProcessorsRequest method.
//    req := client.ListStreamProcessorsRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) ListStreamProcessorsRequest(input *ListStreamProcessorsInput) ListStreamProcessorsRequest {
	op := &aws.Operation{
		Name:       opListStreamProcessors,
		HTTPMethod: "POST",
		HTTPPath:   "/",
		Paginator: &aws.Paginator{
			InputTokens:     []string{"NextToken"},
			OutputTokens:    []string{"NextToken"},
			LimitToken:      "MaxResults",
			TruncationToken: "",
		},
	}

	if input == nil {
		input = &ListStreamProcessorsInput{}
	}

	output := &ListStreamProcessorsOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return ListStreamProcessorsRequest{Request: req, Input: input, Copy: c.ListStreamProcessorsRequest}
}

// Paginate pages iterates over the pages of a ListStreamProcessorsRequest operation,
// calling the Next method for each page. Using the paginators Next
// method will depict whether or not there are more pages.
//
// Note: This operation can generate multiple requests to a service.
//
//    // Example iterating over at most 3 pages of a ListStreamProcessors operation.
//		req := client.ListStreamProcessorsRequest(input)
//		p := req.Paginate()
//		for p.Next() {
//			page := p.CurrentPage()
//		}
//
//		if err := p.Err(); err != nil {
//			return err
//		}
//
func (p *ListStreamProcessorsRequest) Paginate(opts ...aws.Option) ListStreamProcessorsPager {
	return ListStreamProcessorsPager{
		Pager: aws.Pager{
			NewRequest: func() (*aws.Request, error) {
				var inCpy *ListStreamProcessorsInput
				if p.Input != nil {
					tmp := *p.Input
					inCpy = &tmp
				}

				req := p.Copy(inCpy)
				req.ApplyOptions(opts...)

				return req.Request, nil
			},
		},
	}
}

// ListStreamProcessorsPager is used to paginate the request. This can be done by
// calling Next and CurrentPage.
type ListStreamProcessorsPager struct {
	aws.Pager
}

func (p *ListStreamProcessorsPager) CurrentPage() *ListStreamProcessorsOutput {
	return p.Pager.CurrentPage().(*ListStreamProcessorsOutput)
}

const opRecognizeCelebrities = "RecognizeCelebrities"

// RecognizeCelebritiesRequest is a API request type for the RecognizeCelebrities API operation.
type RecognizeCelebritiesRequest struct {
	*aws.Request
	Input *RecognizeCelebritiesInput
	Copy  func(*RecognizeCelebritiesInput) RecognizeCelebritiesRequest
}

// Send marshals and sends the RecognizeCelebrities API request.
func (r RecognizeCelebritiesRequest) Send() (*RecognizeCelebritiesOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*RecognizeCelebritiesOutput), nil
}

// RecognizeCelebritiesRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Returns an array of celebrities recognized in the input image. For more information,
// see Recognizing Celebrities in the Amazon Rekognition Developer Guide.
//
// RecognizeCelebrities returns the 100 largest faces in the image. It lists
// recognized celebrities in the CelebrityFaces array and unrecognized faces
// in the UnrecognizedFaces array. RecognizeCelebrities doesn't return celebrities
// whose faces aren't among the largest 100 faces in the image.
//
// For each celebrity recognized, RecognizeCelebrities returns a Celebrity object.
// The Celebrity object contains the celebrity name, ID, URL links to additional
// information, match confidence, and a ComparedFace object that you can use
// to locate the celebrity's face on the image.
//
// Amazon Rekognition doesn't retain information about which images a celebrity
// has been recognized in. Your application must store this information and
// use the Celebrity ID property as a unique identifier for the celebrity. If
// you don't store the celebrity name or additional information URLs returned
// by RecognizeCelebrities, you will need the ID to identify the celebrity in
// a call to the operation.
//
// You pass the input image either as base64-encoded image bytes or as a reference
// to an image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon
// Rekognition operations, passing image bytes is not supported. The image must
// be either a PNG or JPEG formatted file.
//
// For an example, see Recognizing Celebrities in an Image in the Amazon Rekognition
// Developer Guide.
//
// This operation requires permissions to perform the rekognition:RecognizeCelebrities
// operation.
//
//    // Example sending a request using the RecognizeCelebritiesRequest method.
//    req := client.RecognizeCelebritiesRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) RecognizeCelebritiesRequest(input *RecognizeCelebritiesInput) RecognizeCelebritiesRequest {
	op := &aws.Operation{
		Name:       opRecognizeCelebrities,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &RecognizeCelebritiesInput{}
	}

	output := &RecognizeCelebritiesOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return RecognizeCelebritiesRequest{Request: req, Input: input, Copy: c.RecognizeCelebritiesRequest}
}

const opSearchFaces = "SearchFaces"

// SearchFacesRequest is a API request type for the SearchFaces API operation.
type SearchFacesRequest struct {
	*aws.Request
	Input *SearchFacesInput
	Copy  func(*SearchFacesInput) SearchFacesRequest
}

// Send marshals and sends the SearchFaces API request.
func (r SearchFacesRequest) Send() (*SearchFacesOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*SearchFacesOutput), nil
}

// SearchFacesRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// For a given input face ID, searches for matching faces in the collection
// the face belongs to. You get a face ID when you add a face to the collection
// using the IndexFaces operation. The operation compares the features of the
// input face with faces in the specified collection.
//
// You can also search faces without indexing faces by using the SearchFacesByImage
// operation.
//
// The operation response returns an array of faces that match, ordered by similarity
// score with the highest similarity first. More specifically, it is an array
// of metadata for each face match that is found. Along with the metadata, the
// response also includes a confidence value for each face match, indicating
// the confidence that the specific face matches the input face.
//
// For an example, see Searching for a Face Using Its Face ID in the Amazon
// Rekognition Developer Guide.
//
// This operation requires permissions to perform the rekognition:SearchFaces
// action.
//
//    // Example sending a request using the SearchFacesRequest method.
//    req := client.SearchFacesRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) SearchFacesRequest(input *SearchFacesInput) SearchFacesRequest {
	op := &aws.Operation{
		Name:       opSearchFaces,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &SearchFacesInput{}
	}

	output := &SearchFacesOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return SearchFacesRequest{Request: req, Input: input, Copy: c.SearchFacesRequest}
}

const opSearchFacesByImage = "SearchFacesByImage"

// SearchFacesByImageRequest is a API request type for the SearchFacesByImage API operation.
type SearchFacesByImageRequest struct {
	*aws.Request
	Input *SearchFacesByImageInput
	Copy  func(*SearchFacesByImageInput) SearchFacesByImageRequest
}

// Send marshals and sends the SearchFacesByImage API request.
func (r SearchFacesByImageRequest) Send() (*SearchFacesByImageOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*SearchFacesByImageOutput), nil
}

// SearchFacesByImageRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// For a given input image, first detects the largest face in the image, and
// then searches the specified collection for matching faces. The operation
// compares the features of the input face with faces in the specified collection.
//
// To search for all faces in an input image, you might first call the operation,
// and then use the face IDs returned in subsequent calls to the operation.
//
//  You can also call the DetectFaces operation and use the bounding boxes in
// the response to make face crops, which then you can pass in to the SearchFacesByImage
// operation.
//
// You pass the input image either as base64-encoded image bytes or as a reference
// to an image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon
// Rekognition operations, passing image bytes is not supported. The image must
// be either a PNG or JPEG formatted file.
//
// The response returns an array of faces that match, ordered by similarity
// score with the highest similarity first. More specifically, it is an array
// of metadata for each face match found. Along with the metadata, the response
// also includes a similarity indicating how similar the face is to the input
// face. In the response, the operation also returns the bounding box (and a
// confidence level that the bounding box contains a face) of the face that
// Amazon Rekognition used for the input image.
//
// For an example, Searching for a Face Using an Image in the Amazon Rekognition
// Developer Guide.
//
// This operation requires permissions to perform the rekognition:SearchFacesByImage
// action.
//
//    // Example sending a request using the SearchFacesByImageRequest method.
//    req := client.SearchFacesByImageRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) SearchFacesByImageRequest(input *SearchFacesByImageInput) SearchFacesByImageRequest {
	op := &aws.Operation{
		Name:       opSearchFacesByImage,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &SearchFacesByImageInput{}
	}

	output := &SearchFacesByImageOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return SearchFacesByImageRequest{Request: req, Input: input, Copy: c.SearchFacesByImageRequest}
}

const opStartCelebrityRecognition = "StartCelebrityRecognition"

// StartCelebrityRecognitionRequest is a API request type for the StartCelebrityRecognition API operation.
type StartCelebrityRecognitionRequest struct {
	*aws.Request
	Input *StartCelebrityRecognitionInput
	Copy  func(*StartCelebrityRecognitionInput) StartCelebrityRecognitionRequest
}

// Send marshals and sends the StartCelebrityRecognition API request.
func (r StartCelebrityRecognitionRequest) Send() (*StartCelebrityRecognitionOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*StartCelebrityRecognitionOutput), nil
}

// StartCelebrityRecognitionRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Starts asynchronous recognition of celebrities in a stored video.
//
// Amazon Rekognition Video can detect celebrities in a video must be stored
// in an Amazon S3 bucket. Use Video to specify the bucket name and the filename
// of the video. StartCelebrityRecognition returns a job identifier (JobId)
// which you use to get the results of the analysis. When celebrity recognition
// analysis is finished, Amazon Rekognition Video publishes a completion status
// to the Amazon Simple Notification Service topic that you specify in NotificationChannel.
// To get the results of the celebrity recognition analysis, first check that
// the status value published to the Amazon SNS topic is SUCCEEDED. If so, call
// and pass the job identifier (JobId) from the initial call to StartCelebrityRecognition.
//
// For more information, see Recognizing Celebrities in the Amazon Rekognition
// Developer Guide.
//
//    // Example sending a request using the StartCelebrityRecognitionRequest method.
//    req := client.StartCelebrityRecognitionRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) StartCelebrityRecognitionRequest(input *StartCelebrityRecognitionInput) StartCelebrityRecognitionRequest {
	op := &aws.Operation{
		Name:       opStartCelebrityRecognition,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &StartCelebrityRecognitionInput{}
	}

	output := &StartCelebrityRecognitionOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return StartCelebrityRecognitionRequest{Request: req, Input: input, Copy: c.StartCelebrityRecognitionRequest}
}

const opStartContentModeration = "StartContentModeration"

// StartContentModerationRequest is a API request type for the StartContentModeration API operation.
type StartContentModerationRequest struct {
	*aws.Request
	Input *StartContentModerationInput
	Copy  func(*StartContentModerationInput) StartContentModerationRequest
}

// Send marshals and sends the StartContentModeration API request.
func (r StartContentModerationRequest) Send() (*StartContentModerationOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*StartContentModerationOutput), nil
}

// StartContentModerationRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Starts asynchronous detection of explicit or suggestive adult content in
// a stored video.
//
// Amazon Rekognition Video can moderate content in a video stored in an Amazon
// S3 bucket. Use Video to specify the bucket name and the filename of the video.
// StartContentModeration returns a job identifier (JobId) which you use to
// get the results of the analysis. When content moderation analysis is finished,
// Amazon Rekognition Video publishes a completion status to the Amazon Simple
// Notification Service topic that you specify in NotificationChannel.
//
// To get the results of the content moderation analysis, first check that the
// status value published to the Amazon SNS topic is SUCCEEDED. If so, call
// and pass the job identifier (JobId) from the initial call to StartContentModeration.
//
// For more information, see Detecting Unsafe Content in the Amazon Rekognition
// Developer Guide.
//
//    // Example sending a request using the StartContentModerationRequest method.
//    req := client.StartContentModerationRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) StartContentModerationRequest(input *StartContentModerationInput) StartContentModerationRequest {
	op := &aws.Operation{
		Name:       opStartContentModeration,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &StartContentModerationInput{}
	}

	output := &StartContentModerationOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return StartContentModerationRequest{Request: req, Input: input, Copy: c.StartContentModerationRequest}
}

const opStartFaceDetection = "StartFaceDetection"

// StartFaceDetectionRequest is a API request type for the StartFaceDetection API operation.
type StartFaceDetectionRequest struct {
	*aws.Request
	Input *StartFaceDetectionInput
	Copy  func(*StartFaceDetectionInput) StartFaceDetectionRequest
}

// Send marshals and sends the StartFaceDetection API request.
func (r StartFaceDetectionRequest) Send() (*StartFaceDetectionOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*StartFaceDetectionOutput), nil
}

// StartFaceDetectionRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Starts asynchronous detection of faces in a stored video.
//
// Amazon Rekognition Video can detect faces in a video stored in an Amazon
// S3 bucket. Use Video to specify the bucket name and the filename of the video.
// StartFaceDetection returns a job identifier (JobId) that you use to get the
// results of the operation. When face detection is finished, Amazon Rekognition
// Video publishes a completion status to the Amazon Simple Notification Service
// topic that you specify in NotificationChannel. To get the results of the
// face detection operation, first check that the status value published to
// the Amazon SNS topic is SUCCEEDED. If so, call and pass the job identifier
// (JobId) from the initial call to StartFaceDetection.
//
// For more information, see Detecting Faces in a Stored Video in the Amazon
// Rekognition Developer Guide.
//
//    // Example sending a request using the StartFaceDetectionRequest method.
//    req := client.StartFaceDetectionRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) StartFaceDetectionRequest(input *StartFaceDetectionInput) StartFaceDetectionRequest {
	op := &aws.Operation{
		Name:       opStartFaceDetection,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &StartFaceDetectionInput{}
	}

	output := &StartFaceDetectionOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return StartFaceDetectionRequest{Request: req, Input: input, Copy: c.StartFaceDetectionRequest}
}

const opStartFaceSearch = "StartFaceSearch"

// StartFaceSearchRequest is a API request type for the StartFaceSearch API operation.
type StartFaceSearchRequest struct {
	*aws.Request
	Input *StartFaceSearchInput
	Copy  func(*StartFaceSearchInput) StartFaceSearchRequest
}

// Send marshals and sends the StartFaceSearch API request.
func (r StartFaceSearchRequest) Send() (*StartFaceSearchOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*StartFaceSearchOutput), nil
}

// StartFaceSearchRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Starts the asynchronous search for faces in a collection that match the faces
// of persons detected in a stored video.
//
// The video must be stored in an Amazon S3 bucket. Use Video to specify the
// bucket name and the filename of the video. StartFaceSearch returns a job
// identifier (JobId) which you use to get the search results once the search
// has completed. When searching is finished, Amazon Rekognition Video publishes
// a completion status to the Amazon Simple Notification Service topic that
// you specify in NotificationChannel. To get the search results, first check
// that the status value published to the Amazon SNS topic is SUCCEEDED. If
// so, call and pass the job identifier (JobId) from the initial call to StartFaceSearch.
// For more information, see procedure-person-search-videos.
//
//    // Example sending a request using the StartFaceSearchRequest method.
//    req := client.StartFaceSearchRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) StartFaceSearchRequest(input *StartFaceSearchInput) StartFaceSearchRequest {
	op := &aws.Operation{
		Name:       opStartFaceSearch,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &StartFaceSearchInput{}
	}

	output := &StartFaceSearchOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return StartFaceSearchRequest{Request: req, Input: input, Copy: c.StartFaceSearchRequest}
}

const opStartLabelDetection = "StartLabelDetection"

// StartLabelDetectionRequest is a API request type for the StartLabelDetection API operation.
type StartLabelDetectionRequest struct {
	*aws.Request
	Input *StartLabelDetectionInput
	Copy  func(*StartLabelDetectionInput) StartLabelDetectionRequest
}

// Send marshals and sends the StartLabelDetection API request.
func (r StartLabelDetectionRequest) Send() (*StartLabelDetectionOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*StartLabelDetectionOutput), nil
}

// StartLabelDetectionRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Starts asynchronous detection of labels in a stored video.
//
// Amazon Rekognition Video can detect labels in a video. Labels are instances
// of real-world entities. This includes objects like flower, tree, and table;
// events like wedding, graduation, and birthday party; concepts like landscape,
// evening, and nature; and activities like a person getting out of a car or
// a person skiing.
//
// The video must be stored in an Amazon S3 bucket. Use Video to specify the
// bucket name and the filename of the video. StartLabelDetection returns a
// job identifier (JobId) which you use to get the results of the operation.
// When label detection is finished, Amazon Rekognition Video publishes a completion
// status to the Amazon Simple Notification Service topic that you specify in
// NotificationChannel.
//
// To get the results of the label detection operation, first check that the
// status value published to the Amazon SNS topic is SUCCEEDED. If so, call
// and pass the job identifier (JobId) from the initial call to StartLabelDetection.
//
//    // Example sending a request using the StartLabelDetectionRequest method.
//    req := client.StartLabelDetectionRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) StartLabelDetectionRequest(input *StartLabelDetectionInput) StartLabelDetectionRequest {
	op := &aws.Operation{
		Name:       opStartLabelDetection,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &StartLabelDetectionInput{}
	}

	output := &StartLabelDetectionOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return StartLabelDetectionRequest{Request: req, Input: input, Copy: c.StartLabelDetectionRequest}
}

const opStartPersonTracking = "StartPersonTracking"

// StartPersonTrackingRequest is a API request type for the StartPersonTracking API operation.
type StartPersonTrackingRequest struct {
	*aws.Request
	Input *StartPersonTrackingInput
	Copy  func(*StartPersonTrackingInput) StartPersonTrackingRequest
}

// Send marshals and sends the StartPersonTracking API request.
func (r StartPersonTrackingRequest) Send() (*StartPersonTrackingOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*StartPersonTrackingOutput), nil
}

// StartPersonTrackingRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Starts the asynchronous tracking of a person's path in a stored video.
//
// Amazon Rekognition Video can track the path of people in a video stored in
// an Amazon S3 bucket. Use Video to specify the bucket name and the filename
// of the video. StartPersonTracking returns a job identifier (JobId) which
// you use to get the results of the operation. When label detection is finished,
// Amazon Rekognition publishes a completion status to the Amazon Simple Notification
// Service topic that you specify in NotificationChannel.
//
// To get the results of the person detection operation, first check that the
// status value published to the Amazon SNS topic is SUCCEEDED. If so, call
// and pass the job identifier (JobId) from the initial call to StartPersonTracking.
//
//    // Example sending a request using the StartPersonTrackingRequest method.
//    req := client.StartPersonTrackingRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) StartPersonTrackingRequest(input *StartPersonTrackingInput) StartPersonTrackingRequest {
	op := &aws.Operation{
		Name:       opStartPersonTracking,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &StartPersonTrackingInput{}
	}

	output := &StartPersonTrackingOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return StartPersonTrackingRequest{Request: req, Input: input, Copy: c.StartPersonTrackingRequest}
}

const opStartStreamProcessor = "StartStreamProcessor"

// StartStreamProcessorRequest is a API request type for the StartStreamProcessor API operation.
type StartStreamProcessorRequest struct {
	*aws.Request
	Input *StartStreamProcessorInput
	Copy  func(*StartStreamProcessorInput) StartStreamProcessorRequest
}

// Send marshals and sends the StartStreamProcessor API request.
func (r StartStreamProcessorRequest) Send() (*StartStreamProcessorOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*StartStreamProcessorOutput), nil
}

// StartStreamProcessorRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Starts processing a stream processor. You create a stream processor by calling
// . To tell StartStreamProcessor which stream processor to start, use the value
// of the Name field specified in the call to CreateStreamProcessor.
//
//    // Example sending a request using the StartStreamProcessorRequest method.
//    req := client.StartStreamProcessorRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) StartStreamProcessorRequest(input *StartStreamProcessorInput) StartStreamProcessorRequest {
	op := &aws.Operation{
		Name:       opStartStreamProcessor,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &StartStreamProcessorInput{}
	}

	output := &StartStreamProcessorOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return StartStreamProcessorRequest{Request: req, Input: input, Copy: c.StartStreamProcessorRequest}
}

const opStopStreamProcessor = "StopStreamProcessor"

// StopStreamProcessorRequest is a API request type for the StopStreamProcessor API operation.
type StopStreamProcessorRequest struct {
	*aws.Request
	Input *StopStreamProcessorInput
	Copy  func(*StopStreamProcessorInput) StopStreamProcessorRequest
}

// Send marshals and sends the StopStreamProcessor API request.
func (r StopStreamProcessorRequest) Send() (*StopStreamProcessorOutput, error) {
	err := r.Request.Send()
	if err != nil {
		return nil, err
	}

	return r.Request.Data.(*StopStreamProcessorOutput), nil
}

// StopStreamProcessorRequest returns a request value for making API operation for
// Amazon Rekognition.
//
// Stops a running stream processor that was created by .
//
//    // Example sending a request using the StopStreamProcessorRequest method.
//    req := client.StopStreamProcessorRequest(params)
//    resp, err := req.Send()
//    if err == nil {
//        fmt.Println(resp)
//    }
func (c *Rekognition) StopStreamProcessorRequest(input *StopStreamProcessorInput) StopStreamProcessorRequest {
	op := &aws.Operation{
		Name:       opStopStreamProcessor,
		HTTPMethod: "POST",
		HTTPPath:   "/",
	}

	if input == nil {
		input = &StopStreamProcessorInput{}
	}

	output := &StopStreamProcessorOutput{}
	req := c.newRequest(op, input, output)
	output.responseMetadata = aws.Response{Request: req}

	return StopStreamProcessorRequest{Request: req, Input: input, Copy: c.StopStreamProcessorRequest}
}

// Structure containing the estimated age range, in years, for a face.
//
// Amazon Rekognition estimates an age range for faces detected in the input
// image. Estimated age ranges can overlap. A face of a 5-year-old might have
// an estimated range of 4-6, while the face of a 6-year-old might have an estimated
// range of 4-8.
type AgeRange struct {
	_ struct{} `type:"structure"`

	// The highest estimated age.
	High *int64 `type:"integer"`

	// The lowest estimated age.
	Low *int64 `type:"integer"`
}

// String returns the string representation
func (s AgeRange) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s AgeRange) GoString() string {
	return s.String()
}

// Indicates whether or not the face has a beard, and the confidence level in
// the determination.
type Beard struct {
	_ struct{} `type:"structure"`

	// Level of confidence in the determination.
	Confidence *float64 `type:"float"`

	// Boolean value that indicates whether the face has beard or not.
	Value *bool `type:"boolean"`
}

// String returns the string representation
func (s Beard) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Beard) GoString() string {
	return s.String()
}

// Identifies the bounding box around the label, face, or text. The left (x-coordinate)
// and top (y-coordinate) are coordinates representing the top and left sides
// of the bounding box. Note that the upper-left corner of the image is the
// origin (0,0).
//
// The top and left values returned are ratios of the overall image size. For
// example, if the input image is 700x200 pixels, and the top-left coordinate
// of the bounding box is 350x50 pixels, the API returns a left value of 0.5
// (350/700) and a top value of 0.25 (50/200).
//
// The width and height values represent the dimensions of the bounding box
// as a ratio of the overall image dimension. For example, if the input image
// is 700x200 pixels, and the bounding box width is 70 pixels, the width returned
// is 0.1.
//
// The bounding box coordinates can have negative values. For example, if Amazon
// Rekognition is able to detect a face that is at the image edge and is only
// partially visible, the service can return coordinates that are outside the
// image bounds and, depending on the image edge, you might get negative values
// or values greater than 1 for the left or top values.
type BoundingBox struct {
	_ struct{} `type:"structure"`

	// Height of the bounding box as a ratio of the overall image height.
	Height *float64 `type:"float"`

	// Left coordinate of the bounding box as a ratio of overall image width.
	Left *float64 `type:"float"`

	// Top coordinate of the bounding box as a ratio of overall image height.
	Top *float64 `type:"float"`

	// Width of the bounding box as a ratio of the overall image width.
	Width *float64 `type:"float"`
}

// String returns the string representation
func (s BoundingBox) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s BoundingBox) GoString() string {
	return s.String()
}

// Provides information about a celebrity recognized by the operation.
type Celebrity struct {
	_ struct{} `type:"structure"`

	// Provides information about the celebrity's face, such as its location on
	// the image.
	Face *ComparedFace `type:"structure"`

	// A unique identifier for the celebrity.
	Id *string `type:"string"`

	// The confidence, in percentage, that Amazon Rekognition has that the recognized
	// face is the celebrity.
	MatchConfidence *float64 `type:"float"`

	// The name of the celebrity.
	Name *string `type:"string"`

	// An array of URLs pointing to additional information about the celebrity.
	// If there is no additional information about the celebrity, this list is empty.
	Urls []string `type:"list"`
}

// String returns the string representation
func (s Celebrity) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Celebrity) GoString() string {
	return s.String()
}

// Information about a recognized celebrity.
type CelebrityDetail struct {
	_ struct{} `type:"structure"`

	// Bounding box around the body of a celebrity.
	BoundingBox *BoundingBox `type:"structure"`

	// The confidence, in percentage, that Amazon Rekognition has that the recognized
	// face is the celebrity.
	Confidence *float64 `type:"float"`

	// Face details for the recognized celebrity.
	Face *FaceDetail `type:"structure"`

	// The unique identifier for the celebrity.
	Id *string `type:"string"`

	// The name of the celebrity.
	Name *string `type:"string"`

	// An array of URLs pointing to additional celebrity information.
	Urls []string `type:"list"`
}

// String returns the string representation
func (s CelebrityDetail) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s CelebrityDetail) GoString() string {
	return s.String()
}

// Information about a detected celebrity and the time the celebrity was detected
// in a stored video. For more information, see GetCelebrityRecognition in the
// Amazon Rekognition Developer Guide.
type CelebrityRecognition struct {
	_ struct{} `type:"structure"`

	// Information about a recognized celebrity.
	Celebrity *CelebrityDetail `type:"structure"`

	// The time, in milliseconds from the start of the video, that the celebrity
	// was recognized.
	Timestamp *int64 `type:"long"`
}

// String returns the string representation
func (s CelebrityRecognition) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s CelebrityRecognition) GoString() string {
	return s.String()
}

type CompareFacesInput struct {
	_ struct{} `type:"structure"`

	// The minimum level of confidence in the face matches that a match must meet
	// to be included in the FaceMatches array.
	SimilarityThreshold *float64 `type:"float"`

	// The input image as base64-encoded bytes or an S3 object. If you use the AWS
	// CLI to call Amazon Rekognition operations, passing base64-encoded image bytes
	// is not supported.
	//
	// SourceImage is a required field
	SourceImage *Image `type:"structure" required:"true"`

	// The target image as base64-encoded bytes or an S3 object. If you use the
	// AWS CLI to call Amazon Rekognition operations, passing base64-encoded image
	// bytes is not supported.
	//
	// TargetImage is a required field
	TargetImage *Image `type:"structure" required:"true"`
}

// String returns the string representation
func (s CompareFacesInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s CompareFacesInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *CompareFacesInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "CompareFacesInput"}

	if s.SourceImage == nil {
		invalidParams.Add(aws.NewErrParamRequired("SourceImage"))
	}

	if s.TargetImage == nil {
		invalidParams.Add(aws.NewErrParamRequired("TargetImage"))
	}
	if s.SourceImage != nil {
		if err := s.SourceImage.Validate(); err != nil {
			invalidParams.AddNested("SourceImage", err.(aws.ErrInvalidParams))
		}
	}
	if s.TargetImage != nil {
		if err := s.TargetImage.Validate(); err != nil {
			invalidParams.AddNested("TargetImage", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

// Provides information about a face in a target image that matches the source
// image face analyzed by CompareFaces. The Face property contains the bounding
// box of the face in the target image. The Similarity property is the confidence
// that the source image face matches the face in the bounding box.
type CompareFacesMatch struct {
	_ struct{} `type:"structure"`

	// Provides face metadata (bounding box and confidence that the bounding box
	// actually contains a face).
	Face *ComparedFace `type:"structure"`

	// Level of confidence that the faces match.
	Similarity *float64 `type:"float"`
}

// String returns the string representation
func (s CompareFacesMatch) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s CompareFacesMatch) GoString() string {
	return s.String()
}

type CompareFacesOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// An array of faces in the target image that match the source image face. Each
	// CompareFacesMatch object provides the bounding box, the confidence level
	// that the bounding box contains a face, and the similarity score for the face
	// in the bounding box and the face in the source image.
	FaceMatches []CompareFacesMatch `type:"list"`

	// The face in the source image that was used for comparison.
	SourceImageFace *ComparedSourceImageFace `type:"structure"`

	// The value of SourceImageOrientationCorrection is always null.
	//
	// If the input image is in .jpeg format, it might contain exchangeable image
	// file format (Exif) metadata that includes the image's orientation. Amazon
	// Rekognition uses this orientation information to perform image correction.
	// The bounding box coordinates are translated to represent object locations
	// after the orientation information in the Exif metadata is used to correct
	// the image orientation. Images in .png format don't contain Exif metadata.
	//
	// Amazon Rekognition doesnt perform image correction for images in .png format
	// and .jpeg images without orientation information in the image Exif metadata.
	// The bounding box coordinates aren't translated and represent the object locations
	// before the image is rotated.
	SourceImageOrientationCorrection OrientationCorrection `type:"string" enum:"true"`

	// The value of TargetImageOrientationCorrection is always null.
	//
	// If the input image is in .jpeg format, it might contain exchangeable image
	// file format (Exif) metadata that includes the image's orientation. Amazon
	// Rekognition uses this orientation information to perform image correction.
	// The bounding box coordinates are translated to represent object locations
	// after the orientation information in the Exif metadata is used to correct
	// the image orientation. Images in .png format don't contain Exif metadata.
	//
	// Amazon Rekognition doesnt perform image correction for images in .png format
	// and .jpeg images without orientation information in the image Exif metadata.
	// The bounding box coordinates aren't translated and represent the object locations
	// before the image is rotated.
	TargetImageOrientationCorrection OrientationCorrection `type:"string" enum:"true"`

	// An array of faces in the target image that did not match the source image
	// face.
	UnmatchedFaces []ComparedFace `type:"list"`
}

// String returns the string representation
func (s CompareFacesOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s CompareFacesOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s CompareFacesOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

// Provides face metadata for target image faces that are analyzed by CompareFaces
// and RecognizeCelebrities.
type ComparedFace struct {
	_ struct{} `type:"structure"`

	// Bounding box of the face.
	BoundingBox *BoundingBox `type:"structure"`

	// Level of confidence that what the bounding box contains is a face.
	Confidence *float64 `type:"float"`

	// An array of facial landmarks.
	Landmarks []Landmark `type:"list"`

	// Indicates the pose of the face as determined by its pitch, roll, and yaw.
	Pose *Pose `type:"structure"`

	// Identifies face image brightness and sharpness.
	Quality *ImageQuality `type:"structure"`
}

// String returns the string representation
func (s ComparedFace) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s ComparedFace) GoString() string {
	return s.String()
}

// Type that describes the face Amazon Rekognition chose to compare with the
// faces in the target. This contains a bounding box for the selected face and
// confidence level that the bounding box contains a face. Note that Amazon
// Rekognition selects the largest face in the source image for this comparison.
type ComparedSourceImageFace struct {
	_ struct{} `type:"structure"`

	// Bounding box of the face.
	BoundingBox *BoundingBox `type:"structure"`

	// Confidence level that the selected bounding box contains a face.
	Confidence *float64 `type:"float"`
}

// String returns the string representation
func (s ComparedSourceImageFace) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s ComparedSourceImageFace) GoString() string {
	return s.String()
}

// Information about a moderation label detection in a stored video.
type ContentModerationDetection struct {
	_ struct{} `type:"structure"`

	// The moderation label detected by in the stored video.
	ModerationLabel *ModerationLabel `type:"structure"`

	// Time, in milliseconds from the beginning of the video, that the moderation
	// label was detected.
	Timestamp *int64 `type:"long"`
}

// String returns the string representation
func (s ContentModerationDetection) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s ContentModerationDetection) GoString() string {
	return s.String()
}

type CreateCollectionInput struct {
	_ struct{} `type:"structure"`

	// ID for the collection that you are creating.
	//
	// CollectionId is a required field
	CollectionId *string `min:"1" type:"string" required:"true"`
}

// String returns the string representation
func (s CreateCollectionInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s CreateCollectionInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *CreateCollectionInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "CreateCollectionInput"}

	if s.CollectionId == nil {
		invalidParams.Add(aws.NewErrParamRequired("CollectionId"))
	}
	if s.CollectionId != nil && len(*s.CollectionId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("CollectionId", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type CreateCollectionOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// Amazon Resource Name (ARN) of the collection. You can use this to manage
	// permissions on your resources.
	CollectionArn *string `type:"string"`

	// Version number of the face detection model associated with the collection
	// you are creating.
	FaceModelVersion *string `type:"string"`

	// HTTP status code indicating the result of the operation.
	StatusCode *int64 `type:"integer"`
}

// String returns the string representation
func (s CreateCollectionOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s CreateCollectionOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s CreateCollectionOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type CreateStreamProcessorInput struct {
	_ struct{} `type:"structure"`

	// Kinesis video stream stream that provides the source streaming video. If
	// you are using the AWS CLI, the parameter name is StreamProcessorInput.
	//
	// Input is a required field
	Input *StreamProcessorInput `type:"structure" required:"true"`

	// An identifier you assign to the stream processor. You can use Name to manage
	// the stream processor. For example, you can get the current status of the
	// stream processor by calling . Name is idempotent.
	//
	// Name is a required field
	Name *string `min:"1" type:"string" required:"true"`

	// Kinesis data stream stream to which Amazon Rekognition Video puts the analysis
	// results. If you are using the AWS CLI, the parameter name is StreamProcessorOutput.
	//
	// Output is a required field
	Output *StreamProcessorOutput `type:"structure" required:"true"`

	// ARN of the IAM role that allows access to the stream processor.
	//
	// RoleArn is a required field
	RoleArn *string `type:"string" required:"true"`

	// Face recognition input parameters to be used by the stream processor. Includes
	// the collection to use for face recognition and the face attributes to detect.
	//
	// Settings is a required field
	Settings *StreamProcessorSettings `type:"structure" required:"true"`
}

// String returns the string representation
func (s CreateStreamProcessorInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s CreateStreamProcessorInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *CreateStreamProcessorInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "CreateStreamProcessorInput"}

	if s.Input == nil {
		invalidParams.Add(aws.NewErrParamRequired("Input"))
	}

	if s.Name == nil {
		invalidParams.Add(aws.NewErrParamRequired("Name"))
	}
	if s.Name != nil && len(*s.Name) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("Name", 1))
	}

	if s.Output == nil {
		invalidParams.Add(aws.NewErrParamRequired("Output"))
	}

	if s.RoleArn == nil {
		invalidParams.Add(aws.NewErrParamRequired("RoleArn"))
	}

	if s.Settings == nil {
		invalidParams.Add(aws.NewErrParamRequired("Settings"))
	}
	if s.Settings != nil {
		if err := s.Settings.Validate(); err != nil {
			invalidParams.AddNested("Settings", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type CreateStreamProcessorOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// ARN for the newly create stream processor.
	StreamProcessorArn *string `type:"string"`
}

// String returns the string representation
func (s CreateStreamProcessorOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s CreateStreamProcessorOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s CreateStreamProcessorOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type DeleteCollectionInput struct {
	_ struct{} `type:"structure"`

	// ID of the collection to delete.
	//
	// CollectionId is a required field
	CollectionId *string `min:"1" type:"string" required:"true"`
}

// String returns the string representation
func (s DeleteCollectionInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DeleteCollectionInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *DeleteCollectionInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "DeleteCollectionInput"}

	if s.CollectionId == nil {
		invalidParams.Add(aws.NewErrParamRequired("CollectionId"))
	}
	if s.CollectionId != nil && len(*s.CollectionId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("CollectionId", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type DeleteCollectionOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// HTTP status code that indicates the result of the operation.
	StatusCode *int64 `type:"integer"`
}

// String returns the string representation
func (s DeleteCollectionOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DeleteCollectionOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s DeleteCollectionOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type DeleteFacesInput struct {
	_ struct{} `type:"structure"`

	// Collection from which to remove the specific faces.
	//
	// CollectionId is a required field
	CollectionId *string `min:"1" type:"string" required:"true"`

	// An array of face IDs to delete.
	//
	// FaceIds is a required field
	FaceIds []string `min:"1" type:"list" required:"true"`
}

// String returns the string representation
func (s DeleteFacesInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DeleteFacesInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *DeleteFacesInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "DeleteFacesInput"}

	if s.CollectionId == nil {
		invalidParams.Add(aws.NewErrParamRequired("CollectionId"))
	}
	if s.CollectionId != nil && len(*s.CollectionId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("CollectionId", 1))
	}

	if s.FaceIds == nil {
		invalidParams.Add(aws.NewErrParamRequired("FaceIds"))
	}
	if s.FaceIds != nil && len(s.FaceIds) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("FaceIds", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type DeleteFacesOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// An array of strings (face IDs) of the faces that were deleted.
	DeletedFaces []string `min:"1" type:"list"`
}

// String returns the string representation
func (s DeleteFacesOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DeleteFacesOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s DeleteFacesOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type DeleteStreamProcessorInput struct {
	_ struct{} `type:"structure"`

	// The name of the stream processor you want to delete.
	//
	// Name is a required field
	Name *string `min:"1" type:"string" required:"true"`
}

// String returns the string representation
func (s DeleteStreamProcessorInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DeleteStreamProcessorInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *DeleteStreamProcessorInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "DeleteStreamProcessorInput"}

	if s.Name == nil {
		invalidParams.Add(aws.NewErrParamRequired("Name"))
	}
	if s.Name != nil && len(*s.Name) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("Name", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type DeleteStreamProcessorOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response
}

// String returns the string representation
func (s DeleteStreamProcessorOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DeleteStreamProcessorOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s DeleteStreamProcessorOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type DescribeCollectionInput struct {
	_ struct{} `type:"structure"`

	// The ID of the collection to describe.
	//
	// CollectionId is a required field
	CollectionId *string `min:"1" type:"string" required:"true"`
}

// String returns the string representation
func (s DescribeCollectionInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DescribeCollectionInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *DescribeCollectionInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "DescribeCollectionInput"}

	if s.CollectionId == nil {
		invalidParams.Add(aws.NewErrParamRequired("CollectionId"))
	}
	if s.CollectionId != nil && len(*s.CollectionId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("CollectionId", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type DescribeCollectionOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The Amazon Resource Name (ARN) of the collection.
	CollectionARN *string `type:"string"`

	// The number of milliseconds since the Unix epoch time until the creation of
	// the collection. The Unix epoch time is 00:00:00 Coordinated Universal Time
	// (UTC), Thursday, 1 January 1970.
	CreationTimestamp *time.Time `type:"timestamp" timestampFormat:"unix"`

	// The number of faces that are indexed into the collection. To index faces
	// into a collection, use .
	FaceCount *int64 `type:"long"`

	// The version of the face model that's used by the collection for face detection.
	//
	// For more information, see Model Versioning in the Amazon Rekognition Developer
	// Guide.
	FaceModelVersion *string `type:"string"`
}

// String returns the string representation
func (s DescribeCollectionOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DescribeCollectionOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s DescribeCollectionOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type DescribeStreamProcessorInput struct {
	_ struct{} `type:"structure"`

	// Name of the stream processor for which you want information.
	//
	// Name is a required field
	Name *string `min:"1" type:"string" required:"true"`
}

// String returns the string representation
func (s DescribeStreamProcessorInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DescribeStreamProcessorInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *DescribeStreamProcessorInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "DescribeStreamProcessorInput"}

	if s.Name == nil {
		invalidParams.Add(aws.NewErrParamRequired("Name"))
	}
	if s.Name != nil && len(*s.Name) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("Name", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type DescribeStreamProcessorOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// Date and time the stream processor was created
	CreationTimestamp *time.Time `type:"timestamp" timestampFormat:"unix"`

	// Kinesis video stream that provides the source streaming video.
	Input *StreamProcessorInput `type:"structure"`

	// The time, in Unix format, the stream processor was last updated. For example,
	// when the stream processor moves from a running state to a failed state, or
	// when the user starts or stops the stream processor.
	LastUpdateTimestamp *time.Time `type:"timestamp" timestampFormat:"unix"`

	// Name of the stream processor.
	Name *string `min:"1" type:"string"`

	// Kinesis data stream to which Amazon Rekognition Video puts the analysis results.
	Output *StreamProcessorOutput `type:"structure"`

	// ARN of the IAM role that allows access to the stream processor.
	RoleArn *string `type:"string"`

	// Face recognition input parameters that are being used by the stream processor.
	// Includes the collection to use for face recognition and the face attributes
	// to detect.
	Settings *StreamProcessorSettings `type:"structure"`

	// Current status of the stream processor.
	Status StreamProcessorStatus `type:"string" enum:"true"`

	// Detailed status message about the stream processor.
	StatusMessage *string `type:"string"`

	// ARN of the stream processor.
	StreamProcessorArn *string `type:"string"`
}

// String returns the string representation
func (s DescribeStreamProcessorOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DescribeStreamProcessorOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s DescribeStreamProcessorOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type DetectFacesInput struct {
	_ struct{} `type:"structure"`

	// An array of facial attributes you want to be returned. This can be the default
	// list of attributes or all attributes. If you don't specify a value for Attributes
	// or if you specify ["DEFAULT"], the API returns the following subset of facial
	// attributes: BoundingBox, Confidence, Pose, Quality, and Landmarks. If you
	// provide ["ALL"], all facial attributes are returned, but the operation takes
	// longer to complete.
	//
	// If you provide both, ["ALL", "DEFAULT"], the service uses a logical AND operator
	// to determine which attributes to return (in this case, all attributes).
	Attributes []Attribute `type:"list"`

	// The input image as base64-encoded bytes or an S3 object. If you use the AWS
	// CLI to call Amazon Rekognition operations, passing base64-encoded image bytes
	// is not supported.
	//
	// Image is a required field
	Image *Image `type:"structure" required:"true"`
}

// String returns the string representation
func (s DetectFacesInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DetectFacesInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *DetectFacesInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "DetectFacesInput"}

	if s.Image == nil {
		invalidParams.Add(aws.NewErrParamRequired("Image"))
	}
	if s.Image != nil {
		if err := s.Image.Validate(); err != nil {
			invalidParams.AddNested("Image", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type DetectFacesOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// Details of each face found in the image.
	FaceDetails []FaceDetail `type:"list"`

	// The value of OrientationCorrection is always null.
	//
	// If the input image is in .jpeg format, it might contain exchangeable image
	// file format (Exif) metadata that includes the image's orientation. Amazon
	// Rekognition uses this orientation information to perform image correction.
	// The bounding box coordinates are translated to represent object locations
	// after the orientation information in the Exif metadata is used to correct
	// the image orientation. Images in .png format don't contain Exif metadata.
	//
	// Amazon Rekognition doesnt perform image correction for images in .png format
	// and .jpeg images without orientation information in the image Exif metadata.
	// The bounding box coordinates aren't translated and represent the object locations
	// before the image is rotated.
	OrientationCorrection OrientationCorrection `type:"string" enum:"true"`
}

// String returns the string representation
func (s DetectFacesOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DetectFacesOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s DetectFacesOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type DetectLabelsInput struct {
	_ struct{} `type:"structure"`

	// The input image as base64-encoded bytes or an S3 object. If you use the AWS
	// CLI to call Amazon Rekognition operations, passing base64-encoded image bytes
	// is not supported.
	//
	// Image is a required field
	Image *Image `type:"structure" required:"true"`

	// Maximum number of labels you want the service to return in the response.
	// The service returns the specified number of highest confidence labels.
	MaxLabels *int64 `type:"integer"`

	// Specifies the minimum confidence level for the labels to return. Amazon Rekognition
	// doesn't return any labels with confidence lower than this specified value.
	//
	// If MinConfidence is not specified, the operation returns labels with a confidence
	// values greater than or equal to 50 percent.
	MinConfidence *float64 `type:"float"`
}

// String returns the string representation
func (s DetectLabelsInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DetectLabelsInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *DetectLabelsInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "DetectLabelsInput"}

	if s.Image == nil {
		invalidParams.Add(aws.NewErrParamRequired("Image"))
	}
	if s.Image != nil {
		if err := s.Image.Validate(); err != nil {
			invalidParams.AddNested("Image", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type DetectLabelsOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// Version number of the label detection model that was used to detect labels.
	LabelModelVersion *string `type:"string"`

	// An array of labels for the real-world objects detected.
	Labels []Label `type:"list"`

	// The value of OrientationCorrection is always null.
	//
	// If the input image is in .jpeg format, it might contain exchangeable image
	// file format (Exif) metadata that includes the image's orientation. Amazon
	// Rekognition uses this orientation information to perform image correction.
	// The bounding box coordinates are translated to represent object locations
	// after the orientation information in the Exif metadata is used to correct
	// the image orientation. Images in .png format don't contain Exif metadata.
	//
	// Amazon Rekognition doesnt perform image correction for images in .png format
	// and .jpeg images without orientation information in the image Exif metadata.
	// The bounding box coordinates aren't translated and represent the object locations
	// before the image is rotated.
	OrientationCorrection OrientationCorrection `type:"string" enum:"true"`
}

// String returns the string representation
func (s DetectLabelsOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DetectLabelsOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s DetectLabelsOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type DetectModerationLabelsInput struct {
	_ struct{} `type:"structure"`

	// The input image as base64-encoded bytes or an S3 object. If you use the AWS
	// CLI to call Amazon Rekognition operations, passing base64-encoded image bytes
	// is not supported.
	//
	// Image is a required field
	Image *Image `type:"structure" required:"true"`

	// Specifies the minimum confidence level for the labels to return. Amazon Rekognition
	// doesn't return any labels with a confidence level lower than this specified
	// value.
	//
	// If you don't specify MinConfidence, the operation returns labels with confidence
	// values greater than or equal to 50 percent.
	MinConfidence *float64 `type:"float"`
}

// String returns the string representation
func (s DetectModerationLabelsInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DetectModerationLabelsInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *DetectModerationLabelsInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "DetectModerationLabelsInput"}

	if s.Image == nil {
		invalidParams.Add(aws.NewErrParamRequired("Image"))
	}
	if s.Image != nil {
		if err := s.Image.Validate(); err != nil {
			invalidParams.AddNested("Image", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type DetectModerationLabelsOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// Array of detected Moderation labels and the time, in millseconds from the
	// start of the video, they were detected.
	ModerationLabels []ModerationLabel `type:"list"`
}

// String returns the string representation
func (s DetectModerationLabelsOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DetectModerationLabelsOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s DetectModerationLabelsOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type DetectTextInput struct {
	_ struct{} `type:"structure"`

	// The input image as base64-encoded bytes or an Amazon S3 object. If you use
	// the AWS CLI to call Amazon Rekognition operations, you can't pass image bytes.
	//
	// Image is a required field
	Image *Image `type:"structure" required:"true"`
}

// String returns the string representation
func (s DetectTextInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DetectTextInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *DetectTextInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "DetectTextInput"}

	if s.Image == nil {
		invalidParams.Add(aws.NewErrParamRequired("Image"))
	}
	if s.Image != nil {
		if err := s.Image.Validate(); err != nil {
			invalidParams.AddNested("Image", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type DetectTextOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// An array of text that was detected in the input image.
	TextDetections []TextDetection `type:"list"`
}

// String returns the string representation
func (s DetectTextOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s DetectTextOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s DetectTextOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

// The emotions detected on the face, and the confidence level in the determination.
// For example, HAPPY, SAD, and ANGRY.
type Emotion struct {
	_ struct{} `type:"structure"`

	// Level of confidence in the determination.
	Confidence *float64 `type:"float"`

	// Type of emotion detected.
	Type EmotionName `type:"string" enum:"true"`
}

// String returns the string representation
func (s Emotion) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Emotion) GoString() string {
	return s.String()
}

// Indicates whether or not the eyes on the face are open, and the confidence
// level in the determination.
type EyeOpen struct {
	_ struct{} `type:"structure"`

	// Level of confidence in the determination.
	Confidence *float64 `type:"float"`

	// Boolean value that indicates whether the eyes on the face are open.
	Value *bool `type:"boolean"`
}

// String returns the string representation
func (s EyeOpen) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s EyeOpen) GoString() string {
	return s.String()
}

// Indicates whether or not the face is wearing eye glasses, and the confidence
// level in the determination.
type Eyeglasses struct {
	_ struct{} `type:"structure"`

	// Level of confidence in the determination.
	Confidence *float64 `type:"float"`

	// Boolean value that indicates whether the face is wearing eye glasses or not.
	Value *bool `type:"boolean"`
}

// String returns the string representation
func (s Eyeglasses) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Eyeglasses) GoString() string {
	return s.String()
}

// Describes the face properties such as the bounding box, face ID, image ID
// of the input image, and external image ID that you assigned.
type Face struct {
	_ struct{} `type:"structure"`

	// Bounding box of the face.
	BoundingBox *BoundingBox `type:"structure"`

	// Confidence level that the bounding box contains a face (and not a different
	// object such as a tree).
	Confidence *float64 `type:"float"`

	// Identifier that you assign to all the faces in the input image.
	ExternalImageId *string `min:"1" type:"string"`

	// Unique identifier that Amazon Rekognition assigns to the face.
	FaceId *string `type:"string"`

	// Unique identifier that Amazon Rekognition assigns to the input image.
	ImageId *string `type:"string"`
}

// String returns the string representation
func (s Face) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Face) GoString() string {
	return s.String()
}

// Structure containing attributes of the face that the algorithm detected.
//
// A FaceDetail object contains either the default facial attributes or all
// facial attributes. The default attributes are BoundingBox, Confidence, Landmarks,
// Pose, and Quality.
//
// is the only Amazon Rekognition Video stored video operation that can return
// a FaceDetail object with all attributes. To specify which attributes to return,
// use the FaceAttributes input parameter for . The following Amazon Rekognition
// Video operations return only the default attributes. The corresponding Start
// operations don't have a FaceAttributes input parameter.
//
//    * GetCelebrityRecognition
//
//    * GetPersonTracking
//
//    * GetFaceSearch
//
// The Amazon Rekognition Image and operations can return all facial attributes.
// To specify which attributes to return, use the Attributes input parameter
// for DetectFaces. For IndexFaces, use the DetectAttributes input parameter.
type FaceDetail struct {
	_ struct{} `type:"structure"`

	// The estimated age range, in years, for the face. Low represents the lowest
	// estimated age and High represents the highest estimated age.
	AgeRange *AgeRange `type:"structure"`

	// Indicates whether or not the face has a beard, and the confidence level in
	// the determination.
	Beard *Beard `type:"structure"`

	// Bounding box of the face. Default attribute.
	BoundingBox *BoundingBox `type:"structure"`

	// Confidence level that the bounding box contains a face (and not a different
	// object such as a tree). Default attribute.
	Confidence *float64 `type:"float"`

	// The emotions detected on the face, and the confidence level in the determination.
	// For example, HAPPY, SAD, and ANGRY.
	Emotions []Emotion `type:"list"`

	// Indicates whether or not the face is wearing eye glasses, and the confidence
	// level in the determination.
	Eyeglasses *Eyeglasses `type:"structure"`

	// Indicates whether or not the eyes on the face are open, and the confidence
	// level in the determination.
	EyesOpen *EyeOpen `type:"structure"`

	// Gender of the face and the confidence level in the determination.
	Gender *Gender `type:"structure"`

	// Indicates the location of landmarks on the face. Default attribute.
	Landmarks []Landmark `type:"list"`

	// Indicates whether or not the mouth on the face is open, and the confidence
	// level in the determination.
	MouthOpen *MouthOpen `type:"structure"`

	// Indicates whether or not the face has a mustache, and the confidence level
	// in the determination.
	Mustache *Mustache `type:"structure"`

	// Indicates the pose of the face as determined by its pitch, roll, and yaw.
	// Default attribute.
	Pose *Pose `type:"structure"`

	// Identifies image brightness and sharpness. Default attribute.
	Quality *ImageQuality `type:"structure"`

	// Indicates whether or not the face is smiling, and the confidence level in
	// the determination.
	Smile *Smile `type:"structure"`

	// Indicates whether or not the face is wearing sunglasses, and the confidence
	// level in the determination.
	Sunglasses *Sunglasses `type:"structure"`
}

// String returns the string representation
func (s FaceDetail) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s FaceDetail) GoString() string {
	return s.String()
}

// Information about a face detected in a video analysis request and the time
// the face was detected in the video.
type FaceDetection struct {
	_ struct{} `type:"structure"`

	// The face properties for the detected face.
	Face *FaceDetail `type:"structure"`

	// Time, in milliseconds from the start of the video, that the face was detected.
	Timestamp *int64 `type:"long"`
}

// String returns the string representation
func (s FaceDetection) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s FaceDetection) GoString() string {
	return s.String()
}

// Provides face metadata. In addition, it also provides the confidence in the
// match of this face with the input face.
type FaceMatch struct {
	_ struct{} `type:"structure"`

	// Describes the face properties such as the bounding box, face ID, image ID
	// of the source image, and external image ID that you assigned.
	Face *Face `type:"structure"`

	// Confidence in the match of this face with the input face.
	Similarity *float64 `type:"float"`
}

// String returns the string representation
func (s FaceMatch) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s FaceMatch) GoString() string {
	return s.String()
}

// Object containing both the face metadata (stored in the backend database),
// and facial attributes that are detected but aren't stored in the database.
type FaceRecord struct {
	_ struct{} `type:"structure"`

	// Describes the face properties such as the bounding box, face ID, image ID
	// of the input image, and external image ID that you assigned.
	Face *Face `type:"structure"`

	// Structure containing attributes of the face that the algorithm detected.
	FaceDetail *FaceDetail `type:"structure"`
}

// String returns the string representation
func (s FaceRecord) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s FaceRecord) GoString() string {
	return s.String()
}

// Input face recognition parameters for an Amazon Rekognition stream processor.
// FaceRecognitionSettings is a request parameter for .
type FaceSearchSettings struct {
	_ struct{} `type:"structure"`

	// The ID of a collection that contains faces that you want to search for.
	CollectionId *string `min:"1" type:"string"`

	// Minimum face match confidence score that must be met to return a result for
	// a recognized face. Default is 70. 0 is the lowest confidence. 100 is the
	// highest confidence.
	FaceMatchThreshold *float64 `type:"float"`
}

// String returns the string representation
func (s FaceSearchSettings) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s FaceSearchSettings) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *FaceSearchSettings) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "FaceSearchSettings"}
	if s.CollectionId != nil && len(*s.CollectionId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("CollectionId", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

// Gender of the face and the confidence level in the determination.
type Gender struct {
	_ struct{} `type:"structure"`

	// Level of confidence in the determination.
	Confidence *float64 `type:"float"`

	// Gender of the face.
	Value GenderType `type:"string" enum:"true"`
}

// String returns the string representation
func (s Gender) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Gender) GoString() string {
	return s.String()
}

// Information about where the text detected by is located on an image.
type Geometry struct {
	_ struct{} `type:"structure"`

	// An axis-aligned coarse representation of the detected text's location on
	// the image.
	BoundingBox *BoundingBox `type:"structure"`

	// Within the bounding box, a fine-grained polygon around the detected text.
	Polygon []Point `type:"list"`
}

// String returns the string representation
func (s Geometry) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Geometry) GoString() string {
	return s.String()
}

type GetCelebrityInfoInput struct {
	_ struct{} `type:"structure"`

	// The ID for the celebrity. You get the celebrity ID from a call to the operation,
	// which recognizes celebrities in an image.
	//
	// Id is a required field
	Id *string `type:"string" required:"true"`
}

// String returns the string representation
func (s GetCelebrityInfoInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetCelebrityInfoInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *GetCelebrityInfoInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "GetCelebrityInfoInput"}

	if s.Id == nil {
		invalidParams.Add(aws.NewErrParamRequired("Id"))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type GetCelebrityInfoOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The name of the celebrity.
	Name *string `type:"string"`

	// An array of URLs pointing to additional celebrity information.
	Urls []string `type:"list"`
}

// String returns the string representation
func (s GetCelebrityInfoOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetCelebrityInfoOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s GetCelebrityInfoOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type GetCelebrityRecognitionInput struct {
	_ struct{} `type:"structure"`

	// Job identifier for the required celebrity recognition analysis. You can get
	// the job identifer from a call to StartCelebrityRecognition.
	//
	// JobId is a required field
	JobId *string `min:"1" type:"string" required:"true"`

	// Maximum number of results to return per paginated call. The largest value
	// you can specify is 1000. If you specify a value greater than 1000, a maximum
	// of 1000 results is returned. The default value is 1000.
	MaxResults *int64 `min:"1" type:"integer"`

	// If the previous response was incomplete (because there is more recognized
	// celebrities to retrieve), Amazon Rekognition Video returns a pagination token
	// in the response. You can use this pagination token to retrieve the next set
	// of celebrities.
	NextToken *string `type:"string"`

	// Sort to use for celebrities returned in Celebrities field. Specify ID to
	// sort by the celebrity identifier, specify TIMESTAMP to sort by the time the
	// celebrity was recognized.
	SortBy CelebrityRecognitionSortBy `type:"string" enum:"true"`
}

// String returns the string representation
func (s GetCelebrityRecognitionInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetCelebrityRecognitionInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *GetCelebrityRecognitionInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "GetCelebrityRecognitionInput"}

	if s.JobId == nil {
		invalidParams.Add(aws.NewErrParamRequired("JobId"))
	}
	if s.JobId != nil && len(*s.JobId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("JobId", 1))
	}
	if s.MaxResults != nil && *s.MaxResults < 1 {
		invalidParams.Add(aws.NewErrParamMinValue("MaxResults", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type GetCelebrityRecognitionOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// Array of celebrities recognized in the video.
	Celebrities []CelebrityRecognition `type:"list"`

	// The current status of the celebrity recognition job.
	JobStatus VideoJobStatus `type:"string" enum:"true"`

	// If the response is truncated, Amazon Rekognition Video returns this token
	// that you can use in the subsequent request to retrieve the next set of celebrities.
	NextToken *string `type:"string"`

	// If the job fails, StatusMessage provides a descriptive error message.
	StatusMessage *string `type:"string"`

	// Information about a video that Amazon Rekognition Video analyzed. Videometadata
	// is returned in every page of paginated responses from a Amazon Rekognition
	// Video operation.
	VideoMetadata *VideoMetadata `type:"structure"`
}

// String returns the string representation
func (s GetCelebrityRecognitionOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetCelebrityRecognitionOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s GetCelebrityRecognitionOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type GetContentModerationInput struct {
	_ struct{} `type:"structure"`

	// The identifier for the content moderation job. Use JobId to identify the
	// job in a subsequent call to GetContentModeration.
	//
	// JobId is a required field
	JobId *string `min:"1" type:"string" required:"true"`

	// Maximum number of results to return per paginated call. The largest value
	// you can specify is 1000. If you specify a value greater than 1000, a maximum
	// of 1000 results is returned. The default value is 1000.
	MaxResults *int64 `min:"1" type:"integer"`

	// If the previous response was incomplete (because there is more data to retrieve),
	// Amazon Rekognition returns a pagination token in the response. You can use
	// this pagination token to retrieve the next set of content moderation labels.
	NextToken *string `type:"string"`

	// Sort to use for elements in the ModerationLabelDetections array. Use TIMESTAMP
	// to sort array elements by the time labels are detected. Use NAME to alphabetically
	// group elements for a label together. Within each label group, the array element
	// are sorted by detection confidence. The default sort is by TIMESTAMP.
	SortBy ContentModerationSortBy `type:"string" enum:"true"`
}

// String returns the string representation
func (s GetContentModerationInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetContentModerationInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *GetContentModerationInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "GetContentModerationInput"}

	if s.JobId == nil {
		invalidParams.Add(aws.NewErrParamRequired("JobId"))
	}
	if s.JobId != nil && len(*s.JobId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("JobId", 1))
	}
	if s.MaxResults != nil && *s.MaxResults < 1 {
		invalidParams.Add(aws.NewErrParamMinValue("MaxResults", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type GetContentModerationOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The current status of the content moderation job.
	JobStatus VideoJobStatus `type:"string" enum:"true"`

	// The detected moderation labels and the time(s) they were detected.
	ModerationLabels []ContentModerationDetection `type:"list"`

	// If the response is truncated, Amazon Rekognition Video returns this token
	// that you can use in the subsequent request to retrieve the next set of moderation
	// labels.
	NextToken *string `type:"string"`

	// If the job fails, StatusMessage provides a descriptive error message.
	StatusMessage *string `type:"string"`

	// Information about a video that Amazon Rekognition analyzed. Videometadata
	// is returned in every page of paginated responses from GetContentModeration.
	VideoMetadata *VideoMetadata `type:"structure"`
}

// String returns the string representation
func (s GetContentModerationOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetContentModerationOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s GetContentModerationOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type GetFaceDetectionInput struct {
	_ struct{} `type:"structure"`

	// Unique identifier for the face detection job. The JobId is returned from
	// StartFaceDetection.
	//
	// JobId is a required field
	JobId *string `min:"1" type:"string" required:"true"`

	// Maximum number of results to return per paginated call. The largest value
	// you can specify is 1000. If you specify a value greater than 1000, a maximum
	// of 1000 results is returned. The default value is 1000.
	MaxResults *int64 `min:"1" type:"integer"`

	// If the previous response was incomplete (because there are more faces to
	// retrieve), Amazon Rekognition Video returns a pagination token in the response.
	// You can use this pagination token to retrieve the next set of faces.
	NextToken *string `type:"string"`
}

// String returns the string representation
func (s GetFaceDetectionInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetFaceDetectionInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *GetFaceDetectionInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "GetFaceDetectionInput"}

	if s.JobId == nil {
		invalidParams.Add(aws.NewErrParamRequired("JobId"))
	}
	if s.JobId != nil && len(*s.JobId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("JobId", 1))
	}
	if s.MaxResults != nil && *s.MaxResults < 1 {
		invalidParams.Add(aws.NewErrParamMinValue("MaxResults", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type GetFaceDetectionOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// An array of faces detected in the video. Each element contains a detected
	// face's details and the time, in milliseconds from the start of the video,
	// the face was detected.
	Faces []FaceDetection `type:"list"`

	// The current status of the face detection job.
	JobStatus VideoJobStatus `type:"string" enum:"true"`

	// If the response is truncated, Amazon Rekognition returns this token that
	// you can use in the subsequent request to retrieve the next set of faces.
	NextToken *string `type:"string"`

	// If the job fails, StatusMessage provides a descriptive error message.
	StatusMessage *string `type:"string"`

	// Information about a video that Amazon Rekognition Video analyzed. Videometadata
	// is returned in every page of paginated responses from a Amazon Rekognition
	// video operation.
	VideoMetadata *VideoMetadata `type:"structure"`
}

// String returns the string representation
func (s GetFaceDetectionOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetFaceDetectionOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s GetFaceDetectionOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type GetFaceSearchInput struct {
	_ struct{} `type:"structure"`

	// The job identifer for the search request. You get the job identifier from
	// an initial call to StartFaceSearch.
	//
	// JobId is a required field
	JobId *string `min:"1" type:"string" required:"true"`

	// Maximum number of results to return per paginated call. The largest value
	// you can specify is 1000. If you specify a value greater than 1000, a maximum
	// of 1000 results is returned. The default value is 1000.
	MaxResults *int64 `min:"1" type:"integer"`

	// If the previous response was incomplete (because there is more search results
	// to retrieve), Amazon Rekognition Video returns a pagination token in the
	// response. You can use this pagination token to retrieve the next set of search
	// results.
	NextToken *string `type:"string"`

	// Sort to use for grouping faces in the response. Use TIMESTAMP to group faces
	// by the time that they are recognized. Use INDEX to sort by recognized faces.
	SortBy FaceSearchSortBy `type:"string" enum:"true"`
}

// String returns the string representation
func (s GetFaceSearchInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetFaceSearchInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *GetFaceSearchInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "GetFaceSearchInput"}

	if s.JobId == nil {
		invalidParams.Add(aws.NewErrParamRequired("JobId"))
	}
	if s.JobId != nil && len(*s.JobId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("JobId", 1))
	}
	if s.MaxResults != nil && *s.MaxResults < 1 {
		invalidParams.Add(aws.NewErrParamMinValue("MaxResults", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type GetFaceSearchOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The current status of the face search job.
	JobStatus VideoJobStatus `type:"string" enum:"true"`

	// If the response is truncated, Amazon Rekognition Video returns this token
	// that you can use in the subsequent request to retrieve the next set of search
	// results.
	NextToken *string `type:"string"`

	// An array of persons, , in the video whose face(s) match the face(s) in an
	// Amazon Rekognition collection. It also includes time information for when
	// persons are matched in the video. You specify the input collection in an
	// initial call to StartFaceSearch. Each Persons element includes a time the
	// person was matched, face match details (FaceMatches) for matching faces in
	// the collection, and person information (Person) for the matched person.
	Persons []PersonMatch `type:"list"`

	// If the job fails, StatusMessage provides a descriptive error message.
	StatusMessage *string `type:"string"`

	// Information about a video that Amazon Rekognition analyzed. Videometadata
	// is returned in every page of paginated responses from a Amazon Rekognition
	// Video operation.
	VideoMetadata *VideoMetadata `type:"structure"`
}

// String returns the string representation
func (s GetFaceSearchOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetFaceSearchOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s GetFaceSearchOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type GetLabelDetectionInput struct {
	_ struct{} `type:"structure"`

	// Job identifier for the label detection operation for which you want results
	// returned. You get the job identifer from an initial call to StartlabelDetection.
	//
	// JobId is a required field
	JobId *string `min:"1" type:"string" required:"true"`

	// Maximum number of results to return per paginated call. The largest value
	// you can specify is 1000. If you specify a value greater than 1000, a maximum
	// of 1000 results is returned. The default value is 1000.
	MaxResults *int64 `min:"1" type:"integer"`

	// If the previous response was incomplete (because there are more labels to
	// retrieve), Amazon Rekognition Video returns a pagination token in the response.
	// You can use this pagination token to retrieve the next set of labels.
	NextToken *string `type:"string"`

	// Sort to use for elements in the Labels array. Use TIMESTAMP to sort array
	// elements by the time labels are detected. Use NAME to alphabetically group
	// elements for a label together. Within each label group, the array element
	// are sorted by detection confidence. The default sort is by TIMESTAMP.
	SortBy LabelDetectionSortBy `type:"string" enum:"true"`
}

// String returns the string representation
func (s GetLabelDetectionInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetLabelDetectionInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *GetLabelDetectionInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "GetLabelDetectionInput"}

	if s.JobId == nil {
		invalidParams.Add(aws.NewErrParamRequired("JobId"))
	}
	if s.JobId != nil && len(*s.JobId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("JobId", 1))
	}
	if s.MaxResults != nil && *s.MaxResults < 1 {
		invalidParams.Add(aws.NewErrParamMinValue("MaxResults", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type GetLabelDetectionOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The current status of the label detection job.
	JobStatus VideoJobStatus `type:"string" enum:"true"`

	// An array of labels detected in the video. Each element contains the detected
	// label and the time, in milliseconds from the start of the video, that the
	// label was detected.
	Labels []LabelDetection `type:"list"`

	// If the response is truncated, Amazon Rekognition Video returns this token
	// that you can use in the subsequent request to retrieve the next set of labels.
	NextToken *string `type:"string"`

	// If the job fails, StatusMessage provides a descriptive error message.
	StatusMessage *string `type:"string"`

	// Information about a video that Amazon Rekognition Video analyzed. Videometadata
	// is returned in every page of paginated responses from a Amazon Rekognition
	// video operation.
	VideoMetadata *VideoMetadata `type:"structure"`
}

// String returns the string representation
func (s GetLabelDetectionOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetLabelDetectionOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s GetLabelDetectionOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type GetPersonTrackingInput struct {
	_ struct{} `type:"structure"`

	// The identifier for a job that tracks persons in a video. You get the JobId
	// from a call to StartPersonTracking.
	//
	// JobId is a required field
	JobId *string `min:"1" type:"string" required:"true"`

	// Maximum number of results to return per paginated call. The largest value
	// you can specify is 1000. If you specify a value greater than 1000, a maximum
	// of 1000 results is returned. The default value is 1000.
	MaxResults *int64 `min:"1" type:"integer"`

	// If the previous response was incomplete (because there are more persons to
	// retrieve), Amazon Rekognition Video returns a pagination token in the response.
	// You can use this pagination token to retrieve the next set of persons.
	NextToken *string `type:"string"`

	// Sort to use for elements in the Persons array. Use TIMESTAMP to sort array
	// elements by the time persons are detected. Use INDEX to sort by the tracked
	// persons. If you sort by INDEX, the array elements for each person are sorted
	// by detection confidence. The default sort is by TIMESTAMP.
	SortBy PersonTrackingSortBy `type:"string" enum:"true"`
}

// String returns the string representation
func (s GetPersonTrackingInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetPersonTrackingInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *GetPersonTrackingInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "GetPersonTrackingInput"}

	if s.JobId == nil {
		invalidParams.Add(aws.NewErrParamRequired("JobId"))
	}
	if s.JobId != nil && len(*s.JobId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("JobId", 1))
	}
	if s.MaxResults != nil && *s.MaxResults < 1 {
		invalidParams.Add(aws.NewErrParamMinValue("MaxResults", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type GetPersonTrackingOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The current status of the person tracking job.
	JobStatus VideoJobStatus `type:"string" enum:"true"`

	// If the response is truncated, Amazon Rekognition Video returns this token
	// that you can use in the subsequent request to retrieve the next set of persons.
	NextToken *string `type:"string"`

	// An array of the persons detected in the video and the time(s) their path
	// was tracked throughout the video. An array element will exist for each time
	// a person's path is tracked.
	Persons []PersonDetection `type:"list"`

	// If the job fails, StatusMessage provides a descriptive error message.
	StatusMessage *string `type:"string"`

	// Information about a video that Amazon Rekognition Video analyzed. Videometadata
	// is returned in every page of paginated responses from a Amazon Rekognition
	// Video operation.
	VideoMetadata *VideoMetadata `type:"structure"`
}

// String returns the string representation
func (s GetPersonTrackingOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s GetPersonTrackingOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s GetPersonTrackingOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

// Provides the input image either as bytes or an S3 object.
//
// You pass image bytes to an Amazon Rekognition API operation by using the
// Bytes property. For example, you would use the Bytes property to pass an
// image loaded from a local file system. Image bytes passed by using the Bytes
// property must be base64-encoded. Your code may not need to encode image bytes
// if you are using an AWS SDK to call Amazon Rekognition API operations.
//
// For more information, see Analyzing an Image Loaded from a Local File System
// in the Amazon Rekognition Developer Guide.
//
// You pass images stored in an S3 bucket to an Amazon Rekognition API operation
// by using the S3Object property. Images stored in an S3 bucket do not need
// to be base64-encoded.
//
// The region for the S3 bucket containing the S3 object must match the region
// you use for Amazon Rekognition operations.
//
// If you use the AWS CLI to call Amazon Rekognition operations, passing image
// bytes using the Bytes property is not supported. You must first upload the
// image to an Amazon S3 bucket and then call the operation using the S3Object
// property.
//
// For Amazon Rekognition to process an S3 object, the user must have permission
// to access the S3 object. For more information, see Resource Based Policies
// in the Amazon Rekognition Developer Guide.
type Image struct {
	_ struct{} `type:"structure"`

	// Blob of image bytes up to 5 MBs.
	//
	// Bytes is automatically base64 encoded/decoded by the SDK.
	Bytes []byte `min:"1" type:"blob"`

	// Identifies an S3 object as the image source.
	S3Object *S3Object `type:"structure"`
}

// String returns the string representation
func (s Image) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Image) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *Image) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "Image"}
	if s.Bytes != nil && len(s.Bytes) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("Bytes", 1))
	}
	if s.S3Object != nil {
		if err := s.S3Object.Validate(); err != nil {
			invalidParams.AddNested("S3Object", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

// Identifies face image brightness and sharpness.
type ImageQuality struct {
	_ struct{} `type:"structure"`

	// Value representing brightness of the face. The service returns a value between
	// 0 and 100 (inclusive). A higher value indicates a brighter face image.
	Brightness *float64 `type:"float"`

	// Value representing sharpness of the face. The service returns a value between
	// 0 and 100 (inclusive). A higher value indicates a sharper face image.
	Sharpness *float64 `type:"float"`
}

// String returns the string representation
func (s ImageQuality) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s ImageQuality) GoString() string {
	return s.String()
}

type IndexFacesInput struct {
	_ struct{} `type:"structure"`

	// The ID of an existing collection to which you want to add the faces that
	// are detected in the input images.
	//
	// CollectionId is a required field
	CollectionId *string `min:"1" type:"string" required:"true"`

	// An array of facial attributes that you want to be returned. This can be the
	// default list of attributes or all attributes. If you don't specify a value
	// for Attributes or if you specify ["DEFAULT"], the API returns the following
	// subset of facial attributes: BoundingBox, Confidence, Pose, Quality, and
	// Landmarks. If you provide ["ALL"], all facial attributes are returned, but
	// the operation takes longer to complete.
	//
	// If you provide both, ["ALL", "DEFAULT"], the service uses a logical AND operator
	// to determine which attributes to return (in this case, all attributes).
	DetectionAttributes []Attribute `type:"list"`

	// The ID you want to assign to all the faces detected in the image.
	ExternalImageId *string `min:"1" type:"string"`

	// The input image as base64-encoded bytes or an S3 object. If you use the AWS
	// CLI to call Amazon Rekognition operations, passing base64-encoded image bytes
	// isn't supported.
	//
	// Image is a required field
	Image *Image `type:"structure" required:"true"`

	// The maximum number of faces to index. The value of MaxFaces must be greater
	// than or equal to 1. IndexFaces returns no more than 100 detected faces in
	// an image, even if you specify a larger value for MaxFaces.
	//
	// If IndexFaces detects more faces than the value of MaxFaces, the faces with
	// the lowest quality are filtered out first. If there are still more faces
	// than the value of MaxFaces, the faces with the smallest bounding boxes are
	// filtered out (up to the number that's needed to satisfy the value of MaxFaces).
	// Information about the unindexed faces is available in the UnindexedFaces
	// array.
	//
	// The faces that are returned by IndexFaces are sorted by the largest face
	// bounding box size to the smallest size, in descending order.
	//
	// MaxFaces can be used with a collection associated with any version of the
	// face model.
	MaxFaces *int64 `min:"1" type:"integer"`

	// A filter that specifies how much filtering is done to identify faces that
	// are detected with low quality. Filtered faces aren't indexed. If you specify
	// AUTO, filtering prioritizes the identification of faces that dont meet the
	// required quality bar chosen by Amazon Rekognition. The quality bar is based
	// on a variety of common use cases. Low-quality detections can occur for a
	// number of reasons. Some examples are an object that's misidentified as a
	// face, a face that's too blurry, or a face with a pose that's too extreme
	// to use. If you specify NONE, no filtering is performed. The default value
	// is AUTO.
	//
	// To use quality filtering, the collection you are using must be associated
	// with version 3 of the face model.
	QualityFilter QualityFilter `type:"string" enum:"true"`
}

// String returns the string representation
func (s IndexFacesInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s IndexFacesInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *IndexFacesInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "IndexFacesInput"}

	if s.CollectionId == nil {
		invalidParams.Add(aws.NewErrParamRequired("CollectionId"))
	}
	if s.CollectionId != nil && len(*s.CollectionId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("CollectionId", 1))
	}
	if s.ExternalImageId != nil && len(*s.ExternalImageId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("ExternalImageId", 1))
	}

	if s.Image == nil {
		invalidParams.Add(aws.NewErrParamRequired("Image"))
	}
	if s.MaxFaces != nil && *s.MaxFaces < 1 {
		invalidParams.Add(aws.NewErrParamMinValue("MaxFaces", 1))
	}
	if s.Image != nil {
		if err := s.Image.Validate(); err != nil {
			invalidParams.AddNested("Image", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type IndexFacesOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The version number of the face detection model that's associated with the
	// input collection (CollectionId).
	FaceModelVersion *string `type:"string"`

	// An array of faces detected and added to the collection. For more information,
	// see Searching Faces in a Collection in the Amazon Rekognition Developer Guide.
	FaceRecords []FaceRecord `type:"list"`

	// If your collection is associated with a face detection model that's later
	// than version 3.0, the value of OrientationCorrection is always null and no
	// orientation information is returned.
	//
	// If your collection is associated with a face detection model that's version
	// 3.0 or earlier, the following applies:
	//
	//    * If the input image is in .jpeg format, it might contain exchangeable
	//    image file format (Exif) metadata that includes the image's orientation.
	//    Amazon Rekognition uses this orientation information to perform image
	//    correction - the bounding box coordinates are translated to represent
	//    object locations after the orientation information in the Exif metadata
	//    is used to correct the image orientation. Images in .png format don't
	//    contain Exif metadata. The value of OrientationCorrection is null.
	//
	//    * If the image doesn't contain orientation information in its Exif metadata,
	//    Amazon Rekognition returns an estimated orientation (ROTATE_0, ROTATE_90,
	//    ROTATE_180, ROTATE_270). Amazon Rekognition doesnt perform image correction
	//    for images. The bounding box coordinates aren't translated and represent
	//    the object locations before the image is rotated.
	//
	// Bounding box information is returned in the FaceRecords array. You can get
	// the version of the face detection model by calling .
	OrientationCorrection OrientationCorrection `type:"string" enum:"true"`

	// An array of faces that were detected in the image but weren't indexed. They
	// weren't indexed because the quality filter identified them as low quality,
	// or the MaxFaces request parameter filtered them out. To use the quality filter,
	// you specify the QualityFilter request parameter.
	UnindexedFaces []UnindexedFace `type:"list"`
}

// String returns the string representation
func (s IndexFacesOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s IndexFacesOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s IndexFacesOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

// An instance of a label detected by .
type Instance struct {
	_ struct{} `type:"structure"`

	// The position of the label instance on the image.
	BoundingBox *BoundingBox `type:"structure"`

	// The confidence that Amazon Rekognition Image has in the accuracy of the bounding
	// box.
	Confidence *float64 `type:"float"`
}

// String returns the string representation
func (s Instance) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Instance) GoString() string {
	return s.String()
}

// The Kinesis data stream Amazon Rekognition to which the analysis results
// of a Amazon Rekognition stream processor are streamed. For more information,
// see CreateStreamProcessor in the Amazon Rekognition Developer Guide.
type KinesisDataStream struct {
	_ struct{} `type:"structure"`

	// ARN of the output Amazon Kinesis Data Streams stream.
	Arn *string `type:"string"`
}

// String returns the string representation
func (s KinesisDataStream) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s KinesisDataStream) GoString() string {
	return s.String()
}

// Kinesis video stream stream that provides the source streaming video for
// a Amazon Rekognition Video stream processor. For more information, see CreateStreamProcessor
// in the Amazon Rekognition Developer Guide.
type KinesisVideoStream struct {
	_ struct{} `type:"structure"`

	// ARN of the Kinesis video stream stream that streams the source video.
	Arn *string `type:"string"`
}

// String returns the string representation
func (s KinesisVideoStream) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s KinesisVideoStream) GoString() string {
	return s.String()
}

// Structure containing details about the detected label, including the name,
// and level of confidence.
//
// The Amazon Rekognition Image operation operation returns a hierarchical taxonomy
// (Parents) for detected labels and also bounding box information (Instances)
// for detected labels. Amazon Rekognition Video doesn't return this information
// and returns null for the Parents and Instances attributes.
type Label struct {
	_ struct{} `type:"structure"`

	// Level of confidence.
	Confidence *float64 `type:"float"`

	// If Label represents an object, Instances contains the bounding boxes for
	// each instance of the detected object. Bounding boxes are returned for common
	// object labels such as people, cars, furniture, apparel or pets.
	//
	// Amazon Rekognition Video does not support bounding box information for detected
	// labels. The value of Instances is returned as null by GetLabelDetection.
	Instances []Instance `type:"list"`

	// The name (label) of the object or scene.
	Name *string `type:"string"`

	// The parent labels for a label. The response includes all ancestor labels.
	//
	// Amazon Rekognition Video does not support a hierarchical taxonomy of detected
	// labels. The value of Parents is returned as null by GetLabelDetection.
	Parents []Parent `type:"list"`
}

// String returns the string representation
func (s Label) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Label) GoString() string {
	return s.String()
}

// Information about a label detected in a video analysis request and the time
// the label was detected in the video.
type LabelDetection struct {
	_ struct{} `type:"structure"`

	// Details about the detected label.
	Label *Label `type:"structure"`

	// Time, in milliseconds from the start of the video, that the label was detected.
	Timestamp *int64 `type:"long"`
}

// String returns the string representation
func (s LabelDetection) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s LabelDetection) GoString() string {
	return s.String()
}

// Indicates the location of the landmark on the face.
type Landmark struct {
	_ struct{} `type:"structure"`

	// Type of landmark.
	Type LandmarkType `type:"string" enum:"true"`

	// The x-coordinate from the top left of the landmark expressed as the ratio
	// of the width of the image. For example, if the image is 700 x 200 and the
	// x-coordinate of the landmark is at 350 pixels, this value is 0.5.
	X *float64 `type:"float"`

	// The y-coordinate from the top left of the landmark expressed as the ratio
	// of the height of the image. For example, if the image is 700 x 200 and the
	// y-coordinate of the landmark is at 100 pixels, this value is 0.5.
	Y *float64 `type:"float"`
}

// String returns the string representation
func (s Landmark) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Landmark) GoString() string {
	return s.String()
}

type ListCollectionsInput struct {
	_ struct{} `type:"structure"`

	// Maximum number of collection IDs to return.
	MaxResults *int64 `type:"integer"`

	// Pagination token from the previous response.
	NextToken *string `type:"string"`
}

// String returns the string representation
func (s ListCollectionsInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s ListCollectionsInput) GoString() string {
	return s.String()
}

type ListCollectionsOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// An array of collection IDs.
	CollectionIds []string `type:"list"`

	// Version numbers of the face detection models associated with the collections
	// in the array CollectionIds. For example, the value of FaceModelVersions[2]
	// is the version number for the face detection model used by the collection
	// in CollectionId[2].
	FaceModelVersions []string `type:"list"`

	// If the result is truncated, the response provides a NextToken that you can
	// use in the subsequent request to fetch the next set of collection IDs.
	NextToken *string `type:"string"`
}

// String returns the string representation
func (s ListCollectionsOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s ListCollectionsOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s ListCollectionsOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type ListFacesInput struct {
	_ struct{} `type:"structure"`

	// ID of the collection from which to list the faces.
	//
	// CollectionId is a required field
	CollectionId *string `min:"1" type:"string" required:"true"`

	// Maximum number of faces to return.
	MaxResults *int64 `type:"integer"`

	// If the previous response was incomplete (because there is more data to retrieve),
	// Amazon Rekognition returns a pagination token in the response. You can use
	// this pagination token to retrieve the next set of faces.
	NextToken *string `type:"string"`
}

// String returns the string representation
func (s ListFacesInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s ListFacesInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *ListFacesInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "ListFacesInput"}

	if s.CollectionId == nil {
		invalidParams.Add(aws.NewErrParamRequired("CollectionId"))
	}
	if s.CollectionId != nil && len(*s.CollectionId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("CollectionId", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type ListFacesOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// Version number of the face detection model associated with the input collection
	// (CollectionId).
	FaceModelVersion *string `type:"string"`

	// An array of Face objects.
	Faces []Face `type:"list"`

	// If the response is truncated, Amazon Rekognition returns this token that
	// you can use in the subsequent request to retrieve the next set of faces.
	NextToken *string `type:"string"`
}

// String returns the string representation
func (s ListFacesOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s ListFacesOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s ListFacesOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type ListStreamProcessorsInput struct {
	_ struct{} `type:"structure"`

	// Maximum number of stream processors you want Amazon Rekognition Video to
	// return in the response. The default is 1000.
	MaxResults *int64 `min:"1" type:"integer"`

	// If the previous response was incomplete (because there are more stream processors
	// to retrieve), Amazon Rekognition Video returns a pagination token in the
	// response. You can use this pagination token to retrieve the next set of stream
	// processors.
	NextToken *string `type:"string"`
}

// String returns the string representation
func (s ListStreamProcessorsInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s ListStreamProcessorsInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *ListStreamProcessorsInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "ListStreamProcessorsInput"}
	if s.MaxResults != nil && *s.MaxResults < 1 {
		invalidParams.Add(aws.NewErrParamMinValue("MaxResults", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type ListStreamProcessorsOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// If the response is truncated, Amazon Rekognition Video returns this token
	// that you can use in the subsequent request to retrieve the next set of stream
	// processors.
	NextToken *string `type:"string"`

	// List of stream processors that you have created.
	StreamProcessors []StreamProcessor `type:"list"`
}

// String returns the string representation
func (s ListStreamProcessorsOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s ListStreamProcessorsOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s ListStreamProcessorsOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

// Provides information about a single type of moderated content found in an
// image or video. Each type of moderated content has a label within a hierarchical
// taxonomy. For more information, see Detecting Unsafe Content in the Amazon
// Rekognition Developer Guide.
type ModerationLabel struct {
	_ struct{} `type:"structure"`

	// Specifies the confidence that Amazon Rekognition has that the label has been
	// correctly identified.
	//
	// If you don't specify the MinConfidence parameter in the call to DetectModerationLabels,
	// the operation returns labels with a confidence value greater than or equal
	// to 50 percent.
	Confidence *float64 `type:"float"`

	// The label name for the type of content detected in the image.
	Name *string `type:"string"`

	// The name for the parent label. Labels at the top level of the hierarchy have
	// the parent label "".
	ParentName *string `type:"string"`
}

// String returns the string representation
func (s ModerationLabel) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s ModerationLabel) GoString() string {
	return s.String()
}

// Indicates whether or not the mouth on the face is open, and the confidence
// level in the determination.
type MouthOpen struct {
	_ struct{} `type:"structure"`

	// Level of confidence in the determination.
	Confidence *float64 `type:"float"`

	// Boolean value that indicates whether the mouth on the face is open or not.
	Value *bool `type:"boolean"`
}

// String returns the string representation
func (s MouthOpen) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s MouthOpen) GoString() string {
	return s.String()
}

// Indicates whether or not the face has a mustache, and the confidence level
// in the determination.
type Mustache struct {
	_ struct{} `type:"structure"`

	// Level of confidence in the determination.
	Confidence *float64 `type:"float"`

	// Boolean value that indicates whether the face has mustache or not.
	Value *bool `type:"boolean"`
}

// String returns the string representation
func (s Mustache) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Mustache) GoString() string {
	return s.String()
}

// The Amazon Simple Notification Service topic to which Amazon Rekognition
// publishes the completion status of a video analysis operation. For more information,
// see api-video.
type NotificationChannel struct {
	_ struct{} `type:"structure"`

	// The ARN of an IAM role that gives Amazon Rekognition publishing permissions
	// to the Amazon SNS topic.
	//
	// RoleArn is a required field
	RoleArn *string `type:"string" required:"true"`

	// The Amazon SNS topic to which Amazon Rekognition to posts the completion
	// status.
	//
	// SNSTopicArn is a required field
	SNSTopicArn *string `type:"string" required:"true"`
}

// String returns the string representation
func (s NotificationChannel) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s NotificationChannel) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *NotificationChannel) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "NotificationChannel"}

	if s.RoleArn == nil {
		invalidParams.Add(aws.NewErrParamRequired("RoleArn"))
	}

	if s.SNSTopicArn == nil {
		invalidParams.Add(aws.NewErrParamRequired("SNSTopicArn"))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

// A parent label for a label. A label can have 0, 1, or more parents.
type Parent struct {
	_ struct{} `type:"structure"`

	// The name of the parent label.
	Name *string `type:"string"`
}

// String returns the string representation
func (s Parent) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Parent) GoString() string {
	return s.String()
}

// Details about a person detected in a video analysis request.
type PersonDetail struct {
	_ struct{} `type:"structure"`

	// Bounding box around the detected person.
	BoundingBox *BoundingBox `type:"structure"`

	// Face details for the detected person.
	Face *FaceDetail `type:"structure"`

	// Identifier for the person detected person within a video. Use to keep track
	// of the person throughout the video. The identifier is not stored by Amazon
	// Rekognition.
	Index *int64 `type:"long"`
}

// String returns the string representation
func (s PersonDetail) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s PersonDetail) GoString() string {
	return s.String()
}

// Details and path tracking information for a single time a person's path is
// tracked in a video. Amazon Rekognition operations that track people's paths
// return an array of PersonDetection objects with elements for each time a
// person's path is tracked in a video.
//
// For more information, see API_GetPersonTracking in the Amazon Rekognition
// Developer Guide.
type PersonDetection struct {
	_ struct{} `type:"structure"`

	// Details about a person whose path was tracked in a video.
	Person *PersonDetail `type:"structure"`

	// The time, in milliseconds from the start of the video, that the person's
	// path was tracked.
	Timestamp *int64 `type:"long"`
}

// String returns the string representation
func (s PersonDetection) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s PersonDetection) GoString() string {
	return s.String()
}

// Information about a person whose face matches a face(s) in an Amazon Rekognition
// collection. Includes information about the faces in the Amazon Rekognition
// collection (), information about the person (PersonDetail), and the time
// stamp for when the person was detected in a video. An array of PersonMatch
// objects is returned by .
type PersonMatch struct {
	_ struct{} `type:"structure"`

	// Information about the faces in the input collection that match the face of
	// a person in the video.
	FaceMatches []FaceMatch `type:"list"`

	// Information about the matched person.
	Person *PersonDetail `type:"structure"`

	// The time, in milliseconds from the beginning of the video, that the person
	// was matched in the video.
	Timestamp *int64 `type:"long"`
}

// String returns the string representation
func (s PersonMatch) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s PersonMatch) GoString() string {
	return s.String()
}

// The X and Y coordinates of a point on an image. The X and Y values returned
// are ratios of the overall image size. For example, if the input image is
// 700x200 and the operation returns X=0.5 and Y=0.25, then the point is at
// the (350,50) pixel coordinate on the image.
//
// An array of Point objects, Polygon, is returned by . Polygon represents a
// fine-grained polygon around detected text. For more information, see Geometry
// in the Amazon Rekognition Developer Guide.
type Point struct {
	_ struct{} `type:"structure"`

	// The value of the X coordinate for a point on a Polygon.
	X *float64 `type:"float"`

	// The value of the Y coordinate for a point on a Polygon.
	Y *float64 `type:"float"`
}

// String returns the string representation
func (s Point) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Point) GoString() string {
	return s.String()
}

// Indicates the pose of the face as determined by its pitch, roll, and yaw.
type Pose struct {
	_ struct{} `type:"structure"`

	// Value representing the face rotation on the pitch axis.
	Pitch *float64 `type:"float"`

	// Value representing the face rotation on the roll axis.
	Roll *float64 `type:"float"`

	// Value representing the face rotation on the yaw axis.
	Yaw *float64 `type:"float"`
}

// String returns the string representation
func (s Pose) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Pose) GoString() string {
	return s.String()
}

type RecognizeCelebritiesInput struct {
	_ struct{} `type:"structure"`

	// The input image as base64-encoded bytes or an S3 object. If you use the AWS
	// CLI to call Amazon Rekognition operations, passing base64-encoded image bytes
	// is not supported.
	//
	// Image is a required field
	Image *Image `type:"structure" required:"true"`
}

// String returns the string representation
func (s RecognizeCelebritiesInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s RecognizeCelebritiesInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *RecognizeCelebritiesInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "RecognizeCelebritiesInput"}

	if s.Image == nil {
		invalidParams.Add(aws.NewErrParamRequired("Image"))
	}
	if s.Image != nil {
		if err := s.Image.Validate(); err != nil {
			invalidParams.AddNested("Image", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type RecognizeCelebritiesOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// Details about each celebrity found in the image. Amazon Rekognition can detect
	// a maximum of 15 celebrities in an image.
	CelebrityFaces []Celebrity `type:"list"`

	// The orientation of the input image (counterclockwise direction). If your
	// application displays the image, you can use this value to correct the orientation.
	// The bounding box coordinates returned in CelebrityFaces and UnrecognizedFaces
	// represent face locations before the image orientation is corrected.
	//
	// If the input image is in .jpeg format, it might contain exchangeable image
	// (Exif) metadata that includes the image's orientation. If so, and the Exif
	// metadata for the input image populates the orientation field, the value of
	// OrientationCorrection is null. The CelebrityFaces and UnrecognizedFaces bounding
	// box coordinates represent face locations after Exif metadata is used to correct
	// the image orientation. Images in .png format don't contain Exif metadata.
	OrientationCorrection OrientationCorrection `type:"string" enum:"true"`

	// Details about each unrecognized face in the image.
	UnrecognizedFaces []ComparedFace `type:"list"`
}

// String returns the string representation
func (s RecognizeCelebritiesOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s RecognizeCelebritiesOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s RecognizeCelebritiesOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

// Provides the S3 bucket name and object name.
//
// The region for the S3 bucket containing the S3 object must match the region
// you use for Amazon Rekognition operations.
//
// For Amazon Rekognition to process an S3 object, the user must have permission
// to access the S3 object. For more information, see Resource-Based Policies
// in the Amazon Rekognition Developer Guide.
type S3Object struct {
	_ struct{} `type:"structure"`

	// Name of the S3 bucket.
	Bucket *string `min:"3" type:"string"`

	// S3 object key name.
	Name *string `min:"1" type:"string"`

	// If the bucket is versioning enabled, you can specify the object version.
	Version *string `min:"1" type:"string"`
}

// String returns the string representation
func (s S3Object) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s S3Object) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *S3Object) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "S3Object"}
	if s.Bucket != nil && len(*s.Bucket) < 3 {
		invalidParams.Add(aws.NewErrParamMinLen("Bucket", 3))
	}
	if s.Name != nil && len(*s.Name) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("Name", 1))
	}
	if s.Version != nil && len(*s.Version) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("Version", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type SearchFacesByImageInput struct {
	_ struct{} `type:"structure"`

	// ID of the collection to search.
	//
	// CollectionId is a required field
	CollectionId *string `min:"1" type:"string" required:"true"`

	// (Optional) Specifies the minimum confidence in the face match to return.
	// For example, don't return any matches where confidence in matches is less
	// than 70%.
	FaceMatchThreshold *float64 `type:"float"`

	// The input image as base64-encoded bytes or an S3 object. If you use the AWS
	// CLI to call Amazon Rekognition operations, passing base64-encoded image bytes
	// is not supported.
	//
	// Image is a required field
	Image *Image `type:"structure" required:"true"`

	// Maximum number of faces to return. The operation returns the maximum number
	// of faces with the highest confidence in the match.
	MaxFaces *int64 `min:"1" type:"integer"`
}

// String returns the string representation
func (s SearchFacesByImageInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s SearchFacesByImageInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *SearchFacesByImageInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "SearchFacesByImageInput"}

	if s.CollectionId == nil {
		invalidParams.Add(aws.NewErrParamRequired("CollectionId"))
	}
	if s.CollectionId != nil && len(*s.CollectionId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("CollectionId", 1))
	}

	if s.Image == nil {
		invalidParams.Add(aws.NewErrParamRequired("Image"))
	}
	if s.MaxFaces != nil && *s.MaxFaces < 1 {
		invalidParams.Add(aws.NewErrParamMinValue("MaxFaces", 1))
	}
	if s.Image != nil {
		if err := s.Image.Validate(); err != nil {
			invalidParams.AddNested("Image", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type SearchFacesByImageOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// An array of faces that match the input face, along with the confidence in
	// the match.
	FaceMatches []FaceMatch `type:"list"`

	// Version number of the face detection model associated with the input collection
	// (CollectionId).
	FaceModelVersion *string `type:"string"`

	// The bounding box around the face in the input image that Amazon Rekognition
	// used for the search.
	SearchedFaceBoundingBox *BoundingBox `type:"structure"`

	// The level of confidence that the searchedFaceBoundingBox, contains a face.
	SearchedFaceConfidence *float64 `type:"float"`
}

// String returns the string representation
func (s SearchFacesByImageOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s SearchFacesByImageOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s SearchFacesByImageOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type SearchFacesInput struct {
	_ struct{} `type:"structure"`

	// ID of the collection the face belongs to.
	//
	// CollectionId is a required field
	CollectionId *string `min:"1" type:"string" required:"true"`

	// ID of a face to find matches for in the collection.
	//
	// FaceId is a required field
	FaceId *string `type:"string" required:"true"`

	// Optional value specifying the minimum confidence in the face match to return.
	// For example, don't return any matches where confidence in matches is less
	// than 70%.
	FaceMatchThreshold *float64 `type:"float"`

	// Maximum number of faces to return. The operation returns the maximum number
	// of faces with the highest confidence in the match.
	MaxFaces *int64 `min:"1" type:"integer"`
}

// String returns the string representation
func (s SearchFacesInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s SearchFacesInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *SearchFacesInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "SearchFacesInput"}

	if s.CollectionId == nil {
		invalidParams.Add(aws.NewErrParamRequired("CollectionId"))
	}
	if s.CollectionId != nil && len(*s.CollectionId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("CollectionId", 1))
	}

	if s.FaceId == nil {
		invalidParams.Add(aws.NewErrParamRequired("FaceId"))
	}
	if s.MaxFaces != nil && *s.MaxFaces < 1 {
		invalidParams.Add(aws.NewErrParamMinValue("MaxFaces", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type SearchFacesOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// An array of faces that matched the input face, along with the confidence
	// in the match.
	FaceMatches []FaceMatch `type:"list"`

	// Version number of the face detection model associated with the input collection
	// (CollectionId).
	FaceModelVersion *string `type:"string"`

	// ID of the face that was searched for matches in a collection.
	SearchedFaceId *string `type:"string"`
}

// String returns the string representation
func (s SearchFacesOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s SearchFacesOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s SearchFacesOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

// Indicates whether or not the face is smiling, and the confidence level in
// the determination.
type Smile struct {
	_ struct{} `type:"structure"`

	// Level of confidence in the determination.
	Confidence *float64 `type:"float"`

	// Boolean value that indicates whether the face is smiling or not.
	Value *bool `type:"boolean"`
}

// String returns the string representation
func (s Smile) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Smile) GoString() string {
	return s.String()
}

type StartCelebrityRecognitionInput struct {
	_ struct{} `type:"structure"`

	// Idempotent token used to identify the start request. If you use the same
	// token with multiple StartCelebrityRecognition requests, the same JobId is
	// returned. Use ClientRequestToken to prevent the same job from being accidently
	// started more than once.
	ClientRequestToken *string `min:"1" type:"string"`

	// Unique identifier you specify to identify the job in the completion status
	// published to the Amazon Simple Notification Service topic.
	JobTag *string `min:"1" type:"string"`

	// The Amazon SNS topic ARN that you want Amazon Rekognition Video to publish
	// the completion status of the celebrity recognition analysis to.
	NotificationChannel *NotificationChannel `type:"structure"`

	// The video in which you want to recognize celebrities. The video must be stored
	// in an Amazon S3 bucket.
	//
	// Video is a required field
	Video *Video `type:"structure" required:"true"`
}

// String returns the string representation
func (s StartCelebrityRecognitionInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartCelebrityRecognitionInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *StartCelebrityRecognitionInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "StartCelebrityRecognitionInput"}
	if s.ClientRequestToken != nil && len(*s.ClientRequestToken) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("ClientRequestToken", 1))
	}
	if s.JobTag != nil && len(*s.JobTag) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("JobTag", 1))
	}

	if s.Video == nil {
		invalidParams.Add(aws.NewErrParamRequired("Video"))
	}
	if s.NotificationChannel != nil {
		if err := s.NotificationChannel.Validate(); err != nil {
			invalidParams.AddNested("NotificationChannel", err.(aws.ErrInvalidParams))
		}
	}
	if s.Video != nil {
		if err := s.Video.Validate(); err != nil {
			invalidParams.AddNested("Video", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type StartCelebrityRecognitionOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The identifier for the celebrity recognition analysis job. Use JobId to identify
	// the job in a subsequent call to GetCelebrityRecognition.
	JobId *string `min:"1" type:"string"`
}

// String returns the string representation
func (s StartCelebrityRecognitionOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartCelebrityRecognitionOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s StartCelebrityRecognitionOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type StartContentModerationInput struct {
	_ struct{} `type:"structure"`

	// Idempotent token used to identify the start request. If you use the same
	// token with multiple StartContentModeration requests, the same JobId is returned.
	// Use ClientRequestToken to prevent the same job from being accidently started
	// more than once.
	ClientRequestToken *string `min:"1" type:"string"`

	// Unique identifier you specify to identify the job in the completion status
	// published to the Amazon Simple Notification Service topic.
	JobTag *string `min:"1" type:"string"`

	// Specifies the minimum confidence that Amazon Rekognition must have in order
	// to return a moderated content label. Confidence represents how certain Amazon
	// Rekognition is that the moderated content is correctly identified. 0 is the
	// lowest confidence. 100 is the highest confidence. Amazon Rekognition doesn't
	// return any moderated content labels with a confidence level lower than this
	// specified value. If you don't specify MinConfidence, GetContentModeration
	// returns labels with confidence values greater than or equal to 50 percent.
	MinConfidence *float64 `type:"float"`

	// The Amazon SNS topic ARN that you want Amazon Rekognition Video to publish
	// the completion status of the content moderation analysis to.
	NotificationChannel *NotificationChannel `type:"structure"`

	// The video in which you want to moderate content. The video must be stored
	// in an Amazon S3 bucket.
	//
	// Video is a required field
	Video *Video `type:"structure" required:"true"`
}

// String returns the string representation
func (s StartContentModerationInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartContentModerationInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *StartContentModerationInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "StartContentModerationInput"}
	if s.ClientRequestToken != nil && len(*s.ClientRequestToken) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("ClientRequestToken", 1))
	}
	if s.JobTag != nil && len(*s.JobTag) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("JobTag", 1))
	}

	if s.Video == nil {
		invalidParams.Add(aws.NewErrParamRequired("Video"))
	}
	if s.NotificationChannel != nil {
		if err := s.NotificationChannel.Validate(); err != nil {
			invalidParams.AddNested("NotificationChannel", err.(aws.ErrInvalidParams))
		}
	}
	if s.Video != nil {
		if err := s.Video.Validate(); err != nil {
			invalidParams.AddNested("Video", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type StartContentModerationOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The identifier for the content moderation analysis job. Use JobId to identify
	// the job in a subsequent call to GetContentModeration.
	JobId *string `min:"1" type:"string"`
}

// String returns the string representation
func (s StartContentModerationOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartContentModerationOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s StartContentModerationOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type StartFaceDetectionInput struct {
	_ struct{} `type:"structure"`

	// Idempotent token used to identify the start request. If you use the same
	// token with multiple StartFaceDetection requests, the same JobId is returned.
	// Use ClientRequestToken to prevent the same job from being accidently started
	// more than once.
	ClientRequestToken *string `min:"1" type:"string"`

	// The face attributes you want returned.
	//
	// DEFAULT - The following subset of facial attributes are returned: BoundingBox,
	// Confidence, Pose, Quality and Landmarks.
	//
	// ALL - All facial attributes are returned.
	FaceAttributes FaceAttributes `type:"string" enum:"true"`

	// Unique identifier you specify to identify the job in the completion status
	// published to the Amazon Simple Notification Service topic.
	JobTag *string `min:"1" type:"string"`

	// The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video
	// to publish the completion status of the face detection operation.
	NotificationChannel *NotificationChannel `type:"structure"`

	// The video in which you want to detect faces. The video must be stored in
	// an Amazon S3 bucket.
	//
	// Video is a required field
	Video *Video `type:"structure" required:"true"`
}

// String returns the string representation
func (s StartFaceDetectionInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartFaceDetectionInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *StartFaceDetectionInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "StartFaceDetectionInput"}
	if s.ClientRequestToken != nil && len(*s.ClientRequestToken) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("ClientRequestToken", 1))
	}
	if s.JobTag != nil && len(*s.JobTag) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("JobTag", 1))
	}

	if s.Video == nil {
		invalidParams.Add(aws.NewErrParamRequired("Video"))
	}
	if s.NotificationChannel != nil {
		if err := s.NotificationChannel.Validate(); err != nil {
			invalidParams.AddNested("NotificationChannel", err.(aws.ErrInvalidParams))
		}
	}
	if s.Video != nil {
		if err := s.Video.Validate(); err != nil {
			invalidParams.AddNested("Video", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type StartFaceDetectionOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The identifier for the face detection job. Use JobId to identify the job
	// in a subsequent call to GetFaceDetection.
	JobId *string `min:"1" type:"string"`
}

// String returns the string representation
func (s StartFaceDetectionOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartFaceDetectionOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s StartFaceDetectionOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type StartFaceSearchInput struct {
	_ struct{} `type:"structure"`

	// Idempotent token used to identify the start request. If you use the same
	// token with multiple StartFaceSearch requests, the same JobId is returned.
	// Use ClientRequestToken to prevent the same job from being accidently started
	// more than once.
	ClientRequestToken *string `min:"1" type:"string"`

	// ID of the collection that contains the faces you want to search for.
	//
	// CollectionId is a required field
	CollectionId *string `min:"1" type:"string" required:"true"`

	// The minimum confidence in the person match to return. For example, don't
	// return any matches where confidence in matches is less than 70%.
	FaceMatchThreshold *float64 `type:"float"`

	// Unique identifier you specify to identify the job in the completion status
	// published to the Amazon Simple Notification Service topic.
	JobTag *string `min:"1" type:"string"`

	// The ARN of the Amazon SNS topic to which you want Amazon Rekognition Video
	// to publish the completion status of the search.
	NotificationChannel *NotificationChannel `type:"structure"`

	// The video you want to search. The video must be stored in an Amazon S3 bucket.
	//
	// Video is a required field
	Video *Video `type:"structure" required:"true"`
}

// String returns the string representation
func (s StartFaceSearchInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartFaceSearchInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *StartFaceSearchInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "StartFaceSearchInput"}
	if s.ClientRequestToken != nil && len(*s.ClientRequestToken) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("ClientRequestToken", 1))
	}

	if s.CollectionId == nil {
		invalidParams.Add(aws.NewErrParamRequired("CollectionId"))
	}
	if s.CollectionId != nil && len(*s.CollectionId) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("CollectionId", 1))
	}
	if s.JobTag != nil && len(*s.JobTag) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("JobTag", 1))
	}

	if s.Video == nil {
		invalidParams.Add(aws.NewErrParamRequired("Video"))
	}
	if s.NotificationChannel != nil {
		if err := s.NotificationChannel.Validate(); err != nil {
			invalidParams.AddNested("NotificationChannel", err.(aws.ErrInvalidParams))
		}
	}
	if s.Video != nil {
		if err := s.Video.Validate(); err != nil {
			invalidParams.AddNested("Video", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type StartFaceSearchOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The identifier for the search job. Use JobId to identify the job in a subsequent
	// call to GetFaceSearch.
	JobId *string `min:"1" type:"string"`
}

// String returns the string representation
func (s StartFaceSearchOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartFaceSearchOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s StartFaceSearchOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type StartLabelDetectionInput struct {
	_ struct{} `type:"structure"`

	// Idempotent token used to identify the start request. If you use the same
	// token with multiple StartLabelDetection requests, the same JobId is returned.
	// Use ClientRequestToken to prevent the same job from being accidently started
	// more than once.
	ClientRequestToken *string `min:"1" type:"string"`

	// Unique identifier you specify to identify the job in the completion status
	// published to the Amazon Simple Notification Service topic.
	JobTag *string `min:"1" type:"string"`

	// Specifies the minimum confidence that Amazon Rekognition Video must have
	// in order to return a detected label. Confidence represents how certain Amazon
	// Rekognition is that a label is correctly identified.0 is the lowest confidence.
	// 100 is the highest confidence. Amazon Rekognition Video doesn't return any
	// labels with a confidence level lower than this specified value.
	//
	// If you don't specify MinConfidence, the operation returns labels with confidence
	// values greater than or equal to 50 percent.
	MinConfidence *float64 `type:"float"`

	// The Amazon SNS topic ARN you want Amazon Rekognition Video to publish the
	// completion status of the label detection operation to.
	NotificationChannel *NotificationChannel `type:"structure"`

	// The video in which you want to detect labels. The video must be stored in
	// an Amazon S3 bucket.
	//
	// Video is a required field
	Video *Video `type:"structure" required:"true"`
}

// String returns the string representation
func (s StartLabelDetectionInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartLabelDetectionInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *StartLabelDetectionInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "StartLabelDetectionInput"}
	if s.ClientRequestToken != nil && len(*s.ClientRequestToken) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("ClientRequestToken", 1))
	}
	if s.JobTag != nil && len(*s.JobTag) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("JobTag", 1))
	}

	if s.Video == nil {
		invalidParams.Add(aws.NewErrParamRequired("Video"))
	}
	if s.NotificationChannel != nil {
		if err := s.NotificationChannel.Validate(); err != nil {
			invalidParams.AddNested("NotificationChannel", err.(aws.ErrInvalidParams))
		}
	}
	if s.Video != nil {
		if err := s.Video.Validate(); err != nil {
			invalidParams.AddNested("Video", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type StartLabelDetectionOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The identifier for the label detection job. Use JobId to identify the job
	// in a subsequent call to GetLabelDetection.
	JobId *string `min:"1" type:"string"`
}

// String returns the string representation
func (s StartLabelDetectionOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartLabelDetectionOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s StartLabelDetectionOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type StartPersonTrackingInput struct {
	_ struct{} `type:"structure"`

	// Idempotent token used to identify the start request. If you use the same
	// token with multiple StartPersonTracking requests, the same JobId is returned.
	// Use ClientRequestToken to prevent the same job from being accidently started
	// more than once.
	ClientRequestToken *string `min:"1" type:"string"`

	// Unique identifier you specify to identify the job in the completion status
	// published to the Amazon Simple Notification Service topic.
	JobTag *string `min:"1" type:"string"`

	// The Amazon SNS topic ARN you want Amazon Rekognition Video to publish the
	// completion status of the people detection operation to.
	NotificationChannel *NotificationChannel `type:"structure"`

	// The video in which you want to detect people. The video must be stored in
	// an Amazon S3 bucket.
	//
	// Video is a required field
	Video *Video `type:"structure" required:"true"`
}

// String returns the string representation
func (s StartPersonTrackingInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartPersonTrackingInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *StartPersonTrackingInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "StartPersonTrackingInput"}
	if s.ClientRequestToken != nil && len(*s.ClientRequestToken) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("ClientRequestToken", 1))
	}
	if s.JobTag != nil && len(*s.JobTag) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("JobTag", 1))
	}

	if s.Video == nil {
		invalidParams.Add(aws.NewErrParamRequired("Video"))
	}
	if s.NotificationChannel != nil {
		if err := s.NotificationChannel.Validate(); err != nil {
			invalidParams.AddNested("NotificationChannel", err.(aws.ErrInvalidParams))
		}
	}
	if s.Video != nil {
		if err := s.Video.Validate(); err != nil {
			invalidParams.AddNested("Video", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type StartPersonTrackingOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response

	// The identifier for the person detection job. Use JobId to identify the job
	// in a subsequent call to GetPersonTracking.
	JobId *string `min:"1" type:"string"`
}

// String returns the string representation
func (s StartPersonTrackingOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartPersonTrackingOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s StartPersonTrackingOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type StartStreamProcessorInput struct {
	_ struct{} `type:"structure"`

	// The name of the stream processor to start processing.
	//
	// Name is a required field
	Name *string `min:"1" type:"string" required:"true"`
}

// String returns the string representation
func (s StartStreamProcessorInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartStreamProcessorInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *StartStreamProcessorInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "StartStreamProcessorInput"}

	if s.Name == nil {
		invalidParams.Add(aws.NewErrParamRequired("Name"))
	}
	if s.Name != nil && len(*s.Name) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("Name", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type StartStreamProcessorOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response
}

// String returns the string representation
func (s StartStreamProcessorOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StartStreamProcessorOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s StartStreamProcessorOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

type StopStreamProcessorInput struct {
	_ struct{} `type:"structure"`

	// The name of a stream processor created by .
	//
	// Name is a required field
	Name *string `min:"1" type:"string" required:"true"`
}

// String returns the string representation
func (s StopStreamProcessorInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StopStreamProcessorInput) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *StopStreamProcessorInput) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "StopStreamProcessorInput"}

	if s.Name == nil {
		invalidParams.Add(aws.NewErrParamRequired("Name"))
	}
	if s.Name != nil && len(*s.Name) < 1 {
		invalidParams.Add(aws.NewErrParamMinLen("Name", 1))
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

type StopStreamProcessorOutput struct {
	_ struct{} `type:"structure"`

	responseMetadata aws.Response
}

// String returns the string representation
func (s StopStreamProcessorOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StopStreamProcessorOutput) GoString() string {
	return s.String()
}

// SDKResponseMetdata return sthe response metadata for the API.
func (s StopStreamProcessorOutput) SDKResponseMetadata() aws.Response {
	return s.responseMetadata
}

// An object that recognizes faces in a streaming video. An Amazon Rekognition
// stream processor is created by a call to . The request parameters for CreateStreamProcessor
// describe the Kinesis video stream source for the streaming video, face recognition
// parameters, and where to stream the analysis resullts.
type StreamProcessor struct {
	_ struct{} `type:"structure"`

	// Name of the Amazon Rekognition stream processor.
	Name *string `min:"1" type:"string"`

	// Current status of the Amazon Rekognition stream processor.
	Status StreamProcessorStatus `type:"string" enum:"true"`
}

// String returns the string representation
func (s StreamProcessor) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StreamProcessor) GoString() string {
	return s.String()
}

// Information about the source streaming video.
type StreamProcessorInput struct {
	_ struct{} `type:"structure"`

	// The Kinesis video stream input stream for the source streaming video.
	KinesisVideoStream *KinesisVideoStream `type:"structure"`
}

// String returns the string representation
func (s StreamProcessorInput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StreamProcessorInput) GoString() string {
	return s.String()
}

// Information about the Amazon Kinesis Data Streams stream to which a Amazon
// Rekognition Video stream processor streams the results of a video analysis.
// For more information, see CreateStreamProcessor in the Amazon Rekognition
// Developer Guide.
type StreamProcessorOutput struct {
	_ struct{} `type:"structure"`

	// The Amazon Kinesis Data Streams stream to which the Amazon Rekognition stream
	// processor streams the analysis results.
	KinesisDataStream *KinesisDataStream `type:"structure"`
}

// String returns the string representation
func (s StreamProcessorOutput) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StreamProcessorOutput) GoString() string {
	return s.String()
}

// Input parameters used to recognize faces in a streaming video analyzed by
// a Amazon Rekognition stream processor.
type StreamProcessorSettings struct {
	_ struct{} `type:"structure"`

	// Face search settings to use on a streaming video.
	FaceSearch *FaceSearchSettings `type:"structure"`
}

// String returns the string representation
func (s StreamProcessorSettings) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s StreamProcessorSettings) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *StreamProcessorSettings) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "StreamProcessorSettings"}
	if s.FaceSearch != nil {
		if err := s.FaceSearch.Validate(); err != nil {
			invalidParams.AddNested("FaceSearch", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

// Indicates whether or not the face is wearing sunglasses, and the confidence
// level in the determination.
type Sunglasses struct {
	_ struct{} `type:"structure"`

	// Level of confidence in the determination.
	Confidence *float64 `type:"float"`

	// Boolean value that indicates whether the face is wearing sunglasses or not.
	Value *bool `type:"boolean"`
}

// String returns the string representation
func (s Sunglasses) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Sunglasses) GoString() string {
	return s.String()
}

// Information about a word or line of text detected by .
//
// The DetectedText field contains the text that Amazon Rekognition detected
// in the image.
//
// Every word and line has an identifier (Id). Each word belongs to a line and
// has a parent identifier (ParentId) that identifies the line of text in which
// the word appears. The word Id is also an index for the word within a line
// of words.
//
// For more information, see Detecting Text in the Amazon Rekognition Developer
// Guide.
type TextDetection struct {
	_ struct{} `type:"structure"`

	// The confidence that Amazon Rekognition has in the accuracy of the detected
	// text and the accuracy of the geometry points around the detected text.
	Confidence *float64 `type:"float"`

	// The word or line of text recognized by Amazon Rekognition.
	DetectedText *string `type:"string"`

	// The location of the detected text on the image. Includes an axis aligned
	// coarse bounding box surrounding the text and a finer grain polygon for more
	// accurate spatial information.
	Geometry *Geometry `type:"structure"`

	// The identifier for the detected text. The identifier is only unique for a
	// single call to DetectText.
	Id *int64 `type:"integer"`

	// The Parent identifier for the detected text identified by the value of ID.
	// If the type of detected text is LINE, the value of ParentId is Null.
	ParentId *int64 `type:"integer"`

	// The type of text that was detected.
	Type TextTypes `type:"string" enum:"true"`
}

// String returns the string representation
func (s TextDetection) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s TextDetection) GoString() string {
	return s.String()
}

// A face that detected, but didn't index. Use the Reasons response attribute
// to determine why a face wasn't indexed.
type UnindexedFace struct {
	_ struct{} `type:"structure"`

	// The structure that contains attributes of a face that IndexFacesdetected,
	// but didn't index.
	FaceDetail *FaceDetail `type:"structure"`

	// An array of reasons that specify why a face wasn't indexed.
	//
	//    * EXTREME_POSE - The face is at a pose that can't be detected. For example,
	//    the head is turned too far away from the camera.
	//
	//    * EXCEEDS_MAX_FACES - The number of faces detected is already higher than
	//    that specified by the MaxFaces input parameter for IndexFaces.
	//
	//    * LOW_BRIGHTNESS - The image is too dark.
	//
	//    * LOW_SHARPNESS - The image is too blurry.
	//
	//    * LOW_CONFIDENCE - The face was detected with a low confidence.
	//
	//    * SMALL_BOUNDING_BOX - The bounding box around the face is too small.
	Reasons []Reason `type:"list"`
}

// String returns the string representation
func (s UnindexedFace) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s UnindexedFace) GoString() string {
	return s.String()
}

// Video file stored in an Amazon S3 bucket. Amazon Rekognition video start
// operations such as use Video to specify a video for analysis. The supported
// file formats are .mp4, .mov and .avi.
type Video struct {
	_ struct{} `type:"structure"`

	// The Amazon S3 bucket name and file name for the video.
	S3Object *S3Object `type:"structure"`
}

// String returns the string representation
func (s Video) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s Video) GoString() string {
	return s.String()
}

// Validate inspects the fields of the type to determine if they are valid.
func (s *Video) Validate() error {
	invalidParams := aws.ErrInvalidParams{Context: "Video"}
	if s.S3Object != nil {
		if err := s.S3Object.Validate(); err != nil {
			invalidParams.AddNested("S3Object", err.(aws.ErrInvalidParams))
		}
	}

	if invalidParams.Len() > 0 {
		return invalidParams
	}
	return nil
}

// Information about a video that Amazon Rekognition analyzed. Videometadata
// is returned in every page of paginated responses from a Amazon Rekognition
// video operation.
type VideoMetadata struct {
	_ struct{} `type:"structure"`

	// Type of compression used in the analyzed video.
	Codec *string `type:"string"`

	// Length of the video in milliseconds.
	DurationMillis *int64 `type:"long"`

	// Format of the analyzed video. Possible values are MP4, MOV and AVI.
	Format *string `type:"string"`

	// Vertical pixel dimension of the video.
	FrameHeight *int64 `type:"long"`

	// Number of frames per second in the video.
	FrameRate *float64 `type:"float"`

	// Horizontal pixel dimension of the video.
	FrameWidth *int64 `type:"long"`
}

// String returns the string representation
func (s VideoMetadata) String() string {
	return awsutil.Prettify(s)
}

// GoString returns the string representation
func (s VideoMetadata) GoString() string {
	return s.String()
}

type Attribute string

// Enum values for Attribute
const (
	AttributeDefault Attribute = "DEFAULT"
	AttributeAll     Attribute = "ALL"
)

func (enum Attribute) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum Attribute) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type CelebrityRecognitionSortBy string

// Enum values for CelebrityRecognitionSortBy
const (
	CelebrityRecognitionSortById        CelebrityRecognitionSortBy = "ID"
	CelebrityRecognitionSortByTimestamp CelebrityRecognitionSortBy = "TIMESTAMP"
)

func (enum CelebrityRecognitionSortBy) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum CelebrityRecognitionSortBy) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type ContentModerationSortBy string

// Enum values for ContentModerationSortBy
const (
	ContentModerationSortByName      ContentModerationSortBy = "NAME"
	ContentModerationSortByTimestamp ContentModerationSortBy = "TIMESTAMP"
)

func (enum ContentModerationSortBy) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum ContentModerationSortBy) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type EmotionName string

// Enum values for EmotionName
const (
	EmotionNameHappy     EmotionName = "HAPPY"
	EmotionNameSad       EmotionName = "SAD"
	EmotionNameAngry     EmotionName = "ANGRY"
	EmotionNameConfused  EmotionName = "CONFUSED"
	EmotionNameDisgusted EmotionName = "DISGUSTED"
	EmotionNameSurprised EmotionName = "SURPRISED"
	EmotionNameCalm      EmotionName = "CALM"
	EmotionNameUnknown   EmotionName = "UNKNOWN"
)

func (enum EmotionName) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum EmotionName) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type FaceAttributes string

// Enum values for FaceAttributes
const (
	FaceAttributesDefault FaceAttributes = "DEFAULT"
	FaceAttributesAll     FaceAttributes = "ALL"
)

func (enum FaceAttributes) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum FaceAttributes) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type FaceSearchSortBy string

// Enum values for FaceSearchSortBy
const (
	FaceSearchSortByIndex     FaceSearchSortBy = "INDEX"
	FaceSearchSortByTimestamp FaceSearchSortBy = "TIMESTAMP"
)

func (enum FaceSearchSortBy) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum FaceSearchSortBy) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type GenderType string

// Enum values for GenderType
const (
	GenderTypeMale   GenderType = "Male"
	GenderTypeFemale GenderType = "Female"
)

func (enum GenderType) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum GenderType) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type LabelDetectionSortBy string

// Enum values for LabelDetectionSortBy
const (
	LabelDetectionSortByName      LabelDetectionSortBy = "NAME"
	LabelDetectionSortByTimestamp LabelDetectionSortBy = "TIMESTAMP"
)

func (enum LabelDetectionSortBy) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum LabelDetectionSortBy) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type LandmarkType string

// Enum values for LandmarkType
const (
	LandmarkTypeEyeLeft           LandmarkType = "eyeLeft"
	LandmarkTypeEyeRight          LandmarkType = "eyeRight"
	LandmarkTypeNose              LandmarkType = "nose"
	LandmarkTypeMouthLeft         LandmarkType = "mouthLeft"
	LandmarkTypeMouthRight        LandmarkType = "mouthRight"
	LandmarkTypeLeftEyeBrowLeft   LandmarkType = "leftEyeBrowLeft"
	LandmarkTypeLeftEyeBrowRight  LandmarkType = "leftEyeBrowRight"
	LandmarkTypeLeftEyeBrowUp     LandmarkType = "leftEyeBrowUp"
	LandmarkTypeRightEyeBrowLeft  LandmarkType = "rightEyeBrowLeft"
	LandmarkTypeRightEyeBrowRight LandmarkType = "rightEyeBrowRight"
	LandmarkTypeRightEyeBrowUp    LandmarkType = "rightEyeBrowUp"
	LandmarkTypeLeftEyeLeft       LandmarkType = "leftEyeLeft"
	LandmarkTypeLeftEyeRight      LandmarkType = "leftEyeRight"
	LandmarkTypeLeftEyeUp         LandmarkType = "leftEyeUp"
	LandmarkTypeLeftEyeDown       LandmarkType = "leftEyeDown"
	LandmarkTypeRightEyeLeft      LandmarkType = "rightEyeLeft"
	LandmarkTypeRightEyeRight     LandmarkType = "rightEyeRight"
	LandmarkTypeRightEyeUp        LandmarkType = "rightEyeUp"
	LandmarkTypeRightEyeDown      LandmarkType = "rightEyeDown"
	LandmarkTypeNoseLeft          LandmarkType = "noseLeft"
	LandmarkTypeNoseRight         LandmarkType = "noseRight"
	LandmarkTypeMouthUp           LandmarkType = "mouthUp"
	LandmarkTypeMouthDown         LandmarkType = "mouthDown"
	LandmarkTypeLeftPupil         LandmarkType = "leftPupil"
	LandmarkTypeRightPupil        LandmarkType = "rightPupil"
	LandmarkTypeUpperJawlineLeft  LandmarkType = "upperJawlineLeft"
	LandmarkTypeMidJawlineLeft    LandmarkType = "midJawlineLeft"
	LandmarkTypeChinBottom        LandmarkType = "chinBottom"
	LandmarkTypeMidJawlineRight   LandmarkType = "midJawlineRight"
	LandmarkTypeUpperJawlineRight LandmarkType = "upperJawlineRight"
)

func (enum LandmarkType) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum LandmarkType) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type OrientationCorrection string

// Enum values for OrientationCorrection
const (
	OrientationCorrectionRotate0   OrientationCorrection = "ROTATE_0"
	OrientationCorrectionRotate90  OrientationCorrection = "ROTATE_90"
	OrientationCorrectionRotate180 OrientationCorrection = "ROTATE_180"
	OrientationCorrectionRotate270 OrientationCorrection = "ROTATE_270"
)

func (enum OrientationCorrection) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum OrientationCorrection) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type PersonTrackingSortBy string

// Enum values for PersonTrackingSortBy
const (
	PersonTrackingSortByIndex     PersonTrackingSortBy = "INDEX"
	PersonTrackingSortByTimestamp PersonTrackingSortBy = "TIMESTAMP"
)

func (enum PersonTrackingSortBy) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum PersonTrackingSortBy) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type QualityFilter string

// Enum values for QualityFilter
const (
	QualityFilterNone QualityFilter = "NONE"
	QualityFilterAuto QualityFilter = "AUTO"
)

func (enum QualityFilter) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum QualityFilter) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type Reason string

// Enum values for Reason
const (
	ReasonExceedsMaxFaces  Reason = "EXCEEDS_MAX_FACES"
	ReasonExtremePose      Reason = "EXTREME_POSE"
	ReasonLowBrightness    Reason = "LOW_BRIGHTNESS"
	ReasonLowSharpness     Reason = "LOW_SHARPNESS"
	ReasonLowConfidence    Reason = "LOW_CONFIDENCE"
	ReasonSmallBoundingBox Reason = "SMALL_BOUNDING_BOX"
)

func (enum Reason) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum Reason) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type StreamProcessorStatus string

// Enum values for StreamProcessorStatus
const (
	StreamProcessorStatusStopped  StreamProcessorStatus = "STOPPED"
	StreamProcessorStatusStarting StreamProcessorStatus = "STARTING"
	StreamProcessorStatusRunning  StreamProcessorStatus = "RUNNING"
	StreamProcessorStatusFailed   StreamProcessorStatus = "FAILED"
	StreamProcessorStatusStopping StreamProcessorStatus = "STOPPING"
)

func (enum StreamProcessorStatus) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum StreamProcessorStatus) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type TextTypes string

// Enum values for TextTypes
const (
	TextTypesLine TextTypes = "LINE"
	TextTypesWord TextTypes = "WORD"
)

func (enum TextTypes) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum TextTypes) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}

type VideoJobStatus string

// Enum values for VideoJobStatus
const (
	VideoJobStatusInProgress VideoJobStatus = "IN_PROGRESS"
	VideoJobStatusSucceeded  VideoJobStatus = "SUCCEEDED"
	VideoJobStatusFailed     VideoJobStatus = "FAILED"
)

func (enum VideoJobStatus) MarshalValue() (string, error) {
	return string(enum), nil
}

func (enum VideoJobStatus) MarshalValueBuf(b []byte) ([]byte, error) {
	b = b[0:0]
	return append(b, enum...), nil
}
